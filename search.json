[
  {
    "objectID": "ADA/ada.html",
    "href": "ADA/ada.html",
    "title": "Analytical Data Access (ADA)",
    "section": "",
    "text": "Guidance for analysts on how to interact with and use data stored in ADA using Databricks",
    "crumbs": [
      "Learning resources",
      "Analytical Data Access (ADA)"
    ]
  },
  {
    "objectID": "ADA/ada.html#what-is-the-ada-project",
    "href": "ADA/ada.html#what-is-the-ada-project",
    "title": "Analytical Data Access (ADA)",
    "section": "What is the ADA project?",
    "text": "What is the ADA project?\nThe Analytical Data Access (ADA) service has been created to support analysts, engineers and policy team members.\nIt brings together:\n\na searchable data catalogue - the Data Discovery Platform (DDP)\na workbench of tools for analysing data - including Databricks\na form for requesting access to data\na support section - with walkthrough instructions\na collection of data reports and dashboards\na news and updates area\n\nAs long as you’re connected to the network - it’s available to anyone who works for the Department of Education.\n\n\n\n\n\n\nTip\n\n\n\nThere is an Analytical Data Access jargon buster to help you understand different ADA and Databricks terms.\n\n\nThe ADA project is currently in its onboarding phase. The ADA team will work with analyst teams to plan migrations. We encourage teams to engage early so that you have time to migrate your work and receive support from the project team.\nData migration will replace three legacy systems:\n\nPupil Data Repository (PDR)\nAnalysis and Modelling (A&M)\nEnterprise Data and Analysis Platform (EDAP1)\n\nDecommissioning of legacy servers will be completed in 2026.\nSee these Databricks support notebooks for more detailed reference guides into how the service has been set up at DfE, including more information on workspaces, environments, notebooks, clusters, catalogs etc. Please note that you will need a Databricks account to access these.\nFrom the ADA homepage you will be able to find data and access cloud analytical tools. This includes:\n\nNavigation to the Data Discovery Platform to find information regarding metadata of the datasets, including how to request access and information about the quality of the data. The catalogue will eventually include all Department data, as well as tagging of datasets available through ADA.\nAn analysis workbench with access to cloud computing and Databricks. This will be extended to include POSIT workbench, which will allow RStudio to be used in the cloud.\nA repository of Reports and Dashboards where outputs of data and analytical work will be saved. This will allow colleagues across DfE to interrogate and visualise data. In time this catalogue will grow and the ADA team are currently building out a formal strategy for this area.\n\n\n\nADA support\n\nThe ADA team has established an analyst Community of Practice that:\n\nIncreases the breadth and depth of knowledge about new tools\nBuilds confidence in using core functionality\nShares learning experiences and best practice\nIdentifies future opportunities to support the Department’s Strategic Data Transformation\n\nYou can access support in the following places:\n\nThe ADA user group on Microsoft Teams\nThe ADA website has a list of support resources\nYou can find a list of current ADA champions can be found on the ADA intranet page, and they are able to provide advice and support if you run into any issues. If you’re passionate about helping others with the migration and using innovative data tools, why not become an ADA champion? To get involved, please contact the ADA team",
    "crumbs": [
      "Learning resources",
      "Analytical Data Access (ADA)"
    ]
  },
  {
    "objectID": "ADA/ada.html#what-does-the-ada-project-mean-for-analysts",
    "href": "ADA/ada.html#what-does-the-ada-project-mean-for-analysts",
    "title": "Analytical Data Access (ADA)",
    "section": "What does the ADA project mean for analysts?",
    "text": "What does the ADA project mean for analysts?\n\nBenefits of the ADA project and Databricks\n\nMigration to Databricks offers a lot of potential benefits to analysts. These include:\n\nHaving all data together in one place and being able to access it via one interface, rather than split across separate areas requiring separate access permissions and pieces of software\nIf your scripts usually take a long time to run, cloud computing can speed up processing and reduce code running time. This is ideal for big data, scripts that require a lot of data transformation (e.g. complicated or large joins), and machine learning projects\nIf you regularly have to run the same code, access to workflows and code scheduling mean that you can set code to run at certain days or times to improve efficiency. Scheduled workflows will run even if your laptop is switched off\nBetter transparency of work being undertaken in the department by making use of the ADA shared reports area.\n\n\n\n\nMaking the most of Databricks\n\n\n\n\n\n\n\nNote\n\n\n\nThe earlier your data is migrated, the more time you will have to dual run code from your existing methodology against code run on the Databricks platform, and the more time you have to influence the project and its offer for analysts. We encourage all analysts to engage with training and ask the ADA team about any questions or concerns as soon as possible so that you are prepared for any upcoming changes.\n\n\nIn order to take advantage of the benefits listed above, we recommend that you and your team:\n\nArrange data migration and access to Databricks. Onboarding is currently voluntary, so it’s a great opportunity to move early and have more time to get familiar with the system and undertake training. Get in touch with the ADA team to discuss onboarding with them.\nTake part in Databricks training for R and / or SQL. R training is currently offered by the ADA team on a monthly basis and SQL training is in the process of being developed. Upcoming training is advertised on the ADA user group Teams channel.\nBe proactive in determining whether any changes will be necessary for your existing code and set time aside to make these changes - see the guidance in the what this means for existing code section below.\n\n\n\n\n\n\n\nImportant\n\n\n\nFor teams that have invested in RAP principles and already have existing code pipelines, it is possible to take advantage of cloud computing and use of data in the Databricks platform with minimal changes. This is covered in more detail in the what this means for existing code section below.",
    "crumbs": [
      "Learning resources",
      "Analytical Data Access (ADA)"
    ]
  },
  {
    "objectID": "ADA/ada.html#what-is-databricks",
    "href": "ADA/ada.html#what-is-databricks",
    "title": "Analytical Data Access (ADA)",
    "section": "What is Databricks?",
    "text": "What is Databricks?\n\nThe Databricks platform\n\nDatabricks is the software that underpins ADA’s analytical workbench, allowing analysts to explore and analyse data via a browser, taking advantage of cloud computing. Unlike RStudio or SQL Server Management Studio (SSMS), you don’t have to download any software to be able to use it. Databricks offers a few different ways to work with data, including scripts and notebooks. Databricks notebooks support multiple programming languages, including R, Python, and SQL.\nYou can also access data held within the Databricks platform using other software, such as RStudio.\n\n\n\nDatabricks FAQs\n\nIs Databricks different to RAP?\nYes - Databricks is a tool that you can use to create code and pipelines for reproducible analysis, similarly to how you’d currently use RStudio or SQL Server Management Studio.\nHow much time will it take me to learn the basics?\nWe recommend that you and your team engage with the platform as soon as possible so that you can learn at your own pace before you fully migrate. There are plenty of training resources available, including monthly training workshops run by the ADA team. These are advertised on the ADA user group Teams channel.\nCan I still use SQL?\nEventually all teams will migrate away from using SQL Server Management Studio. However, you can still write SQL code inside Databricks, and you can still run SQL scripts from RStudio against data held in the Databricks unity catalog.\nDo I have to move all my existing code over to Databricks?\nNo - this is discussed in more depth in the what this means for existing code section below.\n\n\n\nWorking with data and code\nIn the Databricks interface, you have the choice to use notebooks, scripts or the Databricks SQL Editor. You can also connect Databricks to other IDEs, such as RStudio.\n\n\nGetting access to your data\n\nData in Databricks is held in catalogs in the Delta Lake, which is centrally governed by the ADA team. Catalogs provide different levels of access control and security. In Databricks, databases are referred to as “schemas”, with tables sitting below them. You can find out more information about this on the ADA guidance site and on our Databricks fundamentals page.\nYou will need to be given access to Databricks and the data you require by the ADA team by completing an “Analytical Data Access Request” service request form in the IT Help Centre. At the moment, not all of the data currently available in SSMS has been migrated to the Delta Lake for use in Databricks. You can check the status of various datasets in the data catalogue dashboard.\nTo access your data and make use of cloud computing resource, you will need access to a cluster, which is a group of virtual machines that complete your processing for you. Your cluster setup will depend on which programming language you wish to use. You will be given access to a shared cluster when your data access is granted. Please see the how clusters work page on the ADA guidance site and our Databricks fundamentals page for more detail.\n\n\n\nWriting and running code: scripts and IDEs\n\nWriting scripts in SSMS and using IDEs such as RStudio are the way in which we currently write and run the majority of our code in DfE. You can learn more about how existing code will be affected by the introduction of Databricks in the what this means for existing code section below.\nScripts (e.g. R scripts) can be created and saved in your Workspace (the equivalent of your personal folder) and run via the Databricks interface.\nThere is detailed guidance on using the Databricks SQL editor, which is similar to SSMS, on the ADA guidance site.\nYou can also run code from RStudio or other IDEs locally and execute it against the data held in Databricks. To do this, you will need to set up a connection between RStudio and the Databricks SQL Warehouse. This is explained in the R scripts in RStudio section below.\n\n\n\nWriting and running code: workflows\n\nWorkflows are a Databricks feature that allow you to schedule and automatically run your code at specified times and in the order you suggest.\nWorkflows allow you to automate the running of single or multiple scripts, so if you have large amounts of data processing to do on a regular basis, you can set up a workflow to complete this overnight. Your laptop does not need to be switched on for this to work. You can read more about workflows on our Databricks workflows guidance page.\n\n\n\nWriting and running code: notebooks\n\nNotebooks might be something you’ve not come across before. Notebooks are made up of multiple code or markdown cells, allowing you to interactively run code and see the results in the same page. The way you write and use code is different to how you would work with IDEs (Integrated Development Environments) like RStudio. You can find more information on Databricks notebooks and their structure on the ADA guidance site and on our Databricks notebooks guidance page.\nNotebooks can be helpful for ad hoc, exploratory, or one off pieces of analysis to add additional documentation as you go. You can also use notebooks to present findings to others by hiding code cells and only showing Markdown cells containing charts, tables, images or text. They can be exported as HTML files to enable easier sharing outside of the Databricks interface.\n\n\n\n\n\n\nImportant\n\n\n\nIf the rest of your team are unfamiliar with notebooks and will need to pick up your work at a later date, use of notebooks without proper handover or documentation could cause issues and result in work no longer being reproducible. The Statistics Development Team are happy to advise on this and provide guidance if you have any concerns.\n\n\nFor regular analysis pipelines and models adopting RAP principles we’d usually advise against using notebooks and instead recommend a Git controlled repository of code scripts using the standard repository template available in dfeR.\nWe recommend this because in addition to notebooks being different to how many teams write code now, version control, collaboration and reproducibility can all be more difficult in Databricks notebooks. If you would like to use notebooks for analysis, they must be used in conjunction with an Azure DevOps or GitHub repository. If you share a notebook with edit permissions, it is possible for your code run to be accidentally cancelled if another user starts running code in the same notebook. Your code can also be overwritten by other users, and it can be difficult to recover previous versions.\nWhen you have a full script in an IDE, the script will provide the same output consistently. However, with notebooks you can run code cells independently of one another. You must ensure that you run all code cells in the correct sequential order. If you do not run all code cells in order then your output could be inconsistent and result in errors, potentially making QA more difficult. This introduces more manual steps and increases the risk of human error.\n\n\n\n\n\n\nWarning\n\n\n\nWhen using Databricks notebooks with git, cell outputs are automatically cleared by default when your code is pushed up to a repository. However, if you share a notebook using the Databricks interface, cell outputs remain intact from the last time you ran the code. This risks sharing data that should not be shared with others. Before sharing code with someone outside your team, you must use the Clear all cell outputs option in the Databricks “Run” menu to avoid inadvertently sharing data that only you and your team should have access to.\n\n\nThe table below, taken from NHS-R guidance, shows a comparison of the features of notebooks and IDEs to help you decide where each would be most appropriately used:\n\n\n\nFeature\nNotebooks\nIDEs\n\n\n\n\nInteractive outputs (tables, plots, etc)\nYes\nNo*\n\n\nStorytelling and sharing results\nYes\nNo\n\n\nBenefits out-of-the-box (minimal configuration)\nYes\nNo\n\n\nDeterministic outputs (i.e. same result every run)\nNo\nYes\n\n\nSupports modular code structure & packages\nNo\nYes\n\n\nSupports unit testing frameworks\nNo\nYes\n\n\nNice version control (e.g. readable git diffs)\nNo\nYes\n\n\nAutocomplete / auto-formatting & syntax highlighting\nYes**\nYes\n\n\nCompatible with sharing code externally\nYes***\nYes\n\n\n\n* whilst you can still make and preview tables and plots in RStudio and other IDEs, they’re not interactive in the same way as those in notebooks\n** auto-formatting is available for SQL and Python in Databricks, but not R\n*** external users must have software that would allow them to open notebooks\nYou can read more about the differences between IDEs and notebooks and where each might be useful on the NHS-R website.\n\n\n\nAccessing code from a Git repository\n\nDatabricks has inbuilt Git functionality, and you can save your notebooks and scripts inside Git repositories as with any other file formats. There is guidance on using Databricks with Azure DevOps on our Databricks and version control page.",
    "crumbs": [
      "Learning resources",
      "Analytical Data Access (ADA)"
    ]
  },
  {
    "objectID": "ADA/ada.html#what-this-means-for-existing-code",
    "href": "ADA/ada.html#what-this-means-for-existing-code",
    "title": "Analytical Data Access (ADA)",
    "section": "What this means for existing code",
    "text": "What this means for existing code\n\nSQL scripts\n\n\nScripts currently used in SSMS\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce your team’s data has been migrated, you will not immediately lose access to your existing data sources, so you will be able to dual run your code using data from SSMS and data from the Databricks unity catalog to ensure that everything is working as it should be. Once you have confirmed that you are happy with the data in Databricks, your access to the legacy platform will be switched off.\n\n\nIf you currently use data and scripts stored in SSMS for your analysis, there will be small changes you need to make that vary depending on your current process.\nYour SQL code will need to be migrated over to Databricks to allow it to read from the Databricks SQL Warehouse and will require some minor changes to syntax as SSMS uses T-SQL and Databricks uses Spark SQL. The ADA team are currently developing a tool to allow you to easily convert T-SQL queries to Spark SQL syntax, and Microsoft Copilot can also be useful for helping you with code translation. All DfE laptops should provide you with access to Copilot as standard.\nYou can write new SQL code in the Databricks SQL Editor, which is very similar to SSMS. We recommend using the SQL Editor for short, ad-hoc SQL queries, such as those required to answer PQs. For longer or more complex SQL analysis we recommend using standalone scripts run from an RStudio project, or Databricks notebooks.\n\n\n\nSQL scripts in repositories or embedded in R scripts\n\nThese scripts do not need to be moved, but the syntax of the scripts or embedded code will need to be rewritten from T-SQL to Spark SQL.\nYou will also need to change the connection source to allow the code to run. Rather than pointing to data held in current SQL databases, you will need to point towards the Databricks unity catalog instead. To do this, please see our page on setting up a connection between Databricks and RStudio using a SQL warehouse.\n\n\n\nWorkflows\n\nIf you’d like to make use of automated workflows to run scheduled code, then your SQL code will need to be moved into Databricks, using Spark SQL syntax. Workflows can make use of code from notebooks or R or SQL scripts stored in either a Databricks workspace or a Git repository. You can read more about workflows on our Databricks workflows page.\nIn addition to setting up workflows using the point and click interface in Databricks, you can also script them yourself. To learn more about this, see our script workflows in Databricks and script workflows in RStudio user guides.\n\n\n\n\nR scripts in RStudio\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you have an existing RAP process and pipeline set up using RStudio and SSMS, there is no current expectation for you to migrate your existing R code or scripts from RStudio to Databricks notebooks.\n\n\nYour R code can remain in RStudio and all existing R processes can still be run there. Code that points towards SQL scripts will need to be redirected to Databricks rather than SSMS. In order to use RStudio with a Databricks SQL warehouse, you will need to manually set up a connection. You can find more information about that on our set up Databricks SQL Warehouse with RStudio.\n\n\n\nVisualising differences in your existing pipelines\n\nIn the diagram below, we have outlined the potential differences in pipelines before vs after migration to Databricks (changes are highlighted in red):\n\n\n\nADA-diagram",
    "crumbs": [
      "Learning resources",
      "Analytical Data Access (ADA)"
    ]
  },
  {
    "objectID": "ADA/ada.html#useful-links",
    "href": "ADA/ada.html#useful-links",
    "title": "Analytical Data Access (ADA)",
    "section": "Useful links",
    "text": "Useful links\n\nADA support page\n\nSee the ADA support page on how Databricks works.\n\n\n\nDatabricks jargon buster\nThe core concepts of Databricks and any related acronyms are explained in the jargon buster\n\n\n\nDifferences between IDEs and Notebooks\n\nRStudio is an IDE (Integrated Development Environment) whereas Databricks makes use of Notebooks, where you write your code in blocks contained in individual cells. There are advantages and disadvantages to both methods, and this NHS-R article outlines some of the differences you might come across when switching from using IDEs to Notebooks.\n\n\n\nWalkthrough a simple analysis\n\nSee the ADA support page on using R, python and SQL in Databricks.\n\n\n\nHelpful getting started videos\n\nSee the collection of getting started videos covering the basics of Databricks and using notebooks.\n\n\n\nDatabricks code templates\n\nSee examples and code snippets in R, SQL and Python in these Databricks code template notebooks.",
    "crumbs": [
      "Learning resources",
      "Analytical Data Access (ADA)"
    ]
  },
  {
    "objectID": "ADA/databricks_workflow_script_databricks.html",
    "href": "ADA/databricks_workflow_script_databricks.html",
    "title": "Script workflows in Databricks",
    "section": "",
    "text": "Important\n\n\n\nPlease be aware that the Databricks platform is regularly updated and may look different from the guidance included on this site. If you notice any discrepancies between the content on this site and the Databricks platform, please let us know by contacting statistics.development@education.gov.uk.\n\n\nWorkflows can be constructed through the Databricks Workflows user interface (UI), however for large or complex workflows the UI can be a time consuming way to build a workflow. In these scenarios it is quicker and more inline with RAP principles to script your workflow.\nFor a pipeline to be built there must be scripts, queries or notebooks available to read by Databricks, either located in your workspace, or in a Git repository.\nFor this example we will create a folder in our workspace, create two test notebooks to comprise the workflow, and a third to script the job and set it off running. We’ll also set it up to notify us by email when the workflow successfully completes.\n\nCreate a folder in your workspace to store your notebooks. First click ‘Workspace’ in the sidebar (1), then navigate to your user folder (2). Then click the blue ‘Create’ button (3) and select ‘Folder’. Give the folder a name (4) such as ‘Test workflow’ and then click the blue ‘Create’ button (5).\n\n\nOnce in your new folder and click the blue ‘Create’ button again, this time choosing ‘Notebook’. Once in your new Notebook retitle it to ‘Test task 1’ (1), and set the default language to R (2). Then in the first code chunk write print(\"This is a test task\") (3).\n\nCreate a second workbook in the same folder titled ‘Test task 2’, and in the first code chunk write print(\"This is another test task\") .\n\nCreate a third notebook and title it ‘Create and run job’. In the first cell load the tidyversepackage. Install the devtools package and load it, then use it’s install_github() function to install the databricks package. Load the newly installed databricks package.\n\n\n    library(tidyverse)\n\n    install.packages(\"devtools\")\n    library(devtools)\n    install_github(\"databrickslabs/databricks-sdk-r\")\n    library(databricks)\n\nCreate a new code chunk and create a text widget to contain your DataBricks access token. Run this cell to create the widget at the top of the page. Once the widget is there add in your personal access token into the text box.\n\n    dbutils.widgets.text(\"api_token\", \"\")\n\n\n\n\n\n\nDatabricks access token\n\n\n\nPersonal Authentication Token (PAT)s are a unique code that is generated to let Databricks know who is accessing it from the outside. It functions as a password and so must not be shared with anyone.\nIf you haven’t already generated a Databricks token you can find instructions on how to do so in the Setup Databricks personal compute cluster with RStudio article.\n\n\n\n\n\n\n\n\nDon’t put your token into the code\n\n\n\nThe reason we’re using a widget here for your access token is that we don’t want to take any risk of someone else being able to view your PAT token. If we were to hardcode it into the notebook then anyone with access to the code would be able to copy your PAT token and ‘masquerade’ as you.\n\n\n\nWe can now connect to the API through the databricks package using the databricks::DatabricksClient() function. It requires the host which is the URL of the Databricks platform up until (and including) the first /, and your token. We’ll store the result in a variable called client as we need to pass this to the other functions in the databricks library.\nWe can then use the databricks::clustersList() function to fetch a list of the clusters, which we can view using display().\nhost &lt;- \"https://adb-5037484389568426.6.azuredatabricks.net/\"\napi_token &lt;- dbutils.widgets.get(\"api_token\")\n\nclient &lt;- databricks::DatabricksClient(host = host, token = api_token)\n\nclusters &lt;- databricks::clustersList(client)\n\ndisplay(clusters %&gt;% select(cluster_id, creator_user_name, cluster_name, spark_version, cluster_cores, cluster_memory_mb))\n\n\n\n\n\n\n\nClusters\n\n\n\nThe databricks::clustersList() function will return any clusters that you have permission to see.\nThe data returned by the function is hierarchical, and a single ‘column’ may contain several other columns. As the display() function renders a table, you’ll have to select only columns that display() knows how to show. Generally, the columns that are at the left-most position when you run str(databricks::clustersList(client)) (shows the structure).\n\n\nMake a note of your cluster ID and save it in a variable called cluster_id. You could automate this step by filtering the clusters data frame as long as you ensure that it only results a single cluster_id.\n  cluster_id &lt;- \"&lt;your cluster id&gt;\"\nCreate a new code block and we’ll start by setting some parameters for the job. Firstly we’ll need a job_name, and the paths to the Notebooks we’re wanting to include in the workflow. We’ll also need to create a unique task_key for each of the Notebook tasks we’re going to set up.\njob_name &lt;- \"test job\"\nfirst_notebook_path &lt;- \"/Users/nicholas.treece@education.gov.uk/R SDK/Test Notebook\"\nsecond_notebook_path &lt;- \"/Users/nicholas.treece@education.gov.uk/R SDK/Test Notebook 2\"\ntask_key_1 &lt;- \"test_key\"\ntask_key_2 &lt;- \"test_key_2\"\n\nWe can then define the tasks as lists. There are many options available available for setting when creating a task, a full list of which can be found in the tasks section of the job API documentation. When reading this documentation any parameter that is marked as an object needs to be passed as a list (list()) in R, and anything marked as an array should be passed as a vector (c()).\nFor the first task we’ll give it the first task_key we created above, and tell it to run on our existing cluster by passing the ID of our cluster to existing_cluster_id, we’ll then specify that it is a notebook_task and pass that a list with the notebook_path and the source which we will set to WORKSPACE (as opposed to a Git repository) for the purposes of this tutorial.\n first_job_task &lt;- list(\n  task_key = task_key_1,\n  existing_cluster_id = cluster_id,\n  notebook_task = list(\n                    notebook_path = first_notebook_path,\n                    source = \"WORKSPACE\"\n                  )\n )\nFor the second task we will do the same, but using the second task_key and notebook_path we defined. In addition, we’ll also add a depends_on clause with the previous task_key (passed in a list), and specify it is only to run_if ALL_SUCCESS. This means that the second task won’t begin processing unless all of the tasks it depends_on have completed successfully.\nsecond_job_task &lt;- list(\n  task_key = task_key_2,\n  existing_cluster_id = cluster_id,\n  notebook_task = list(\n                    notebook_path = second_notebook_path,\n                    source = \"WORKSPACE\"\n                  ),\n  depends_on = list(task_key = task_key_1),\n  run_if = \"ALL_SUCCESS\"\n )\n\nNow we have both of our tasks defined we can create the job using the databricks::jobCreate() function. We pass it the client as the first argument, then the job name we defined. The tasks are passed as a list which contains each of the task lists we built above.\nWe’ll also tell it to send us email_notifications by passing a list with an on_success value of email addresses.\n\nThe function returns the ID of the job we just created, so we will want to store the response in a variable called workflow so we can refer to it later.\nworkflow &lt;- jobsCreate(client,\n                      name = job_name,\n                      tasks = list(first_job_task, #list\n                                   second_job_task), #list\n                      email_notifications = list(\n                                              on_success = c(\"your-email\")\n                                            ))\n\n\n\n\n\n\nLists of lists\n\n\n\nA list() in R is used to contain any number and type of data, including other list()s. This makes it excellent for storing hierarchical data in one place, however it can get quite confusing quite quickly.\nSometimes it’s easier to break these lists() up into pieces by defining them seperately, as we did above by defining the task lists separately then passing them to the tasks argument in the jobsCreate() function.\nThis often makes it easier to think about and construct, but certainly makes it easier to read. Consider the code below which does exactly the same thing as the code above, but is just written all at once.\nworkflow &lt;- jobsCreate(client,\n                      name = job_name,\n                      tasks = list(\n                                list(\n                                  task_key = task_key_1,\n                                  existing_cluster_id = cluster_id,\n                                  notebook_task = list(\n                                                    notebook_path = first_notebook_path,\n                                                    source = \"WORKSPACE\"\n                                                  )\n                                ),\n                                list(\n                                  task_key = task_key_2,\n                                  existing_cluster_id = cluster_id,\n                                  notebook_task = list(\n                                                    notebook_path = second_notebook_path,\n                                                    source = \"WORKSPACE\"\n                                                  ),\n                                  depends_on = list(task_key = task_key_1),\n                                  run_if = \"ALL_SUCCESS\"\n                                )\n                              ),\n                      email_notifications = list(\n                                              on_success = c(\"your-email\")\n                                            )\n                      )\nWe can see here that the code is getting very long, and is also more difficult to see which options relate to which list. If it weren’t for being diligent with indentation here we’d have to resort to counting brackets to see what belonged where. This is especially problematic if you accidentally delete a bracket and need to work out where it was meant to go.\n\n\nWe can now get the ID of the job that was created and tell the API to run the job. In a new code chunk we’ll store the job_id from the workflow variable above. We’ll then use the databricks::jobsRunNow() function to tell it to run the workflow we just created by passing it the job_id we just stored. We’ll also store the job_run_id returned by the databricks::jobsRunNow() function.\njob_id &lt;- workflow$job_id\n\njob_run &lt;- jobsRunNow(client, \n                      job_id = job_id)\n\njob_run_id &lt;- job_run$run_id\nWe will now use this to create links to the job and the specific run of the job we just set off.\nIn a new code cell, define a job_link by paste0()ing the host variable we passed to the databricks::DatabricksClient() function earlier, followed by \"job/\" followed by the job_id defined above.\nWe can then create a job_run_link by paste0()ing together the job_link followed by \"/runs/\" then the job_run_id from the previous step.\nWe can then output the job_link as text at the bottom of the cell.\njob_link &lt;- paste0(host,\"jobs/\",job_id)\njob_run_link &lt;- paste0(job_link,\"/runs/\", job_run_id)\njob_link\nIn a new cell, output the job_run_link.\n\n\n\n\n\n\n\nOutput limits on code chunks\n\n\n\nEach chunk will display an output (assuming there are any) underneath the chunk once it has been run. Each chunk is limited to a single output though, which defaults to the last output generated.\nSo if we had written a cell to output both links at the same time, we would still only see the job_run_link.\n\n\n\nNow click on the links and check they work.\nYou’ve now created a workflow with code, and each time you re-run this notebook another workflow with the same name will be created. As this is a tutorial which most analysts may have to follow at some point, the logical conclusion is that we will end up with hundreds of ‘test jobs’ cluttering up the workflow page.\n\nTo avoid that let’s use the databricks::jobsDelete() function to clean up after ourselves. All that we need to do is pass the function the client, and job_id variables from above.\njobsDelete(client, job_id)\n\n\n\n\n\n\nNote\n\n\n\nIf you have been running and re-running bits of this code iteratively, there’s a good chance you already have several instances of ‘test job’ listed under your name.\n\nIf this is the case we’ll want to clean up each of these, ideally without having to manually click through the UI process for each one.\nTo do this, firstly call the databricks::jobsList() function, passing it the client variable, and specifying the name of the jobs you want to list. Then filter the list to just the jobs with a creator_user_name of your email address. To see the resulting jobs use the display() function as below at the bottom of a code chunk.\nmy_jobs &lt;- jobsList(client, \n                    name = \"test job\") %&gt;% \n            filter(creator_user_name == 'nicholas.treece@education.gov.uk')\n\ndisplay(my_jobs %&gt;% select(job_id, creator_user_name, run_as_user_name, created_time))\nWe can now loop through the individual job_ids contained in my_jobs and use the databricks::jobsDelete() function to remove them all at once, programmatically.\nfor(job_id in my_jobs$job_id){\n  jobsDelete(client, job_id)\n}\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Databricks Setup Guides",
      "Script workflows in Databricks"
    ]
  },
  {
    "objectID": "ADA/git_databricks.html",
    "href": "ADA/git_databricks.html",
    "title": "Use Databricks with Git",
    "section": "",
    "text": "Guidance for analysts on how to connect Databricks to GitHub and Azure DevOps",
    "crumbs": [
      "Databricks Setup Guides",
      "Use Databricks with Git"
    ]
  },
  {
    "objectID": "ADA/git_databricks.html#why-should-i-use-git-with-databricks",
    "href": "ADA/git_databricks.html#why-should-i-use-git-with-databricks",
    "title": "Use Databricks with Git",
    "section": "Why should I use Git with Databricks?",
    "text": "Why should I use Git with Databricks?\nFor analytical projects developed on Databricks in the DfE, we recommend using GitHub or Azure DevOps for sharing, collaborating and proper version control of work.\n\n\nEasier collaboration\n\nWhen you’re working on notebooks in Databricks without a Git connection, they tend to be saved in your own personal Workspace. This means that they are only accessible by you, unless you share them. This has the potential to cause issues if someone needs to run any code you have stored in a notebook within Databricks e.g. when you leave your current team, take non-working days or annual leave, team members will not be able to access any of your code stored in your notebooks on Databricks. If the notebooks are stored in a Github or Azure DevOps repo, your team can all access them by cloning the repo in Databricks.\n\n\n\nVersion control\n\nDatabricks autosaves your notebooks as you’re working on them, making version control more difficult. If you use Git, you’ll be able to see the full version history of your work and easily roll back to older versions if you need to. It also significantly helps simplify switching between different methodologies as well as facilitating proper code QA and review.",
    "crumbs": [
      "Databricks Setup Guides",
      "Use Databricks with Git"
    ]
  },
  {
    "objectID": "ADA/git_databricks.html#github-or-azure-devops",
    "href": "ADA/git_databricks.html#github-or-azure-devops",
    "title": "Use Databricks with Git",
    "section": "GitHub or Azure DevOps?",
    "text": "GitHub or Azure DevOps?\nEither GitHub or Azure DevOps repos can be used in connection with Databricks, but we advise you to follow the guidance relating to public and private repos in our What is Git for? section.",
    "crumbs": [
      "Databricks Setup Guides",
      "Use Databricks with Git"
    ]
  },
  {
    "objectID": "ADA/git_databricks.html#is-it-safe",
    "href": "ADA/git_databricks.html#is-it-safe",
    "title": "Use Databricks with Git",
    "section": "Is it safe?",
    "text": "Is it safe?\nThe connection between Git and Databricks is established using a secure access token from your Git account, which means it is safe. You will need to renew your access token after a given period of time - if you do not, then your connection between Git and Databricks will no longer work.\nAdditionally, when you commit and push notebooks through the Databricks interface, it will automatically clear any output cells from your notebook, meaning that you cannot accidentally include any unpublished data in there.",
    "crumbs": [
      "Databricks Setup Guides",
      "Use Databricks with Git"
    ]
  },
  {
    "objectID": "ADA/git_databricks.html#setting-up-a-connection-to-azure-devops",
    "href": "ADA/git_databricks.html#setting-up-a-connection-to-azure-devops",
    "title": "Use Databricks with Git",
    "section": "Setting up a connection to Azure DevOps",
    "text": "Setting up a connection to Azure DevOps\n\nPrerequisites\n\n\nAn Azure DevOps account and access to the repo you need to connect to\nA Databricks account and access to your notebooks\n\n\n\n\nGetting set up\n\n\nAccess Tokens\n\nAccess tokens are long strings of numbers and letters that act like a password between two services. They identify the user and their permissions from one service to another.\nIn this case, we will generate an access token in DevOps and give it to Databricks. To generate your Azure DevOps access token, go to DevOps and click the user settings icon to the left of your initials in the top right of your screen - it looks like a person with a cog next to them:\n\nIn the user settings menu, click Personal Access Tokens. On the screen that appears, click the blue “New Token” button in the top right:\n\nThe following window should appear:\n\n\nGive the token a sensible name so that you can identify it in your list of tokens.\nThe organisation field should be pre-populated for you, this can be left alone.\nYou can modify the expiration date. Every time the token expires, you will have to follow this guidance again to set up a new token and reconnect to Git, so choose something sensible.\nYou should make sure that “Full access” is selected under the Scopes heading.\n\nWhen you have entered all of the required information, click “Create” at the bottom of the window.\nA “Success!” window will open containing your new access token. You must copy the token immediately, as you will not be able to access it again once this window is closed. If you fail to copy the token, you will have to regenerate a new one.\n\n\n\n\nConnecting to Databricks\n\nNow that you have your access token, you should go straight to Databricks. In the top right corner of the Databricks window, click your username and then “User Settings”:\n\nYou should then select “Linked Accounts” from the User menu on the left. The following page will open:\n\n\nWe recommend immediately pasting your copied access token into the “Token” field at the bottom of the page to avoid losing it.\nGit provider should be set as “Azure DevOps Services (Personal Access Token)”\nEnter your email into the “Git provider username or email” field\n\nYou can then click Save at the bottom of the page, and now your connection between Azure DevOps and Databricks is established!\n\n\n\nConnecting to repos and cloning\n\nJust like any other way that you’ve worked with Git before, the first step is going to be to clone your repo inside Databricks. Git folders (previously called Repos in Databricks) can now be created in your Home folder.\nIn the blue menu on the left, click Workspace, and click the Home folder. The menu should look like this:\n\nClick the blue “Create” button in the top right corner, and then click “Git folder”:\n\n\n\nYou will first need to go to Azure DevOps and copy the link to clone the repo as you usually would\nPaste this link into the “Git repository URL” field\nUnder “Git Provider”, select “Azure DevOps Services”\nThe “Repository name” field should be automatically populated when you enter the URL.\n\nYou can then click “Create Git folder”. When the folder is created, you will be able to see it under your name in the Home folder.\n\n\nSparse checkout and Cone Patterns\n\nDatabricks cannot clone very large repos. It is best practice not to have a repo of this size, but if you attempt to clone a large repo, you will receive an error message. In this instance, you will need to perform a sparse checkout. This only clones a selection of items in your repo. To do this, select the “Advanced” option when in the “Add repo” menu, and tick the “Sparse checkout mode” option. You can tell Databricks which elements of the repo you would like it to clone, and you can do this by specifying something called Cone Patterns. If you do not specify any Cone Patterns, then the Databricks default is to clone only files in the root folder, but none from any subdirectories. To specify your own Cone Patterns, enter them in the box provided. You can enter multiple patterns but if the folders that they refer to contain more than 800MB of data then your clone will continue to fail.\nYou might define your Cone Patterns as something like:\nfolder_a folder_b/subfolder_c\nThe second example will only clone subfolder_c and not the rest of the contents of folder_b.\nPlease note: You cannot currently disable sparse checkout mode once it is enabled, but you can modify Cone Patterns. If you create a new folder, you must add it to your Cone Pattern list before you can commit and push. You can find more information on sparse checkout and Cone Patterns on the Databricks website.",
    "crumbs": [
      "Databricks Setup Guides",
      "Use Databricks with Git"
    ]
  },
  {
    "objectID": "ADA/git_databricks.html#working-in-repos-in-databricks",
    "href": "ADA/git_databricks.html#working-in-repos-in-databricks",
    "title": "Use Databricks with Git",
    "section": "Working in repos in Databricks",
    "text": "Working in repos in Databricks\n\nFolders in Databricks\n\nTo be able to add your notebooks to a repo, you need to make sure that you save them in the correct place. You will need to find the Git folder that you cloned previously, and work from there. You can see your Git folders underneath the Home section, like this:\n\nIn the screenshot above, you can see the analysts-guide repository with all of its subfolders.\nGit can see things inside the Git folders that you create in your Home area. This means that when you save something in a Git folder, it can be committed and pushed to Git. It is therefore good practice to always save notebooks in a Git folder.\n\n\n\nGit pull, commit, and push in Databricks\n\n\nGit pull\n\nYou can access the menu to pull, commit and push from several places within Databricks. This interface is the same whether you’re working with a DevOps repo or a Github repo.\nAt the top of any notebook that’s saved in a repo, you’ll see a little grey branch icon with the name of a repo next to it. In the case of this notebook, it’s on the main branch:\n\nYou can also access the Git menu by clicking the branch name next to the repo name within your personal folder, which you can access by selecting Workspace &gt; Users &gt; your email address in the left hand menu:\n\nIf you click the branch name, it’ll open the Git interface within Databricks:\n\nFrom here, you can perform Git pull by clicking the pull icon in the top right. There is a dropdown menu in the top left corner that allows you to change the branch or create a new branch if required.\n\n\n\nGit commit and push\n\nWhen you have made changes to a notebook, it will appear in the Changes section of the Git interface. You can also see the actual changes that have been made in the right hand box to make sure that you’re committing the correct file:\n\nIn Databricks, you commit and push as one action, rather than as two separate ones. Enter your commit message into the “Commit message” box (you can ignore the Description box) and click the “Commit & Push” button.\n\n\n\nAdditional Git commands in the Databricks interface\n\nYou can access additional Git features such as merge, rebase and reset directly within the Databricks interface by clicking the 3 dots in the menu as shown in the image below:\n\nWhen merging, you are also able to resolve merge conflicts inside Databricks itself.\n\n\n\n\n\n\nWarning\n\n\n\nPlease be warned that we would usually advise against using Git reset as you can permanently lose changes you’ve made and this cannot be undone.\nThere is additional guidance on each of these advanced features in the Databricks manual. Please use Git reset with caution as you can easily lose recent changes when performing this action.",
    "crumbs": [
      "Databricks Setup Guides",
      "Use Databricks with Git"
    ]
  },
  {
    "objectID": "ADA/databricks_notebooks.html#notebooks",
    "href": "ADA/databricks_notebooks.html#notebooks",
    "title": "Databricks notebooks",
    "section": "Notebooks",
    "text": "Notebooks\nNotebooks are a special kind of script that Databricks supports. They consist of code blocks and markdown blocks which can contain formatted text, links and images. Due to the ability to combine markdown with code they are very well suited to creating and documenting data pipelines, such as creating a core dataset that underpins your other products. They are particularly powerful when parameterised and used in conjuction with Workflows.\nYou can create a notebook in your workspace, either in a folder or a repository. To do this locate the folder / repository you want to create the notebook in then click the ‘Create’ button and select Notebook.\n\n\n\n\n\n\nTip\n\n\n\nAny notebooks used for core business processes are created in a repository linked to GitHub/DevOps where they can be version controlled.\n\n\nOnce you’ve created a notebook it will automatically be opened. Any changes you made are saved in real time so the notebook will always keep the latest version of its contents. In order to ‘save’ a snapshot of your work it is recommended to use Git commits.\nYou can change the title from ‘Untitled Notebook &lt;timestamp&gt;’ (1), and set its default language in the drop down immediately to the right of the notebook title (2).\n\nThe default language is the language the notebook will assume all code chunks are written in. In the screenshot above the default language is ‘R’, so all chunks will be assumed to be written in R unless otherwise specified.\nYou can also add markdown cells to add text, links and graphics to your notebook in order to document the processing done within it.\nTo add a new code of markdown chunk move the mouse above or below another chunk and the buttons ‘+Code’ and ‘+Text’ will appear.\n\nTo run code chunks you’ll first need to attach your compute resource to it by clicking the ‘Connect’ button in the top right hand side of the page.\n\nYou can run a code chunk either by pressing the play button in the top left corner of the chunk, or by pressing Ctrl + Return/Enter on the keyboard. Any outputs that result from the code will be displayed underneath the chunk.\n\n\n\n\n\n\nIn R\n\n\n\nIf you try to View() a data frame in R you’ll notice that the function doesn’t work within Databricks. Instead Databricks providers the display() function for R users to view their data with.\n\n\n\nEverything ran in a notebook is in it’s own ‘session’ meaning that later chunks have access to variables, functions, etc. that were defined above. Chunks can be ran manually, however doing this runs the risk of running code out of order and may consequently produce unexpected results. To avoid this all chunks can be ran in order from the beginning of the Notebook using the ‘Run all’ button at the top of the page, alternatively you can ‘Run all above’ or ‘Run all below’ from any code chunk.\nNotebooks cannot share a session with another Notebook so bear this in mind when constructing your workflows. If you need to pass data between notebooks it can be written out to a table in the unity catalog using SQL / R / Python / Scala as you would write to a SQL table in SQL Server. This can then be accessed from later notebooks.\nNotebooks can be parameterised using ‘widgets’, meaning a single notebook can be re-used with different inputs. This means they can be used in a similar way to a function in R/Python or a stored procedure in SQL.\n\n\n\n\n\n\nDon’t repeat yourself (DRY)\n\n\n\nA coding best-practice is to build components that can be re-used to perform many similar tasks rather than writing repetitive code.\nThis applies equally to notebooks or scripts you create within Databricks which can be made re-usable through parameters. To reference a parameter within a notebook you can use the syntax :parameter_name from Databricks Runtime 15+. In previous DBR versions the syntax was ${parameter_name}.",
    "crumbs": [
      "Learning resources",
      "Databricks notebooks"
    ]
  },
  {
    "objectID": "ADA/databricks_workflow_script_rstudio.html",
    "href": "ADA/databricks_workflow_script_rstudio.html",
    "title": "Script workflows in RStudio",
    "section": "",
    "text": "Important\n\n\n\nPlease be aware that the Databricks platform is regularly updated and may look different from the guidance included on this site. If you notice any discrepancies between the content on this site and the Databricks platform, please let us know by contacting statistics.development@education.gov.uk.\n\n\nWorkflows can be constructed through the Databricks Workflows user interface (UI), however for large or complex workflows the UI can be a time consuming way to build a workflow. In these scenarios it is quicker and more inline with RAP principles to script your workflow.\nFor a pipeline to be built there must be scripts, queries or notebooks available to read by Databricks, either located in your workspace, or in a Git repository.\nFor this example we will create a folder in our workspace, create two test notebooks to comprise the workflow, and then switch to RStudio to script the job and set it off running. We’ll also set it up to notify us by email when the workflow successfully completes.\n\nCreate a folder in your workspace on Databricks to store your notebooks. First click ‘Workspace’ in the sidebar (1), then navigate to your user folder (2). Then click the blue ‘Create’ button (3) and select ‘Folder’. Give the folder a name (4) such as ‘Test workflow’ and then click the blue ‘Create’ button (5).\n\n\nOnce in your new folder and click the blue ‘Create’ button again, this time choosing ‘Notebook’. Once in your new Notebook retitle it to ‘Test task 1’ (1), and set the default language to R (2). Then in the first code chunk write print(\"This is a test task\") (3).\n\nCreate a second workbook in the same folder titled ‘Test task 2’, and in the first code chunk write print(\"This is another test task\") .\n\nSwitch to RStudio and in a new script (/ project) install the devtools package if it isn’t already installed. Then use devtools::install_github() function to install the databricks package. Load the newly installed databricks package.\n\nif(!\"devtools\" %in% installed.packages()){\n  install.packages(\"devtools\")\n}\n\ndevtools::install_github(\"databrickslabs/databricks-sdk-r\")     \n\nlibrary(databricks)      \nlibrary(tidyverse)      \n\nUse the usethis::edit_r_environ() to edit you R environmental variables stored in the .Renviron file. If it doesn’t already exist create a variable called DATABRICKS_TOKEN and paste in your Databricks access token. Save the .Renviron file and close it. You’ll then need to restart your session so that the environmental variable is present.\n\nDATABRICKS_TOKEN=&lt;your-access-token&gt;\n\n\n\n\n\n\nDatabricks access token\n\n\n\nPersonal Authentication Token (PAT)s are a unique code that is generated to let Databricks know who is accessing it from the outside. It functions as a password and so must not be shared with anyone.\nIf you haven’t already generated a Databricks token you can find instructions on how to do so in the Setup Databricks personal compute cluster with RStudio article.\n\n\n\n\n\n\n\n\nDon’t put your token into the code\n\n\n\nThe reason we’re using the .Renviron file here for your access token is that we don’t want to take any risk of someone else being able to view your PAT token. If we were to hardcode it into the script then anyone with access to the code would be able to copy your PAT token and ‘masquerade’ as you.\n\n\n\nWe can now connect to the API through the databricks package using the databricks::DatabricksClient() function. It requires the host which is the URL of the Databricks platform up until (and including) the first /, and your token. We’ll store the result in a variable called client as we need to pass this to the other functions in the databricks library.\nWe can then use the databricks::clustersList() function to fetch a list of the clusters, which we can view using View().\nhost &lt;- \"https://adb-5037484389568426.6.azuredatabricks.net/\" \n\napi_token &lt;- dbutils.widgets.get(\"api_token\")  \n\nclient &lt;- databricks::DatabricksClient(host = host, token = api_token)  \n\nclusters &lt;- databricks::clustersList(client)  \n\nView(clusters %&gt;% \n          select(cluster_id, creator_user_name, cluster_name, \n                 spark_version, cluster_cores, cluster_memory_mb)\n        )\n\nClusters\nThe databricks::clustersList() function will return any clusters that you have permission to see.\nThe data returned by the function is hierarchical, and a single ‘column’ may contain several other columns. The View() function renders a table and as a result flattens the structurein order to see how it is originally structured you can run str(databricks::clustersList(client)).\nMake a note of your cluster ID and save it in a variable called cluster_id. You could automate this step by filtering the clusters data frame as long as you ensure that it only results a single cluster_id.\n  cluster_id &lt;- \"&lt;your cluster id&gt;\"\nWe can now start creating some parameters for the job. Firstly we’ll need a job_name, and the paths to the Notebooks we’re wanting to include in the workflow. We’ll also need to create a unique task_key for each of the Notebook tasks we’re going to set up.\njob_name &lt;- \"test job\" \n\nfirst_notebook_path &lt;- \"/Users/nicholas.treece@education.gov.uk/R SDK/Test Notebook\" \nsecond_notebook_path &lt;- \"/Users/nicholas.treece@education.gov.uk/R SDK/Test Notebook 2\" \n\ntask_key_1 &lt;- \"test_key\" \ntask_key_2 &lt;- \"test_key_2\"\nWe can then define the tasks as lists. There are many options available available for setting when creating a task, a full list of which can be found in the tasks section of the job API documentation. When reading this documentation any parameter that is marked as an object needs to be passed as a list (list()) in R, and anything marked as an array should be passed as a vector (c()).\nFor the first task we’ll give it the first task_key we created above, and tell it to run on our existing cluster by passing the ID of our cluster to existing_cluster_id, we’ll then specify that it is a notebook_task and pass that a list with the notebook_path and the source which we will set to WORKSPACE (as opposed to a Git repository) for the purposes of this tutorial.\n\nfirst_job_task &lt;- list(\n task_key = task_key_1,\n existing_cluster_id = cluster_id,\n notebook_task = list(\n                  notebook_path = first_notebook_path,\n                  source = \"WORKSPACE\"\n                  )\n )\nFor the second task we will do the same, but using the second task_key and notebook_path we defined. In addition, we’ll also add a depends_on clause with the previous task_key (passed in a list), and specify it is only to run_if ALL_SUCCESS. This means that the second task won’t begin processing unless all of the tasks it depends_on have completed successfully.\nsecond_job_task &lt;- list(\n  task_key = task_key_2,\n  existing_cluster_id = cluster_id,\n  notebook_task = list(\n                  notebook_path = second_notebook_path,\n                  source = \"WORKSPACE\"\n                  ),\n  depends_on = list(task_key = task_key_1),\n  run_if = \"ALL_SUCCESS\"\n)\n\nNow we have both of our tasks defined we can create the job using the databricks::jobCreate() function. We pass it the client as the first argument, then the job name we defined. The tasks are passed as a list which contains each of the task lists we built above.\nWe’ll also tell it to send us email_notifications by passing a list with an on_success value of email addresses.\n\nThe function returns the ID of the job we just created, so we will want to store the response in a variable called workflow so we can refer to it later.\n\nworkflow &lt;- jobsCreate(client,\n                       name = job_name,\n                       tasks = list(first_job_task, #list\n                                    second_job_task), #list\n                       email_notifications = list(\n                         on_success = c(\"your-email\")\n                         )\n                       )\n\n\n\n\n\n\nLists of lists\n\n\n\nA list() in R is used to contain any number and type of data, including other list()s. This makes it excellent for storing hierarchical data in one place, however it can get quite confusing quite quickly.\nSometimes it’s easier to break these lists() up into pieces by defining them seperately, as we did above by defining the task lists separately then passing them to the tasks argument in the jobsCreate() function.\nThis often makes it easier to think about and construct, but certainly makes it easier to read. Consider the code below which does exactly the same thing as the code above, but is just written all at once.\nworkflow &lt;- jobsCreate(client,\n                       name = job_name,\n                       tasks = list(\n                         list(\n                           task_key = task_key_1,\n                           existing_cluster_id = cluster_id,\n                           notebook_task = list(\n                             notebook_path = first_notebook_path,\n                             source = \"WORKSPACE\"\n                             )\n                           ),\n                         list(\n                           task_key = task_key_2,\n                           existing_cluster_id = cluster_id,\n                           notebook_task = list(\n                             notebook_path = second_notebook_path,\n                             source = \"WORKSPACE\"\n                             ),\n                           depends_on = list(task_key = task_key_1),\n                           run_if = \"ALL_SUCCESS\"\n                           )\n                         ),\n                       email_notifications = list(\n                         on_success = c(\"your-email\")\n                         )\n                       )\nWe can see here that the code is getting very long, and is also more difficult to see which options relate to which list. If it weren’t for being diligent with indentation here we’d have to resort to counting brackets to see what belonged where. This is especially problematic if you accidentally delete a bracket and need to work out where it was meant to go.\n\n\n\nWe can now get the ID of the job that was created and tell the API to run the job. In a new code chunk we’ll store the job_id from the workflow variable above. We’ll then use the databricks::jobsRunNow() function to tell it to run the workflow we just created by passing it the job_id we just stored. We’ll also store the job_run_id returned by the databricks::jobsRunNow() function.\n\njob_id &lt;- workflow$job_id  \njob_run &lt;- jobsRunNow(client,\n                      job_id = job_id)\njob_run_id &lt;- job_run$run_id\nWe will now use this to create links to the job and the specific run of the job we just set off.\n\nDefine a job_link by paste0()ing the host variable we passed to the databricks::DatabricksClient() function earlier, followed by \"job/\" followed by the job_id defined above.\nWe can then create a job_run_link by paste0()ing together the job_link followed by \"/runs/\" then the job_run_id from the previous step.\nWe can then output the job_link as text at the bottom of the cell.\njob_link &lt;- paste0(host,\"jobs/\",job_id)\njob_run_link &lt;- paste0(job_link,\"/runs/\", job_run_id)\nYou’ve now created a workflow with code, and each time you re-run this scriptanother workflow with the same name will be created. As this is a tutorial which most analysts may have to follow at some point, the logical conclusion is that we will end up with hundreds of ‘test jobs’ cluttering up the workflow page.\n\nTo avoid that let’s use the databricks::jobsDelete() function to clean up after ourselves. All that we need to do is pass the function the client, and job_id variables from above.\njobsDelete(client, job_id)\n\n\n\n\n\n\nNote\n\n\n\nIf you have been running and re-running bits of this code iteratively, there’s a good chance you already have several instances of ‘test job’ listed under your name on the ’Workflows` page in Databricks.\n\nIf this is the case we’ll want to clean up each of these, ideally without having to manually click through the UI process for each one.\nTo do this, firstly call the databricks::jobsList() function, passing it the client variable, and specifying the name of the jobs you want to list. Then filter the list to just the jobs with a creator_user_name of your email address. To see the resulting jobs use the View() function as below at the bottom of a code chunk.\nmy_jobs &lt;- jobsList(client,\n                    name = \"test job\") %&gt;%\n  filter(creator_user_name == 'your-email')\n\nView(\n  my_jobs %&gt;% \n    select(job_id, creator_user_name, run_as_user_name, created_time)\n)\nWe can now loop through the individual job_ids contained in my_jobs and use the databricks::jobsDelete() function to remove them all at once, programmatically.\nfor(job_id in my_jobs$job_id){\n  jobsDelete(client, job_id)\n}\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Databricks Setup Guides",
      "Script workflows in RStudio"
    ]
  },
  {
    "objectID": "statistics-production/examples.html",
    "href": "statistics-production/examples.html",
    "title": "Good examples in EES",
    "section": "",
    "text": "Examples of good practice in EES, from file names to content",
    "crumbs": [
      "Statistics production",
      "Good examples in EES"
    ]
  },
  {
    "objectID": "statistics-production/examples.html#files",
    "href": "statistics-production/examples.html#files",
    "title": "Good examples in EES",
    "section": "Files",
    "text": "Files\nFiles can be downloaded from multiple areas across the platform and often users will judge their contents based on the name alone. For display names, it’s important we make it as clear as possible to users as to what each file contains without cluttering the title with information that is already available from the page around it, saving detailed coverage for the data guidance.\n\n\nFile names\n\nFull guidance for naming files.\nThe Education and training statistics for the UK release has some good examples of file names which clearly explain what is in the file, while falling well under 35-50 characters in length, e.g.:\n\nuk_schools.csv\nuk_expenditure.csv\nuk_pupils.csv\n\nThe file names do not have time periods in them, making them easier for users to make use of newer versions in future\n\n\n\nSubject names\n\nFull guidance for naming subject files.\nThe School Workforce Census applies this guidance well, with short and simple titles that make sense to a non-expert, e.g.:\n\nTeacher pay\nTeacher retention\nSubjects taught\n\nThese titles are not cluttered with geographies and time periods, which automatically populate in the subject metadata.\n\n\n\nSupporting files\n\nFull guidance for using supporting files in EES.\nThe Initial Teacher Training performance profiles publication is one example of needing to upload a file outside of the regular data/metadata structure. Their provider-level tables also contain national and regional data, which is not supported in EES at the time of writing. The file has a clear title, and data guidance clearly explains what is in the file.",
    "crumbs": [
      "Statistics production",
      "Good examples in EES"
    ]
  },
  {
    "objectID": "statistics-production/examples.html#data-guidance",
    "href": "statistics-production/examples.html#data-guidance",
    "title": "Good examples in EES",
    "section": "Data guidance",
    "text": "Data guidance\nThe public data guidance is a key element of your release, to help direct users to the right file to download or build tables with. It is frequently highlighted by users of our stats as a really helpful element of EES. You will be prompted to fill in data guidance in the release checklist. Any release that does not have a complete public data guidance will not be blocked from approval and publishing.\n\n\nPublic data guidance - files\n\nFull guidance for public data guidance.\nThe School Workforce Census, along with having sensible brief file names, also has great examples of useful data guidance, which explain what is in each file without overloading the user with information:\n\n\n\n\nPublic data guidance - overview\n\nFull guidance for public data guidance.\nThe NEET annual brief is a good example of a public metadata overview section done well. It follows the EES template, goes into just the right amount of detail, and is easy to understand.\n\nNEET data guidance on EES.",
    "crumbs": [
      "Statistics production",
      "Good examples in EES"
    ]
  },
  {
    "objectID": "statistics-production/examples.html#charts-and-tables",
    "href": "statistics-production/examples.html#charts-and-tables",
    "title": "Good examples in EES",
    "section": "Charts and tables",
    "text": "Charts and tables\nCharts and tables are key ways to highlight important information and stories in your release. They should be clear, contain few footnotes that are essential to interpreting the chart or table, and have good descriptive alternative text so users with screen readers can understand what is being visualised.\n\n\nOther infographics\n\nFull guidance on using charts built outside of EES.\nThe Further Education: outcome based success measures release contains infographics that cannot be built in EES. They are saved as SVGs so render clearly on any device, and follows the same colour palette as the rest of the release.\n\n\n\n\nCharts and footnotes\n\nFull guidance on creating charts, and guidance on creating footnotes.\nThe Widening participation in higher education publication has a good example of a chart built with a long timeseries in the chart builder, with sparing use of footnotes. For example below, a line in the chart indicates a point where comparisons cannot be made, and a key caveat around the comparability of years is included as a footnote, which is needed to interpret the data and chart accurately.\n\n\n\n\nAccessibility and alt-text\n\nGood alt-text descriptions in charts will not just repeat the title of the chart, but instead describe the type of chart, what the data coverage is, and what trends can be seen in the chart to a user accessing the page with a screenreader.\nThe school workforce census has a good example of alt-text for one of their charts. The associated alt-text for the chart below is: “Line chart showing the percentage of all teachers taking absence across all state funded schools in England between the academic years 2014/15 and 2018/19. The chart shows a fall from 55.7% to 54.0% in this period.”\n\n\n\n\nFeatured tables\n\nFull guidance on creating featured tables.\nGood featured tables have short, easy to understand titles, and point users to commonly requested tables or other tables that might be interesting to a wide range of users. For example, the pupil absence in schools: autumn term release has a good example of a clear title, with further information (including date and geographic coverage) left in the description. Leaving these out of the main title makes it much easier for users to navigate through multiple featured tables:",
    "crumbs": [
      "Statistics production",
      "Good examples in EES"
    ]
  },
  {
    "objectID": "statistics-production/examples.html#content",
    "href": "statistics-production/examples.html#content",
    "title": "Good examples in EES",
    "section": "Content",
    "text": "Content\nClear, concise content is required to direct your users to the right place, and keep them engaged with your release. Our content guidance page contains a raft of great advice on how to structure your release and write for the general public.\n\n\nHeadlines\n\nFull guidance on creating a headline section\nThe Education, health and care plans release has a good example of a headlines section. There are a sensible number of key statistics, and the summary below gives an overview of trends without diving into the numbers and overwhelming the user.\n\nThe Exclusions release has a good example of custom explanations under key stat tiles to show users exactly what the numbers are describing:\n\n\n\n\nAccordion content\n\nFull guidance on writing content for accordions\nThe parental responsibility measures release has good examples of writing for the public in accordion content. Content in each accordion is short and follows the “pyramid” principle of having essential information at the top, summarising the trend, then going into detail at the bottom.\n\n\n\nActive subheadings\n\nFull guidance on using active headings and titles\nThe EHCP release has some great examples of active subheadings within accordions, which explain the overall trends without overloading users with detail.\n\n\n\nGlossary links\n\nThe summary of the CIN / CLA outcomes release has examples of this in action.\n\n\n\nMethodology\n\nFull guidance on creating a methodology in EES.\nGood methodologies will broadly cover topics in our recommended methodology template. The Graduate outcomes (LEO) methodology broadly follows this outline, making use of effective formatting in EES to help users navigate through accordions, although definitions can be moved to the EES glossary.\nThe Key stage 4 destinations methodology makes good use of annexes to show changes to their methodology over time.",
    "crumbs": [
      "Statistics production",
      "Good examples in EES"
    ]
  },
  {
    "objectID": "statistics-production/examples.html#approvals-and-amendments",
    "href": "statistics-production/examples.html#approvals-and-amendments",
    "title": "Good examples in EES",
    "section": "Approvals and amendments",
    "text": "Approvals and amendments\nKeeping track of processes for approving and amending releases is crucial for transparency. Internally, there should be clear paper trail of who has signed off the release, and externally, users need to know if anything in the release has changed from the last time they saw it.\n\n\nPublic amendment notes\n\nFull guidance for creating public amendment notes.\nThe Pupil absence in England - Autumn and Spring terms publication has some good examples of release notes.\nThese notes clearly explain to the user what has changed, which files were affected and at what level:\n\n\n\n\nInternal release status notes\n\nFull guidance for creating public amendment notes.\nThe COVID attendance publication has fortnightly releases at the time of writing, and the team have a good process for sign-off. This includes detailed information at each stage to confirm who has signed off, who has requested an action be taken, and who has carried out the action.",
    "crumbs": [
      "Statistics production",
      "Good examples in EES"
    ]
  },
  {
    "objectID": "statistics-production/ud.html",
    "href": "statistics-production/ud.html",
    "title": "Open Data Standards",
    "section": "",
    "text": "Guidance on how to structure data files",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#introduction",
    "href": "statistics-production/ud.html#introduction",
    "title": "Open Data Standards",
    "section": "Introduction",
    "text": "Introduction\nWith open data standards, our aim is to apply a consistent, logical structure to all data files so that they are easier to use and analyse, minimising the time spent deciphering and cleaning the data. Adopting these principles will give us more power to serve the needs of the users, saving both us and them time when producing and using our data, as well as opening up further opportunities for linking data.\nThe guidance and standards here explicitly apply to all data files prepared for and uploaded to the explore education statistics service, but much of the content below equally applies to any data being prepared and disseminated both within the department and externally.\nData published via EES is released under the terms of the Open Government License and by following the data standards provided here, published (or any other) data meets at least 3 stars for Open Data.\n\n\nHow to check against these standards\n\nAn interactive data screener has been developed in R Shiny to automate checks against the standards as a final stage of automated quality assurance before upload to EES.\nThis can be run on any data file, though requires an associated EES metadata file to be able to process the file. The app runs on our Posit Connect (formerly RSConnect) servers, and is only available when using DfE kit. The app is mostly self-explanatory, though if you have any questions about it, or are curious to know more about how it works, the code is available on GitHub, and you can get in touch with us at explore.statistics@education.gov.uk.\n\n\n\n\n\n\nImportant\n\n\n\nAll data and EES metadata files must be run through the screening app before uploading to EES.\n\n\n\n\n“Tidy datasets are all alike but every messy dataset is messy in its own way.” – Hadley Wickham\n\n\n\n\nOverview of EES data files\n\nFor data to be used with the table tool and charts in EES, it needs to meet the following overall specifications:\n\nThe data should be formatted as a comma separated values (CSV) file.\nThe first row of the data file should contain machine readable column names in snake case.\nThe data should be layed out in line with tidy data principles, consisting of filters (category fields) and indicators (value fields).\nThe file should contain the necessary mandatory columns (i.e. time_period, time_identifier, geographical_level, country_code and country_name);\nThe data file should have an accompanying metadata CSV file, which contains information on the nature of the columns in the data file:\n\nwhether a given column is a filter or indicator;\nhuman readable name for use in tables and charts on EES;\nfilter grouping information;\nnumber of decimal places to display for indicator fields (i.e. allowing a lower precision to be presented in the dervied tables than the underlying data - useful for minimising rounding errors in aggregates);\nunits for indicator fields (e.g. £, %);\n\nThe data should use the appropriate GSS codes for suppressed, low, not available and not applicable entries.\nThe data field IDs, labels and items should conform to DfE harmonised variables where available.\n\nAn example pair of data and metadata files are illustrated in the files and tables below.\n\n\n\nExample data file (ees_demo_datafile.csv)\n\n\n\n\n\n\n\nNote\n\n\n\nThe mandatory columns time_identifier, geographic_level and country_code are abridged in the table below to help with displaying in a web page, but are shown in the example file at the link above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\n…\ncountry_name\nregion_code\nregion_name\nsex\nschool_phase\nchildren_count\nchildren_percent\n\n\n\n\n202021\n…\nEngland\n\n\nTotal\nTotal\n1000\n100.000\n\n\n202021\n…\nEngland\n\n\nMale\nTotal\n490\n49.000\n\n\n202021\n…\nEngland\n\n\nFemale\nTotal\n510\n51.000\n\n\n202021\n…\nEngland\n\n\nTotal\nPrimary\n250\n100.000\n\n\n202021\n…\nEngland\n\n\nMale\nPrimary\n131\n52.400\n\n\n202021\n…\nEngland\n\n\nFemale\nPrimary\n119\n47.600\n\n\n202021\n…\nEngland\nE12000001\nNorth East\nTotal\nTotal\n100\n100.000\n\n\n202021\n…\nEngland\nE12000001\nNorth East\nMale\nTotal\n32\n32.000\n\n\n202021\n…\nEngland\nE12000001\nNorth East\nFemale\nTotal\n64\n64.000\n\n\n202021\n…\nEngland\nE12000001\nNorth East\nTotal\nPrimary\n43\n100.000\n\n\n202021\n…\nEngland\nE12000001\nNorth East\nMale\nPrimary\n12\n27.907\n\n\n202021\n…\nEngland\nE12000001\nNorth East\nFemale\nPrimary\n31\n72.093\n\n\n201920\n…\nEngland\n\n\nTotal\nTotal\n956\n100.000\n\n\n201920\n…\nEngland\n\n\nMale\nTotal\n444\n46.444\n\n\n201920\n…\nEngland\n\n\nFemale\nTotal\n512\n53.556\n\n\n\n\n\n\n\nExample metadata file (ees_demo_datafile.meta.csv)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol_name\ncol_type\nlabel\nindicator_grouping\nindicator_unit\nindicator_dp\nfilter_hint\nfilter_grouping_column\n\n\n\n\nsex\nFilter\nSex\n\n\n\nFilter by pupil sex\n\n\n\nschool_phase\nFilter\nSchool phase\n\n\n\nFilter by the phase of the school\n\n\n\nchildren_count\nIndicator\nNumber of children\n\n\n\n\n\n\n\nchildren_percent\nIndicator\nPercentage of children\n\n%\n1\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the children_percent column, the underlying data is provided to 3 d.p., but the meta data constrains it to 1 d.p. This means that figures in tables in the publication will be presented to 1 d.p., but users will have access to the higher accuracy in the underlying data. As well as allowing EES to meet different users’ needs, this also helps lower the risk of rounding errors in the underlying data creating unwanted behaviour in charts in EES.\n\n\nFurther information on all of the requirements for appropriately prepared data files follow in the sections below.\nOnce you have prepared a draft data file, you should always run the file through our EES data file screener. This will check for common issues that may prevent the file from being used appropriately by EES. Note that some issues may not prevent your file from uploading to EES, but would still cause undesired behaviour once on the platform, so it is imperative to screen data files before uploading.",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#general-requirements",
    "href": "statistics-production/ud.html#general-requirements",
    "title": "Open Data Standards",
    "section": "General requirements",
    "text": "General requirements\nWhen publishing statistics, you should be following these standards for underlying data files.\nFor publishing on EES specifically, please note the following points:\n\nData files uploaded to the explore education statistics will be downloadable, everything in your file must be publishable.\nYou must remember to apply all of your usual suppression policies.\nThe data files run the table and chart tools, so all data that you wish to create a table or chart with must be included.\nWe expect that multiple data files may be provided for each release.\nKeep in mind that a data block can only have a single data source. However, many blocks can share one source.\nYou should include all years of the data you available in your data file to facilitate time series analysis.\nEach data file must be accompanied by a corresponding CSV metadata file as outlined under EES metadata.\n\n\n\nTidy data structure\n\nThe DfE is committed to working with and publishing standardised ‘tidy’ data to give users, both internal and external, consistent machine readable data that can be easily transformed and analysed in modern programming languages used for data processing. Our standards draw upon the ideas of tidy data - this means applying a logical structure to datasets so they are easier to use and analyse, minimising the time spent cleaning the data before use.\nHere is a quick summary video of what exactly tidy data is.\n\n\n\n\n\nFurther details on tidy data, can be found by reading Hadley Wickham’s academic paper on Tidy Data. The key principles to remember are:\n\nEach variable forms a separate column.\nEach observation forms a separate row.\n\nThe variables (columns) in each of the uploaded data files will fall in to the following two categories: filters and indicators.\n\n\nIntroduction to indicators\n\nIndicators are the measureables in any data set. They should be grouped based on the type of measurement, with a column for each different type of measurement. For example, Number of pupils and Percentage of pupils would be two disticnt indicator columns in a data file.\nMore details on indicators in the EES context are provided in the Indicators section below.\n\n\n\nIntroduction to filters\n\nA single filter column should contain all the possible filter values for a single data-sub-aggregation. For example, many publications would have an ethnicity_major column containing all the major ethnic breakdowns contained in the data or an fsm column containing the entries FSM and non-FSM.\nIn general, analysts should use a separate column for each filter in accordance with tidy data principles. This is especially the case where data are presented for combinations of filters (i.e. cross tabulations). User testing has shown this to be the most effective way to structure data for the best user experience with the table tool.\n\n\n\n…\nfsm_status\nsex\npupil_count\n\n\n\n\n…\nTotal\nTotal\n1209\n\n\n…\nTotal\nFemale\n567\n\n\n…\nTotal\nMale\n642\n\n\n…\nFSM\nTotal\n406\n\n\n…\nFSM\nFemale\n203\n\n\n…\nFSM\nMale\n203\n\n\n…\nnon-FSM\nTotal\n803\n\n\n…\nnon-FSM\nFemale\n364\n\n\n…\nnon-FSM\nMale\n439\n\n\n\nWhere data is broken down across combinations of different filters, teams should aim to “complete the matrix”. This means that, for the given filters, all possible filter combinations should have a corresponding data entry. In this way, teams can prevent users getting the ambiguous “No data” result from EES and can instead provide more explicit codes for any missing data (e.g. not applicable, not available, suppressed, etc)\nA possible exception to the above structure is where no filter combinations/cross-tabulations are present in a given data file. For example, this may be the case if a publication requires a highlights level table that shows a result across breakdowns of sex (Male, Female, etc) and Free School Meal status (FSM, non-FSM), but not combinations of the two (Female and FSM, Male and FSM, Female and non-FSM and Male and non-FSM). In this case, analysts may choose to use a overarching collated filter columns named breakdown_topic and breakdown as follows:\n\n\n\n…\nbreakdown_topic\nbreakdown\npupil_count\n\n\n\n\n…\nTotal\nTotal\n1209\n\n\n…\nSex\nFemale\n567\n\n\n…\nSex\nMale\n642\n\n\n…\nFSM status\nFSM\n406\n\n\n…\nFSM status\nnon-FSM\n803\n\n\n\nWith the above structure, breakdown_topic should be added as the fitler_grouping_column for breakdown in the associated meta data file (and therefore should not have its own row in the meta data).\nTo re-iterate, teams should not use the above format when filter combinations/cross-tabulations are present in the data file they are producing, so breakdown_topic and breakdown should not contain entries such as “Sex and FSM” or “Female and FSM” respectively.\nFilters come in two types: standard filters and additional filters.\nThe standard filters encompass time and geography elements (e.g. time_period, geographic_level, la_code). Specific combinations of these standard filters must be present in your data files and the contents of these filters are required to meet specific standards in order for a data file to be compatible with the table tool in EES.\nAdditional filters are the release specific characteristics that we filter our data on, e.g. school types, learner characteristics, grade thresholds, etc. Some of these filters have recommended column names and entries in order to support consistency in data files across publications. For example ethnicities should have column names of ethnicity_major, ethnicity_minor or minority_ethnic and contents should be limited to the GSS standards. Such guidelines are outlined in the Common harmonised variables section of this page, whilst further information on fiters in the EES context is given in the Filters section below.\n\n\n\nOptimising filter-indicator combinations\n\nThe policy of creating tidy data files effectively means optimising your filter-indicator combinations for use within the EES user interface. By doing so, end users will be better able to interact with your data and find the information that they’re looking for.\nThe video below provides some context around this and shows how tidy data structures work better in the EES table tool when compared to wide (or pivoted) data sets.\n\n\n\n\nThe number of indicators should be kept to a minimum, whilst maintaining different types of measurements as distinct indicators. For example a wide structure might consist of something like the following:\n\n\n\n\n\n\n\n\n\n\n\n…\npupil_count_passing_95\npupil_count_passing_94\npupil_percent_passing_95\npupil_percent_passing_94\n\n\n\n\n…\n567\n642\n45.7\n51.8\n\n\n\n\nCreating a tidy form of this data would look something more like this:\n\n\n\n\n\n\n\n\n\n…\ngrade_range\npupil_count\npupil_percent\n\n\n\n\n…\n9 to 5\n567\n45.7\n\n\n…\n9 to 4\n642\n51.8\n\n\n\nThis is a simplified example and your data will likely be more complex, but in making this type of change, you may be able to better identify more optimal ways of organising your data. For example, if you find that restructuring like this creates a lot of empty cells, it may be that the data has incompatible filters and can be separated out into multiple data files.\nThe next video illustrates the differences between wide and tidy data, showing examples of the same data organised in each structure.\n\n\n\n\nIf your current processes produce wide data that you need to switch to a tidy structure, one of doing this is using the pivot_longer() function in R. The following video demonstrates how to do that using the data shown in the previous videos.\n\n\n\n\n\n\n\n\nData format\n\nThese standards give you the power to format the data in a way that best meets the needs of the users. There are only a handful of formatting standards to follow to ensure best practice and consistency across all of our data.\n\n\n\n\n\n\nWarning\n\n\n\nData files must in comma separated values (CSV) format, and use UTF-8 encoding. You can specify this when saving the file in Excel, or exporting from elsewhere.\n\n\nIf you need to use commas within a cell, then you must add a text delimiter such as quotes to your file to define each cell - this is often done automatically for you, though if you’re unsure then you can open up your CSV file in a text editor such as Notepad to check.\nComma separated values simply means that if look at the raw file, each column is separated by a comma, other forms of delimination like this also exist, such as TSV files that are ‘tab’ separated. Next time you have a minute, try opening up your CSV data files in Notepad, to see how they look behind the scenes!\nSaving files in CSV format should be a standard option from any analytical tool, though sometimes you need to watch out for how they are encoding special characters, for more information see the section on how to export data with UTF8 encoding.\nYou should also ensure that your data follows the GSS Standards on symbols, though be aware to ignore the ask that symbols are included in separate cells from the data, which is unpractical and unrealistic.\n\n\n\nFile names\n\nFile names should only include numbers, letters, underscores or hyphens. Special characters must be avoided; for example, the following characters \\ / : * ? \" &lt; &gt; | [ ] & $ , . + are all considered special characters and are used for specific tasks in an electronic environment, which can lead to confusion in some systems. The use of non-English language letters such as á, í, ñ, è, and õ, should also be avoided.\nFile name should ideally be no more than 35-50 characters, file names that try to give too much information end up having the reverse affect and users skim over and get less value than from a more concise name. File names that extend beyond 200 characters will likely cause issues for users using the files in other programs.\nThe metadata file should have exactly the same name as the data file, with a suffix of ‘.meta’. E.g. mydatafile.csv and mydatafile.meta.csv.\nYou should avoid references to time periods in the file name as this information is shown elsewhere and this can make it harder for users to make use of the newer versions of files in future years. File names should be recyclable year on year.\nIn general you should avoid including the geographic level in the file name, unless it is a file that is specifically different (e.g. a file for school level data only).\n\n\n\n\n\n\nWarning\n\n\n\nFor use with EES all file names should be in lower case and avoid special characters or spaces. Any upper case characters in file names will be forced to lower case by EES, and will appear as lower case to the users.\n\n\n\n\n\nVariable names\n\nVariable names must be in the first row of your file as the header row, must not include spaces, and ideally be formatted in snake_case for ease of use.\nAvoid starting variable names with a numeric character.\nAs with file names, you should avoid any special characters; for example, the following characters \\ / : * ? \" &lt; &gt; | [ ] & $ , . + are all considered special characters and are used for specific tasks in an electronic environment, which can lead to confusion in some systems. The use of non-English language letters such as á, í, ñ, è, and õ, should also be avoided.\nVariable names should ideally be kept below 25-35 characters as long names are often cut off when viewing the data file and generally fail to get the information required across to users. It is a balance between giving enough information so it’s clear what it refers to and giving so much that it’s unhelpful. Remember to make use of your public data guidance and methodology for expanding on details.\nTitles should use abbreviations only when necessary to reduce the length of the title if required.\n\n\nIndicator names\n\nMost indicators should be reducible to a simple context / title (e.g. schools, pupils, students, teachers, starts, apprenticeships, expenditure, income, etc) and a data type (e.g. count, sum, percent, score, average, median, fte, etc). Assuming this ideal (tidy data structure) case, the preferred layout is:\n{title}_{data type}\nFor example:\nstarts_count, starts_sum, starts_percent, starts_average, starts_median, absence_percent, pupils_count, pupils_percent\nWhilst data producers should generally aim to fit the basic layout above (i.e. by using filters to cover categorization and limiting criteria), there may be circumstances in which additional flags need to be included that can’t be placed in Filters. In these circumstances, the guidance is to follow the below ordering:\n{title}{levels}{above / below}{exclusivity}{additional}_{data type}\nIf your data doesn’t appear to fit with just using the basic {title}_{data type} column naming format and you’d like to use the extended structure above, then please get in touch and we can work through how the extended structure can work with your data.\nThe table below summarises these guidelines.\n\n\n\n\n\n\n\n\n\n\nName\nIndividual form\nFull indicator example\nDescription\n\n\n\n\nTitle\ntitle / name e.g. population, pupils, starters\npopulation_count, pupil_count, starter_count\ntitle of the field, avoid abbreviations where possible\n\n\nData type\ncount / percent\npupil_count, pupil_percent\nNumber or Percentage where applicable\n\n\nLevels\nl + (level number)\npopulation_count_l1\nl1, l2, l3 etc\n\n\nAbove or Below\nplus / minus\npopulation_count_plus\nUsing ‘plus’ or ‘minus’ to denote above or below\n\n\nExclusivity\nexc / inc\npopulation_count_exc_adult\nexcludes or includes features\n\n\nAdditional\nFurther sub-identifiers should be included as filters rather than in indicator field names wherever possible\n\n(e.g. male / female, English / maths, etc)\n\n\n\n\nThrough the above guidance, we aim to develop the DfE data catalogue into a consistent and predictable collection of data entries that anyone switching between different data files within the same publication or across different publications would more easily be able to navigate. As part of this, publication teams should regularly review their data files against this guidance and as outlined in the Reviewing indicator and filter field naming section\n\n\n\nHow to export data with UTF-8 encoding\n\nMost of the time our data is exported as a CSV file it will have UTF-8 encoding by default. However, there are times when this isn’t the case, and therefore we’ll quickly run through how to check this below in each of Excel, SQL, and R.\nExcel\nExcel tends to save all CSV files as UTF-8, however this is not always the case, particularly if there are symbols in the file (such as £). To ensure that it saves with UTF-8 encoding you can select the following when saving a file:\nFile &gt; Save As &gt; CSV UTF-8 (Comma delimited) (*.csv)\nSQL\nFor saving results out of SQL as a CSV file there isn’t an option to specify the encoding, therefore the best bet is to either open the file in Excel and specify that as above, or to run your SQL query/read your data into R and follow the guidance below.\nR\nWhen writing CSV files out of R, you’ll mostly likely be using either write.csv() from base R, or write_csv() from the readr package. For the first one, you can specify encoding using fileEncoding = like the following example:\nwrite.csv(my_data, file = \"my_data_file.csv\", fileEncoding = \"UTF-8\")\nFor write_csv(), which some of you may be using for increased processing speed, the function automatically encodes as UTF-8 format, meaning that you don’t have to do anything different!\n\n\n\n\nHow much data to publish\n\nYou should publish as many years of data that you have and is practicable.\nIf you are not providing a full timeseries for any reason, you must link to the older published data from your publication release page, and make sure that it’s omission is explained in your methodology and metadata documents.\n\n\n\nDeciding what should be in a file\n\nExplore education statistics is designed to give production teams the freedom of controlling what data users can access, and how they access it. It is expected that most releases on the platform will have multiple data files, and teams have control over how they break these files up.\nThe first key consideration is that the table tool will only create tables from a single a data file, and cannot use multiple files as sources. Therefore any data that you want to compare within a single table must in the same data file. The table tool itself is there to allow users to narrow down the amount of data they have to absorb and to be able to efficiently take away key statistics.\nA useful way to judge how to break up data files is to consider whether all of the data in the file is appropriate to show side-by-side in the same table. If there are data that are conceptually different or may be confusing to compare side by side, then these should be in separate data files. Any data file uploaded to EES is usable by all users in the table tool, and users will be able to download the exact same files as you upload.\nWe generally recommend fewer large files over a larger number of smaller files. If you think you are having issues with file size please tell us so that we can investigate and work towards a solution with you.\n\n\n\nFile size\n\nThere are no character or size limits in a CSV file and there is no size limit for EES, though the larger a file is, the longer it will take to upload and process. Also remember that the files you upload are the files that users will download, consider the software they may access to (e.g. Excel) and whether the size of your files are compatible with this.\nExcel has a cell character limit of 32,760 and a row limit of 1,048,576. It is best to avoid exceeding these as some end users may struggle to open the file. One good way to cut the file down is to split after a certain number of years, or to separate out different geographic levels into separate files, providing school level data as a separate file for example. With the data all being in a tidy format these are then easy enough for secondary analysts to stitch back together if needed.\nA rough guide to file size would be:\n\nAnything under 10 MB is relatively small\n10 MB to 100 MB is a fairly common file size that most teams have\n100 MB to 500 MB is a large file and will struggle to upload if not compressed to a zip folder.\n500 MB and over are very large, and sometimes may struggle to compress small enough to upload.\n4 GB or more in size is larger than any we have seen before and will likely need testing in the platform first.\n\nContact us if you have any issues, or files that might be over 1 GB.",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#data-symbols",
    "href": "statistics-production/ud.html#data-symbols",
    "title": "Open Data Standards",
    "section": "Data symbols",
    "text": "Data symbols\nIn line with the GSS guidance on symbols, special values should be replaced with symbols in the following situations:\n\n\n\n\n\n\n\n\n\nSymbol\nUsage\nExample\nObsolete equivalents\n\n\n\n\nz\nWhen an observation is not applicable\nNo data for boys at an all-girls school\n\n\n\nx\nWhen data is unavailable for other reasons\nData for an indicator is not collected in a certain region\n:\n\n\nc\nConfidential data\nData has been suppressed\n\n\n\nlow\nRounds to 0, but is not 0\nRounding to the nearest thousand, 499 would otherwise show as 0. Only use 0 for true 0’s\n~\n\n\nu\nWhen an observation is of low reliability\nData for a local authority is identified as missing returns so is removed from regional and national totals\n\n\n\n\nIf you have any other conventions you’ve used in previous publications, or a scenario that isn’t covered above, check the GSS guidance (ignoring the part around separate columns for symbols), and contact us.",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#ees-metadata",
    "href": "statistics-production/ud.html#ees-metadata",
    "title": "Open Data Standards",
    "section": "EES metadata",
    "text": "EES metadata\nMetadata in a machine readable (CSV) format must accompany datasets uploaded to the explore education statistics service to ensure that the files can be processed correctly. This data will not be seen by users and is purely for EES to be able to understand and read your data.\nThis EES metadata is different to any metadata files you may provide alongside your data for your users.\nWe only need to provide EES metadata for filters and indicators, we do not need to provide any EES metadata for the compulsory observational units (time and geography), as this has been standardised and the platform is expecting pre-defined columns and values in those fields.\nWe do not need to provide metadata for filters that only have a single level, as this data is not useful for EES, and is there for the benefit of end users downloading the files.\n\n\n\n\n\n\nNote\n\n\n\nAny extra geography columns not specified in the allowable values above, or any time columns extra to time_period and time_identifier, are filters, and should be included in EES metadata if they have two or more levels.\n\n\nWith some files there may be more than one way to specify the metadata that will work in EES and give an accurate representation of the data itself. In these cases we recommend that publication teams test out different approaches and decide their approach based on what will best meet the needs of the users of their data.\n\n\nMandatory EES metadata columns\n\n\n\n\n\n\n\n\ncolumn\ndetails\n\n\n\n\ncol_name\nThis must exactly match the name of the corresponding column in the dataset.\n\n\ncol_type\nThis must be either ‘Filter’ or ‘Indicator’.\n\n\nlabel\nThis is the version of the column name that the users will see on the platform, therefore you must fill this in and not leave it blank. For example, pupil_headcount may be ‘Number of pupil enrolments’. You should aim to keep these short and descriptive, but have the freedom to decide what is best to do for your users.\n\n\nindicator_grouping\nThis column gives production teams the option to add subheadings to group indicators in order to benefit the user. If this column is left blank, all indicators will be presented as one list of individual square radio boxes with no subheadings.\n\n\nindicator_unit\nIf this column is left blank then this will be a number by default, alternatively you can use either of the following units for financial or percentage measures - “£”, “£m”, “%”, “pp”.\n\n\nindicator_dp\nThis column allows you to set decimal place formatting for each of your indicators. If you leave it blank the platform will default to 2 d.p\n\n\nfilter_hint\nThis column gives you the option to add in a hint such as ‘Filter by school type’ for the filter to make the service easier for the users to navigate. If you leave the column blank, no hint will appear. Do not duplicate the column name here, as it will just appear twice\n\n\nfilter_grouping_column\nThis column should be blank unless you are wanting to group your filters. When you are wanting to group your filters this column should contain the exact name of the column/variable that you wish to group by. It is good practice to use the same variable name as that you are grouping, with _group appended at the end, i.e. ‘filter’ and ‘filter_group’\n\n\n\n\n\n\nAcceptable indicator_unit values\n\n\n\n\n\n\n\n\nindicator_unit\nformatting notes\n\n\n\n\n\nWhen left blank we automatically comma separate numeric values, character strings are left as is and will not show in charts\n\n\n£\nAdds a pound symbol before the value\n\n\n%\nAdds a percentage symbol after the value\n\n\npp\nAdds pp after the value\n\n\n£m\nAdds a pound symbol before the data and m after the data to represent millions of pounds, e.g. £12m\n\n\nnumberstring\nThis allows for numeric IDs or codes to be presented as a string, e.g. for a code of 19950808 it will show exactly as that, instead of 19,950,808\n\n\n\nNote that if you are using percentage points (pp) you must include a clear explanation in your release and methodology, so that users can understand what you are referring to.\nPlease contact the explore education statistics platforms team if you have any questions about the formatting or would like more options adding.\n\n\n\nExample EES metadata\n\nEach row represents a column in the data file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol_name\ncol_type\nlabel\nindicator_grouping\nindicator_unit\nindicator_dp\nfilter_hint\nfilter_grouping_column\n\n\n\n\nsex\nFilter\nSex\n\n\n\nFilter by pupil sex\n\n\n\nschool_phase\nFilter\nSchool phase\n\n\n\nFilter by the phase of the school\n\n\n\nchildren_count\nIndicator\nNumber of children\n\n\n\n\n\n\n\nchildren_percent\nIndicator\nPercentage of children\n\n%\n1\n\n\n\n\n\n\n\n\nThe corresponding data file.\n\n\n\n\n\n\n\n\n\n\n\ntime_period\n…\ncountry_name\ngender\nschool_phase\nnumber_children\npercent_children\n\n\n\n\n2018\n…\nEngland\nMale\nPrimary\n240\n26.7\n\n\n2018\n…\nEngland\nFemale\nPrimary\n200\n22.2\n\n\n2018\n…\nEngland\nTotal\nPrimary\n440\n48.9\n\n\n2018\n…\nEngland\nMale\nSecondary\n240\n26.7\n\n\n2018\n…\nEngland\nFemale\nSecondary\n220\n24.4\n\n\n2018\n…\nEngland\nTotal\nSecondary\n460\n51.1\n\n\n2018\n…\nEngland\nMale\nTotal\n480\n53.3\n\n\n2018\n…\nEngland\nFemale\nTotal\n420\n46.7\n\n\n2018\n…\nEngland\nTotal\nTotal\n900\n100.0",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#time-and-geography",
    "href": "statistics-production/ud.html#time-and-geography",
    "title": "Open Data Standards",
    "section": "Time and geography",
    "text": "Time and geography\nEvery observation, or row, in all of the provided data files will have a set of observational units based on the time period and geographic level that the data relates to. The number of these columns will differ across files depending on the number of geographic levels included in the publication.\nAcross every single dataset of official statistics produced by DfE, the following column names must be present at a minimum (example cell values):\n\n\n\n\n\n\n\n\n\n\ntime_period\ntime_identifier\ngeographic_level\ncountry_code\ncountry_name\n\n\n\n\n201819\nAcademic year\nNational\nE92000001\nEngland\n\n\n\n\n\nTime columns\n\nWe use the two columns, time_period and time_identifier, to generalise time across our underlying datasets. All data files must contain these. This is a important for general useability of our data, as well as being critical in driving the charts and tables in the explore education statistics service and making explicit reference to the time in which our measurements relate to. This is a compulsory element of any official statistics dataset.\nIf you think that your data can’t follow this format, please contact the explore education statistics platforms team with details so that we can discuss this further.\n\ntime_period must contain either a four digit year, or a 6 digit year.\ntime_period must be numeric. This allows the platform to understand ranges and order periods in a logical manner.\nsix digit time_periods must represent consecutive years - e.g. 201718, not 201619.\nIf you’re referring to a single term you should use the academic year not the calendar year in the time_period column.\nConceptually different years cannot be mixed in the same dataset.\nConceptually different year breakdowns (e.g. term, quarter, month), can be mixed with a full year of the same type using a filter column.\n\n\n\n\nSpecific time standards\n\nProducers should not mix different types of years in the same dataset. This is to prevent any chance of confusion for users selecting time periods with similar labels in the table tool. For example, you cannot have Academic year and Calendar year data in the same data file. You also cannot mix yearly breakdowns (e.g. full year, quarters, months, or terms) in the time identifier column. Instead, where it makes sense to mix these within a data file you should use a filter column as shown below. Note the use of ‘Total’, this is a part of the standards for filters.\n\n\n\ntime_period\ntime_identifier\nquarter\n\n\n\n\n201718\nAcademic year\nTotal\n\n\n201718\nAcademic year\nQ1\n\n\n201718\nAcademic year\nQ1-2\n\n\n\n\n\n\ntime_period\ntime_identifier\nmonth\n\n\n\n\n2017\nCalendar year\nTotal\n\n\n2017\nCalendar year\nJuly\n\n\n\nIf your row of data spans multiple years (e.g. is a cumulative sum between 2010 and 2018), the starting year should be made clear in the name of the indicator, with the year of the end of the time period listed as the time identifier. For example if you had been recording the number of enrolments in a Local authority since from the start of the 2010/11 Academic year to the end of the 2017/18 Academic year, your data would look like the example on the right.\n\n\n\ntime_period\ntime_identifier\nstarts_since_201011\n\n\n\n\n201718\nAcademic year\n190\n\n\n201617\nAcademic year\n173\n\n\n\n\n\n\nList of allowable time values\n\nAll time_period values should be numeric only, below the number of digits (either 4 or 6) is defined per time_identifier below. Do not include dashes or slashes in six digit years.\nYou can only mix time_identifiers if they appear within the same table below. If they are in separate tables then they should not be mixed.\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nCalendar year\n4 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nReporting year\n4 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nAcademic year\n6 digits\n\n\n\n\n\n\nacceptable time_identifier value\nCorresponding time_period\n\n\n\n\nFinancial year\n6 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nFinancial year Q1\n6 digits\n\n\nFinancial year Q2\n6 digits\n\n\nFinancial year Q3\n6 digits\n\n\nFinancial year Q4\n6 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nPart 1 (April to September)\n6 digits\n\n\nPart 2 (October to March)\n6 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nTax year\n6 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nAutumn term\n6 digits\n\n\nSpring term\n6 digits\n\n\nSummer term\n6 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nAutumn and spring term\n6 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nJanuary\n4 digits\n\n\nFebruary\n4 digits\n\n\nMarch\n4 digits\n\n\nApril\n4 digits\n\n\nMay\n4 digits\n\n\nJune\n4 digits\n\n\nJuly\n4 digits\n\n\nAugust\n4 digits\n\n\nSeptember\n4 digits\n\n\nOctober\n4 digits\n\n\nNovember\n4 digits\n\n\nDecember\n4 digits\n\n\n\n\n\n\nAcceptable time_identifier value\nCorresponding time_period\n\n\n\n\nWeek 1\n4 digits\n\n\nWeek …\n4 digits\n\n\nWeek 52\n4 digits\n\n\n\n\nRemember\n\nYou must include time_period and time_identifier columns in your data files.\nYour data must match the allowable values above.\nUse ‘Reporting year’ if your data does not fit in other categories, i.e. collected on a specific day.\nIf you have different types of year such academic, calendar, and financial, these should be in separate files.\nUse filters to add more detail if you have multiple time breakdowns in the same file (quarter/full year).\nWhere a measure spans multiple years, you should name the starting year, and set the time_period as the year published.\n\n\n\n\nGeography columns\n\nWe publish at a number of different geography breakdowns and these vary from publication to publication. Every publication in the new platform must include the three compulsory geography columns - geographic_level, country_code and country_name in its data files. These are compulsory as the data we are producing must lie within a country boundary.\nThe geographic_level column should describe the level of data present in that row. Therefore data for a collection from a specific local authority would have ‘Local authority’ as the geographic_level, while a National aggregation would have ‘National’ as the geographic_level.\nTeams should make sure that they are regularly checking their geography codes if they are not using a lookup from a maintained database (such as in the PDR). ONS have the Open Geography portal, which can be a useful way of checking these. There is a wealth of data on there, though Local authority boundaries can be hard to find, they can be found using the tabs at the top – Boundaries &gt; Administrative Boundaries &gt; Counties and Unitary Authorities.\nTeams can also use the lookup tables available via the data screener GitHub repository data folder. Guidance on how to link directly to the latest data in your code can be seen in the standardised reference data section\nIf you have data from an unknown location, the standard is to use ‘Not available’ as name, and ‘x’ as the code/s, this clearly marks that the geographical data for that row is unavailable, and does so in a consistent way with the wider GSS. If you have a unique variation of a certain location, e.g. ‘Outside of England and unknown’, we may be able to add these in as exceptions. Locations like this would have a code of ‘z’ as no widely used code is applicable, please get in touch with us to discuss it further if you think this might apply to you.\nWe expect that all geography codes will be based on the Open Geography Portal and run checks against this for various levels in the data screener, please contact us if you have data that doesn’t match this.\n\n\n\n\n\n\nNote\n\n\n\nWhere you have data for a legacy LA that does not have a 9-digit new code, leave those cells as blank instead.\n\n\n\n\nDifferent measures of geography\n\nWhen using geographies that can be measured in multiple ways, you can achieve this by including a filter such as level_methodology in the example below to state how you have measured the geographic level. For example, at Local authority level you may have data that was measured by the residence of the pupil and the location of the school:\n\n\n\n\n\n\n\n\n\n\n\n\ngeographic_level\nold_la_code\nla_name\nnew_la_code\nlevel_methodology\nheadcount\n\n\n\n\nLocal authority\n373\nSheffield\nE08000019\nPupil residence\n689\n\n\nLocal authority\n373\nSheffield\nE08000019\nSchool location\n567\n\n\n\n\n\n\n\n\nAllowable geographic levels\n\nAll rows must have country_code and country_name completed, regardless of geographic level. The additional required columns by level are shown below. You do not have to publish at every level, this is a guide that covers every level that can be published in the platform.\nWhere you have multiple geographic levels in a file, leave any not applicable columns blank for other rows. For example, region_name and region_code should be blank for national rows. Some levels do fit into a hierarchy, for example local authorities all have a region, in these cases you should include the higher level information too. So for a local authority row, the region and country columns would all be completed as well as the la columns.\n\n\n\n\n\n\n\n\ngeographic_level\nrequired columns\nnotes\n\n\n\n\nNational\nNo additional columns\n\n\n\nRegional\nregion_code, region_name\n\n\n\nLocal authority\nold_la_code, new_la_code, la_name\nIt is usually good practice to include the Regional aggregations where possible given the direct link between Local authorities and Regions.\n\n\nRSC region\nrsc_region_lead_name\nFor RSC region data, we generally define them into lead RSC regions where the majority of the data is from.\n\n\nParliamentary constituency\npcon_code, pcon_name\n\n\n\nLocal authority district\nlad_code, lad_name\n\n\n\nLocal skills improvement plan area\nlsip_code, lsip_name\n\n\n\nLocal enterprise partnership\nlocal_enterprise_partnership_code, local_enterprise_partnership_name\n\n\n\nEnglish devolved area\nenglish_devolved_area_code, english_devolved_area_name\n\n\n\nOpportunity area\nopportunity_area_code, opportunity_area_name\n\n\n\nWard\nward_code, ward_name\n\n\n\nMAT\ntrust_id, trust_name\nNote that Trust ID is shown as Group ID on GIAS when looking at a Trust. MATs also have a ‘company number’, this can be included but is not mandatory.\n\n\nSponsor\nsponsor_id, sponsor_name\nNote that Sponsor ID is shown as Group ID on GIAS when looking at a Sponsor.\n\n\n\n\n\n\n\n\n\nEnglish devolved area\n\n\n\nEnglish devolved area is used to refer to combined authorities, mayoral combined authorities and the Greater London Authority.\n\n\nPlanning area, School, Provider, and Institution level data will upload as normal to EES, though will not be read into the table tool or data blocks if they are mixed in with other levels. All data, including these levels are accessible in the downloadable files for users to explore in the same format as they are uploaded.\nFor Provider, any data files that only consist of this level are able to be uploaded and will be usable in the table tool. This solution is still not ideal, though it will allow you to include additional filters at provider level. Wherever there are multiple values per provider this should be marked as a filter, and where there is a single value per provider this should be marked as an indicator.\nFor School, any files consisting of school-level data should be standalone data files (that will work in the table tool, not ancillary files) and these should all be passing the EES data screener. They should also include LA level information so that users can see this when searching.\n\n\n\n\n\n\n\n\ngeographic_level\nrequired columns\nnotes\n\n\n\n\nSchool\nschool_name, school_urn, school_laestab, old_la_code, new_la_code, la_name\n\n\n\nProvider\nprovider_name, provider_ukprn\n\n\n\n\nFor Planning area and Institution data, any data files that only consist of these levels should be uploaded as an ancillary file rather than a data file.\n\n\n\n\n\n\n\n\ngeographic_level\nrequired columns\nnotes\n\n\n\n\nPlanning area\nNo required columns, though we recommend both planning_area_code and planning_area_name\n\n\n\nInstitution\nNo required columns, though we recommend you include institution_id and institution_name to make data matching easier\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have a level that isn’t covered above, please contact the explore education statistics platforms team with details and example data.\n\n\n\n\nUsing School or Provider as a filter\n\nProvider / School breakdowns can work as a filter column in your data file but only if it is the only filter in the file. We’d recommend using school_name or provider_name as the filter, and then including the code as well, such as Malton School (URN: 121681) or Malton School (UKPRN: 10004165). You should also include LA level information so that users can see this when searching.\nThis approach will work for basic filtering of providers and schools but it may not be performant in the table tool and the number of choices made available to users may be overwhelming (especially for schools, or data set to national level).\n\n\n\nUnknown geographic codes\n\n\n\n\n\n\n\n\n\nGeographical code\nUsage\nExample\n\n\n\n\nz\nWhen a geography code is not applicable\nUsing custom geographies like “Scotland, Wales and Northern Ireland” as a group. This has no standardised geography code so will not be applicable\n\n\nx\nWhen a geography code is unavailable\nData is missing, the geography is unknown so no code is available\n\n\n\n\n\n\nGeography summary\n\n\nYou must have the minimum expected geography columns, country_code and country_name.\nYou must also have the additional columns required for all geographies included in your file.\nThe additional columns must exactly match the names above.\nYou do not have to publish at every level in the guide, publish the selection of levels you feel appropriate.\nYou should regularly check that you are using the most up to date geography names and codes.\nAside from blanks, no two geographic locations should have the same code.\nYou can mix geographies in the same file, and should avoid separating files by geography unless size is an issue.",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#common-harmonised-variables",
    "href": "statistics-production/ud.html#common-harmonised-variables",
    "title": "Open Data Standards",
    "section": "Common harmonised variables",
    "text": "Common harmonised variables\nWherever possible variables should confirm to the DfE harmonisation strategy. Having consistent variables across data files held by the DfE, and in particular across data files in publications, allows users to more quickly navigate and understand our statistics. In the EES context, harmonisation will also facilitate more advanced methods of interacting with our data platform such as automated data retrieval for dashboards and other secondary statistics services, whilst also helping with performance and cost effectiveness of the EES platform.\n\n\nSpecial Educational Needs\n\nSpecial educational needs data uploaded to explore education statistics service is expected to conform closely to the School Census data guidance.\nTop level fields are provided by sen_status and sen_provision as follows:\n\n\n\n\n\n\n\nsen_status\nsen_provision\n\n\n\n\nAny special educational need\nAll SEN provision\n\n\n\nEducation, health and care plan\n\n\n\nSEN support / SEN without an EHC plan\n\n\nNo identified special educational need\nNo SEN provision\n\n\nAll pupils\nAll pupils\n\n\n\nIf both are provided, then sen_status should be listed as the filter_grouping_column of sen_provision.\nIndividual needs can be provided using sen_primary_need and sen_secondary_need. These are as follows:\n\n\n\nsen_primary_need\n\n\n\n\nAutistic spectrum disorder\n\n\nHearing impairment\n\n\nNot reported\n\n\nModerate learning difficulty\n\n\nMulti-sensory impairment\n\n\nOther difficulty or disability\n\n\nPhysical disability\n\n\nProfound and multiple learning difficulty\n\n\nSEN support but no specialist assessment of type of need\n\n\nSevere learning difficulty\n\n\nSocial, emotional and mental health\n\n\nSpecific learning difficulty\n\n\nSpeech, language and communication needs\n\n\nVision impairment\n\n\nUnknown\n\n\nNo primary need\n\n\nTotal\n\n\n\nThe options for sen_secondary_need are the same as for sen_primary_need, except for No primary need is replaced by No secondary need.\nIf using breakdown_topic and breakdown instead of individual columns, then the appropriate entries for breakdown_topic are SEN status, SEN provision, SEN primary need and SEN secondary need.\n\n\n\nSex and gender\n\nThe Department’s policy on collecting sex and gender data is that statistics should preferentially be presented for sex rather than gender. These terms, as used in statistics publications, are defined in the following sections and publication teams should be careful not to mislabel sex categories as gender or vice-versa.\n\n\nSex\n\nSex, as reported in DfE statistics, is defined as follows:\n\na value which identifies the sex of a person as recognised in law, i.e. the sex as recorded on a birth certificate (or on a gender recognition certificate).\n\nWhen presented in a filter, this should take the column header sex and standard entries Male, Female and Unknown. This can also be included as part of the standard breakdown_topic / breakdown filters, with the relevant entry under breakdown_topic being Sex and as above, the entries under breakdown being Male, Female and Unknown.\nUnknown as an option is likely to be used rarely - it is appropriate on data entry when the sex of the subject has not been recorded, is not known, or has not been registered (for example in data relating to unborn children in the Children in Need data).\nThe above is in line with current DfE data collection guidelines on sex.\nIn an EES data file, these might look something like:\n\n\n\n…\nsex\npupil_count\n\n\n\n\n…\nTotal\n12\n\n\n…\nFemale\n7\n\n\n…\nMale\n4\n\n\n…\nUnknown\n1\n\n\n\nOr…\n\n\n\n…\nbreakdown_topic\nbreakdown\npupil_count\n\n\n\n\n…\nSex\nTotal\n12\n\n\n…\nSex\nFemale\n7\n\n\n…\nSex\nMale\n4\n\n\n…\nSex\nUnknown\n1\n\n\n\n\n\n\nGender\n\nGender identity is defined by the ONS as follows:\n\nGender identity is a personal internal perception of oneself and, as such, the gender category with which a person identifies may not match the sex they were registered at birth.\n\nThe department is not collecting, and does not plan to collect, data on gender identity. However, the ambiguous use of the word “gender” in previous data collections may have affected the data collected. It is expected that, based on current policy on how the DfE collects data, teams should not be in a position to publish statistics containing the gender category once the new terminology has taken effect in all collections.\n\n\n\nTime-series containing historical data collected as “gender”\n\nThe GSS have updated their guidance on how sex and gender are defined as terms in statistical publications. A number of publications within the DfE will contain unclear data under that updated guidance.\nAs such, statistics producers should look to re-evaluate their categorisations against the current GSS definitions and update categories in future statistics products accordingly. This should include time-series published in any statistics releases going forward.\nWhere teams perform a re-categorisation of what has previously been labelled as gender to sex, this should be clearly stated in the publication methodology, with explicit definitions of any time periods that have undergone re-classification. The recommended text in such cases is as follows:\n\nHistorical use of the word “gender” in data collections may have meant that “gender identity” was reported in some cases, as opposed to legal sex. While this is unlikely to have a significant effect on overall figures, it may affect figures in more granular subdivisions. The definitions used in the data collection relating to this publication were revised on [insert date relevant to the collection] and as such, time series that span [insert date range] and contain sex as a category may be affected.\n\n\n\n\n\nEthnicity\n\nRace is a protected characteristic under the Equality Act 2010 and can include colour, nationality, ethnic or national origins. Data collected on ethnicity is self-identified and captures a person’s feelings of cultural heritage and belonging. Everyone may have slightly different ideas and feelings about what ethnicity encompasses, race and nationality may, or may not, be a part of a persons’ self-identified ethnicity.\nThis guidance has been written by collating the most up to date advice and guidance from the following sources:\n\nGovernment Statistical Service (GSS)\nOffice for National Statistics\nCabinet Office\nCommission on Race and Ethnic Disparities (CRED) report\n\n\n\nGSS ethnicity categories\n\nEthnicity breakdowns should be presented within the standard field names in any data files containing ethnicity breakdowns: ethnicity_major, ethnicity_minor or ethnicity_detailed. The first two are outlined in reference to the GSS guidelines on ethnic groupings below, whilst the third is for any ethnicity fields containing finer grained breakdowns than described in ethnicity_minor below.\nThe GSS publish standards on how to collect ethnicity data based on research conducted for the UK Census. The current guidelines (shown below) were developed as part of the 2011 Census and were unchanged in the 2021 census. We aim to follow as closely as reasonable to the GSS standards and guidance. Note the use of spaces around forward-slashes, which allows us to meet accessibility standards (in particular for the use of screen readers) as well as improving automated wrapping in text and value boxes.\n\n\n\n\n\n\n\nethnicity_major\nethnicity_minor\n\n\n\n\nWhite\nAll white\n\n\n\nEnglish / Welsh / Scottish / Northern Irish / British\n\n\n\nIrish\n\n\n\nGypsy or Irish Traveller\n\n\n\nAny other White background\n\n\nMixed / Multiple ethnic groups\nAll mixed / multiple ethnic groups\n\n\n\nWhite and Black Caribbean\n\n\n\nWhite and Black African\n\n\n\nWhite and Asian\n\n\n\nAny other Mixed / Multiple ethnic background\n\n\nAsian / Asian British\nAll Asian / Asian British\n\n\n\nIndian\n\n\n\nPakistani\n\n\n\nBangladeshi\n\n\n\nChinese\n\n\n\nAny other Asian background\n\n\nBlack / African / Caribbean / Black British\nAll Black / African / Caribbean / Black British\n\n\n\nAfrican\n\n\n\nCaribbean\n\n\n\nAny other Black / African / Caribbean background\n\n\nOther ethnic group\nAll other ethnic groups\n\n\n\nArab\n\n\n\nAny other ethnic group\n\n\nUnknown\nUnknown\n\n\nAll ethnic groups\nAll ethnic groups\n\n\nTotal\nTotal\n\n\n\nNote that these standards are written from the perspective of creating survey questions, i.e. the data collection phase. Where teams are trying to aggregate existing categories to match the GSS guidance, careful consideration of how and where differing systems may or may not match with the guidance is necessary.\nWe recommend two alternative methods of ordering the above ethnic groups when reporting on ethnicity statistics:\n\nAlphabetical: use in tables and when listing ethnic groups (with ‘Other ethnic group’ and sometimes ‘Unknown’ as a final category)\nIn expected order of size (with largest first): useful in charts and visualisations as it makes data and patterns easier to read\n\n\n\n\nReporting on broad ethnic minority categories (e.g. BAME)\n\nPublished in March 2021, the report of the Commission on Race and Ethnic Disparities made a number of suggestions on how we discuss and report on race in the UK.\nOne of the major elements was around aggregation of ethnic minorities into a single over-arching group (in particular BAME):\n\nRecommendation 24: Disaggregate the term ‘BAME’ Stop using aggregated and unhelpful terms such as ‘BAME’, to better focus on understanding disparities and outcomes for specific ethnic groups.\n\nThe reasons for this approach are two-fold:\n\nGrouping separate ethnic minorities into a single over-arching group can mask differing trends that may be present in the underlying data for groups experiencing different pressures and environments within UK society.\nThe phrasing ‘Black and Minority Ethnic’ and ‘Black, Asian and Minority Ethnic’ give an emphasis to specific ethnic groups whilst excluding others. This could be misleading or diminish consideration of the unnamed groups within the statistics.\n\nBased on this guidance, the following is how teams should aggregate and write about ethnic minority groups.\nStatistics producers should avoid where possible the practice of reporting aggregates grouping together disparate ethnic groups. Instead, statistics should be reported individually for the recommended top-level groups:\n\nWhite\nMixed / Multiple ethnic groups\nAsian / Asian British\nBlack / African / Caribbean / Black British\nOther ethnic group\n\nWhere there is a reasonable need to publish or report statistics for a full aggregation of ethnic minorities (such as reporting on existing historical targets), official guidance is as follows:\n\nUse “ethnic minorities” to refer to all ethnic groups except the white British group. This term includes white minorities, such as Gypsy, Roma and Irish Traveller groups. For comparisons with the white group as a whole, use “ethnic minorities (excluding White minorities)”.\n\n\n\n\n\nEstablishment characteristics\nA standardised set of establishment type fields has been developed, largely based on the data reported by the School and Pupils Statistics publication. Reference files for the standardised fields are available in the data screener GitHub repository, with the standards summarised below.\n\n\nEstablishment type group\n\n\n\n\n\nestablishment_type_group\n\n\n\n\nAcademy\n\n\nAll state funded\n\n\nIndependent\n\n\nLocal authority maintained\n\n\nNon-maintained\n\n\nAll schools\n\n\n\n\nIf needed, these items can also be reported in the standard breakdown field, using a breakdown_topic of Establishment type group.\n\n\n\nEstablishment governance\n\nThe establishment_governance field provides an opportunity to list the governance of an establishment, e.g. Academy, Non-maintained, Independent etc. It can be assigned the establishment_type_group as a filter grouping column.\n\n\n\n\nestablishment_governance\n\n\n\n\nAcademy\n\n\nCity technology college\n\n\nCommunity\n\n\nFoundation\n\n\nIndependent\n\n\nNon-maintained\n\n\nVoluntary aided\n\n\nVoluntary controlled\n\n\nTotal\n\n\n\n\nIf needed, these items can also be reported in the standard breakdown field, using a breakdown_topic of Establishment governance.\n\n\n\nEducation phase\n\nEducation phase should be listed under the column name education_phase where given as a standalone filter or Education phase if included in the breakdown_topic / breakdown combination. When included as an individual filter, it can be attributed establishment_type_group as the filter grouping column where available.\n\n\n\n\neducation_phase\nSchool census code\n\n\n\n\nAll-through\nAT\n\n\nAlternative provision\nPR\n\n\nEarly years settings\n\n\n\nFurther education\n\n\n\nHigher education\n\n\n\nIndependent school\n\n\n\nMiddle (deemed primary)\nMP\n\n\nMiddle (deemed secondary)\nMS\n\n\nNo establishment\n\n\n\nNursery\nNS\n\n\nPrimary\nPS\n\n\nSecondary\nSS\n\n\nSpecial\nSP\n\n\nAll education phases\n\n\n\n\n\n\n\n\nCombining education phase and establishment type group\n\nMany teams report a hybrid filter which combines a form of the establishment type group above with education phase. Ideally, teams should report these as two separate filters as follows (with establishment_type_group being listed as the filter_grouping_column of education_phase in the meta data):\n\n\n\n\nestablishment_type_group\neducation_phase\npupil_count\n\n\n\n\nAll state funded\nPrimary\n600\n\n\nAll state funded\nSecondary\n800\n\n\nAll state funded\nAlternative provision\n200\n\n\nAll schools\nSpecial\n400\n\n\nAll schools\nAll education phases\n2000\n\n\n\n\nIf a single field is necessary, the following is the fall back option to report as a single category under phase_type_grouping:\n\n\n\n\nphase_type_grouping\n\n\n\n\nState-funded primary\n\n\nState-funded secondary\n\n\nState-funded alternative provision\n\n\nSpecial\n\n\nAll schools",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#filters",
    "href": "statistics-production/ud.html#filters",
    "title": "Open Data Standards",
    "section": "Filters",
    "text": "Filters\nA filter is a variable that we break our data down by, such as school type, pupil characteristics, or NQT status. There are no required standards for these, you can include any filters that you feel benefit the users of your data.\nFilter names must not include spaces, and ideally be formatted in snake_case for ease of use. You can use abbreviations to keep the names short and usable, your users will be able to see what each variable name means in the data guidance. Avoid starting variable names with a numeric character.\nAggregates should be included for all filters where possible. There will be some situations where aggregation does not work, this will not break the platform, though may make the data less user-friendly. In general, if you can’t aggregate, e.g. if you have headcount and fte, then this is an indication that they are instead separate measures and should have separate indicator columns rather than forcing them into a filter column.\nFor aggregate rows you should refer to these as ‘Total’. When using filters such as school type or FSM eligibility this can be quite simple, as shown in the example below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\ntime_identifier\ngeographic_level\ncountry_code\ncountry_name\nschool_type\nFSM_status\nheadcount\n\n\n\n\n201819\nAcademic year\nNational\nE92000001\nEngland\nTotal\nEligible\n590\n\n\n201819\nAcademic year\nNational\nE92000001\nEngland\nPrimary\nEligible\n280\n\n\n201819\nAcademic year\nNational\nE92000001\nEngland\nSecondary\nEligible\n310\n\n\n\n\nIt is possible that you wish to include a filter that only has a single level, like FSM_status in the above example that only has ‘Eligible’. This is to be expected, and you do not need to unnecessarily double the size of your file by adding a ‘Total’ that duplicates what is already there. Where this is the case, do not include this filter in the EES metadata as the platform does not need to read that column into the table tool.\n\n\nGrouping filters\n\nFor some filters, this can be more complicated. You might have a great number of different filters that you would rather group together in a two-level hierarchy. You can do this by grouping a filter by a filter group. This essentially uses another filter column to provide a higher group level. This also may be helpful for EES users when creating tables. For example, you may want to group percentages into wider rate bands.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\ntime_identifier\ngeographic_level\ncountry_code\ncountry_name\nabsence_band_group\nabsence_band\n\n\n\n\n2018\nCalendar year\nNational\nE92000001\nEngland\n60.0 – 69.9\n67.0 - 67.9\n\n\n2018\nCalendar year\nNational\nE92000001\nEngland\n60.0 – 69.9\n62.0 - 62.9\n\n\n2018\nCalendar year\nNational\nE92000001\nEngland\n10.0 – 19.9\n16.0 - 16.9\n\n\n\n\n\nYou can also see an example with apprenticeship data file, including the corresponding EES metadata, below:\n\nData file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\n…\ncountry_name\nappr_type\nappr_exact\nstarters\ndrop_outs\ndrop_out_rate\n\n\n\n\n2018\n…\nEngland\nHair and beauty\nHairdressing\n200\n10\n0.05\n\n\n2018\n…\nEngland\nHair and beauty\nBeauty therapy\n100\n2\n0.02\n\n\n2018\n…\nEngland\nRail engineering\nTrack signalling\n80\n4\n0.05\n\n\n\n\n\nCorresponding EES metadata file.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol_name\ncol_type\nlabel\nindicator_grouping\nindicator_unit\nfilter_hint\nfilter_grouping_column\n\n\n\n\nstarters\nIndicator\nNumber of learners starting an apprenticeship\nApprentice starts\n\n\n\n\n\ndrop_outs\nIndicator\nNumber of learners not completing an apprenticeship\nApprentice drop-outs\n\n\n\n\n\ndrop_out_rate\nIndicator\nPercentage of pupils not completing an apprenticeship\nApprentice drop-outs\n%\n\n\n\n\nappr_exact\nFilter\nType of apprenticeship\n\n\nSelect the apprenticeship types you’re interested in\nappr_type\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nYou must not include grouping filters as separate rows in the EES metadata, they should only be referenced in the filter_grouping_column.\n\n\n\n\n\n\nDates as filters\n\nIf you are publishing data with a date assigned (e.g. daily data), you can add date in as an additional filter.\nIt is important to maintain a yyyy-mm-dd format as demonstrated below, so that EES lists dates in the correct order in the table builder.\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\ntime_identifier\ndate\ngeographic_level\ncountry_code\ncountry_name\nabsence_band\n\n\n\n\n2020\nWeek 50\n2020-05-14\nNational\nE92000001\nEngland\n67.0 - 67.9\n\n\n2020\nWeek 50\n2020-05-15\nNational\nE92000001\nEngland\n62.0 - 62.9\n\n\n2020\nWeek 50\n2020-05-16\nNational\nE92000001\nEngland\n16.0 - 16.9\n\n\n\n\nR automatically outputs dates in this format, but you can also change this in excel if needed, by selecting the column to change and switching from “General” to the specified date format:\n\n\n\n\n\n\n\n\n\n\nRemember\n\nAll filters should have a ‘Total’ aggregation where possible.\nIf a filter has one level, you should not include it in the EES metadata.\nWhere filters do not aggregate, consider if you should have the levels as indicators instead.\nGrouping filters is an option, but only group filters where you decide it is better for your users.\nIf a filter is used to group another, then it should not appear as a separate row in the EES metadata.",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#indicators",
    "href": "statistics-production/ud.html#indicators",
    "title": "Open Data Standards",
    "section": "Indicators",
    "text": "Indicators\nThe indicators are the variables showing the measurements/statistics themselves, such as the number of pupils. These can be of different formats (e.g. text, numeric), although are numeric by default. The number of indicators will vary across publications and data files.\n\n\n\n\n\n\nWarning\n\n\n\n\nEvery variable in your data set should have its own column, and each column should be a single data type. E.g. do not create an indicator column called “pupils” that has both the number and percentage of pupils in it. Instead, create two separate columns for each measure.\n\n\n\nAs an example, the number and percentage of pupil enrolments are the indicators in this dataset:\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\n…\ncountry_name\nschool_type\nenrolments_count\nenrolments_percent\n\n\n\n\n201819\n…\nEngland\nTotal\n200\n100\n\n\n201819\n…\nEngland\nPrimary\n150\n75\n\n\n201819\n…\nEngland\nSecondary\n50\n25\n\n\n\n\nLike filters, indicator names must not include spaces, and ideally be formatted in snake_case for ease of use. You can use abbreviations to keep the names short and usable, your users will be able to see what each variable name means in the data guidance. Avoid starting variable names with a numeric character.\nIn EES, the platform itself will format large numeric values to include commas, so you do not need to have commas for separating large numbers in your data file.\nIf you are including percentages and want to show 79% in EES, you would have the value 79 in your data file and specify % as the unit for that column in the EES metadata. As best practice we recommend that you include as much detail as possible in your underlying files when it comes to decimal places. As a default EES will round decimals to 2 d.p. in the platform, though if you would prefer to display your data to a different number of d.p. in the platform please use the indicator_dp column to specify this in the CSV metadata. Likewise with financial data, for don’t include £ in the values in the data file, leave those cells as numeric only, include it as an indicator unit instead.\nIn line with GSS best practice for communicating uncertainty, when using confidence intervals you should report the upper and lower bounds as separate columns rather than as one confidence interval column:\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_period\n…\ncountry_name\nschool_type\nestimate\nest_lower_ci\nest_upper_ci\n\n\n\n\n201819\n…\nEngland\nTotal\n100\n96.7\n103.3\n\n\n201819\n…\nEngland\nPrimary\n50\n48.9\n51.1\n\n\n201819\n…\nEngland\nSecondary\n50\n49.5\n50.5\n\n\n\n\n\n\nIndicator grouping\n\nMany of our publications contain a large number of indicators. To improve the experience of the user in the platform we can group these under headings as shown on the right. In the example, you can see how the different indicators have been grouped into ‘Admissions’, ‘Applications’, and ‘Preferences breakdowns’ in the EES metadata, followed by how this will appear in the table tool for users.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol_name\ncol_type\nlabel\nindicator_grouping\nindicator_unit\nfilter_hint\nfilter_grouping_column\n\n\n\n\nnc_year\nFilter\nNC Year\n\n\nFilter by national curriculum year\n\n\n\nadmissions_count\nIndicator\nNumber of admissions\nAdmissions\n\n\n\n\n\napplications_count\nIndicator\nNumber of applications received\nApplications\n\n\n\n\n\napplications_online_count\nIndicator\nNumber of online applications\nApplications\n\n\n\n\n\napplications_online_percent\nIndicator\nPercentage of online applications\nApplications\n\n\n\n\n\n\n\n\nExample of how this would look in the explore education statistics service:",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/ud.html#how-to-name-columns-fields",
    "href": "statistics-production/ud.html#how-to-name-columns-fields",
    "title": "Open Data Standards",
    "section": "How to name columns (fields)",
    "text": "How to name columns (fields)\nIn order to create and maintain a consistent data catalogue, we suggest that teams perform a regular review of their data files against the guidance on this page.\n\n\nField names\n\nEach publication team should regularly review the indicator and filter field naming in their publications to maintain:\n\nconsistency with the above field naming framework,\nconsistency with centrally standardized field and\ninternal consistency within their publications.\n\nThe recommended process for this is to follow these steps:\n\ncollate all col_name, col_type, label, indicator_grouping and filter_grouping_column fields from meta data files into one single CSV file;\ncheck for indicators that contain information better suited to filter entries (in line with tidy data principles);\ncheck indicator and filter col_name entries against published current standard names and assign new col_name entries as appropriate;\ncheck indicator and filter col_name entries against standard naming conventions and assign new col_name entries as appropriate and\ncheck indicator and filter col_name entries for internal consistency and assign new col_name entries as appropriate.\n\nPlease collate the above into a CSV file similar to your publication meta CSV files. This should have the following columns:\n\ncol_name\ncol_type\nlabel\nindicator_grouping\nfilter_grouping_column\ncol_name_new\ncol_type_new\nlabel_new\nindicator_grouping_new\nfilter_grouping_column_new\ndiscontinued\n\nAny “_new” entry where no update is to be made should be left blank, whilst any changes should be listed in the relevant column. Any discontinued col_names (i.e. with no single direct replacement) should have ‘y’ entered in the discontinued field. Any completely new fields with no previous direct precursor should contain blanks in the first 5 columns.\n\n\n\n\n\n\nNote\n\n\n\nYour team should then keep a copy as a log of any changes and also send a copy to explore.statistics@education.gov.uk so we can look to update our standardised list of field names where appropriate.\n\n\n\n\n\nField entries\n\nA further step teams can take to maintain standardization is to review the filter options or entries within their filters. For example, any ethnicity fields should conform to our published harmonized (GSS) ethnicity guidance.\nAs with field names we recommend collating these into a master CSV file covering your entire publication. Expected entries are:\n\ncol_name\nfilter_grouping_column\nfilter_entry\nfilter_grouping_entry\nfilter_entry_new\nfilter_grouping_entry_new\ndiscontinued\n\nFor example, in tabulated form:\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol_name\nfilter_grouping_column\nfilter_entry\nfilter_grouping_entry\nfilter_entry_new\nfilter_grouping_entry_new\ndiscontinued\n\n\n\n\nethnicity_minor\nethnicity_major\nTotal\nChinese\nChinese\nAsian / Asian British\n\n\n\nethnicity_minor\nethnicity_major\nIndian\nAsian / Asian British\n\n\n\n\n\nethnicity_minor\nethnicity_major\nGypsy\nWhite\nGypsy or Irish Traveller\n\n\n\n\nethnicity_minor\nethnicity_major\nIrish\nWhite\n\n\n\n\n\nethnicity_minor\nethnicity_major\nArab\nOther ethnic group",
    "crumbs": [
      "Statistics production",
      "Open Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/embedded-charts.html",
    "href": "statistics-production/embedded-charts.html",
    "title": "Embedded visualisations in EES",
    "section": "",
    "text": "Guidance for creating and embedding R Shiny visualisations in EES publications",
    "crumbs": [
      "Statistics production",
      "Embedded visualisations in EES"
    ]
  },
  {
    "objectID": "statistics-production/embedded-charts.html#introduction",
    "href": "statistics-production/embedded-charts.html#introduction",
    "title": "Embedded visualisations in EES",
    "section": "Introduction",
    "text": "Introduction\nEES now offers the option to add an embedded block, which allows analysts to present custom R created charts within their publications. This necessitates analysts creating an R Shiny based app on GitHub containing the required chart that can then be hosted on a Shiny server (currently using the DfE’s account on shinyapps.io).\nNote that this current combination of GitHub and ShinyApps for publishing Shiny Apps does not offer the appropriate security required for unpublished DfE data. Unpublished uderlying app data should therefore never be uploaded to GitHub / ShinyApps.io until such time as it has been published (i.e. via EES).\nPrior to publication, either dummy data or the already published data should be substituted in for the purposes of uploading to GitHub and deploying to ShinyApps.\nIf you need a live version of the dashboard with the unpublished data for pre-release reviews and access, the following options are available:\n\nDemo the chart in R-Studio\nCreate a copy of the chart repository on Azure DevOps, deploy this to rsconnect and use the rsconnect link as the embedded URL prior to publication (note this will need updating to the public ShinyApps link before the publication goes live).\n\nWe are currently putting in place a case to provide an internal Shiny server platform, which will allow greater control of security around our data and allow draft Shiny applications to use unpublished data for internal use.\n\n\nWhen to use an embedded chart\n\nEES provides a wide range of inbuilt chart options and will always be the first preference for static line, bar and geographical charts. This helps us to clearly maintain consistent styling and accessibility levels across the site.\nHowever, there are some instances where you might want to publish something that we can’t provide through EES. Example use cases would be:\n\ninteractive charts controlled by drop-down filters;\nchart types not provided by EES, e.g. sankey diagrams, box plots, waffle charts, pie charts, dumbbell charts, etc.\n\n\n\n\nTools\n\nWe currently only support custom charts created using R Shiny. These should be created with ggplot and plotly. We provide a template example of a demo R Shiny / ggplot chart on the DfE Analytical Services GitHub site, which is described below.\n\n\n\nReview and authorisation\n\nTo get a custom chart approved for embedding within a publication, you’ll need to get it reviewed by the explore education statistics platforms team (in addition to your standard approval chain).",
    "crumbs": [
      "Statistics production",
      "Embedded visualisations in EES"
    ]
  },
  {
    "objectID": "statistics-production/embedded-charts.html#creating-an-embeddable-r-shiny-app",
    "href": "statistics-production/embedded-charts.html#creating-an-embeddable-r-shiny-app",
    "title": "Embedded visualisations in EES",
    "section": "Creating an embeddable R Shiny App",
    "text": "Creating an embeddable R Shiny App\nAn embedded chart should take the form of a single chart, with necessary input options and nothing else. Avoid incorporating:\n\nmultiple different charts\nusing navigation elements\nimages and logos\n\nAnd definitely don’t include unpublished data in any app uploaded to GitHub / ShinyApps prior to publication.\nOther elements such as tables may also be possible if there’s a clear use case for them that EES alone can’t meet. Get in touch with us if you want to check that what you’d like to include is compatible with the intended use of the embed block.\n\n\nThe DfE Tiny-Shiny template\n\nOur template tiny shiny app repository should be used a starting point for all embedded Shiny charts.\nTo get an app set-up for use with EES, you’ll need the explore education statistics platforms team to create a repo for the app within the DfE Analytical Services area on GitHub.\n\n\n\nWhat’s in the template\n\nThe template provides code for some basic interactive plots. Each example plot is contained with one of the existing branches below for demonstration purposes:\n\ndemo-interactive-bar\ndemo-interactive-line\n\n\n\n\nWorking with data\n\nAs with the full dashboards, the embedded charts currently require the underlying data to be either included within the app repository on GitHub or uploaded elsewhere publicly accessible such as Google Drive or Dropbox. This currently means that any embedded charts being developed will need to use either dummy data or previously published data until the moment of publication. At the point of the parent release going live, the chart can then be updated with the latest data. Do not upload unpublished data to GitHub, Google Drive or Dropbox.\nAs described earlier, where you need to use unpublished data in your chart prior to publication, you can either a) run the chart locally in R-Studio (without pushing the unpublished data to GitHub) or b) create a DevOps/rsconnect deploy of your app, which can be temporarily used as the embed block URL. Note, this will need updating to a URL to the public dashboard on ShinyApps ready for publication.\nWe are currently developing a route to allow charts via R Shiny apps to be hosted on DfE servers, such that draft publications will be able to incorporate embedded charts with the unpublished data. The data itself will then be accessed either from a SQL database on DfE servers.",
    "crumbs": [
      "Statistics production",
      "Embedded visualisations in EES"
    ]
  },
  {
    "objectID": "statistics-production/embedded-charts.html#development-requirements",
    "href": "statistics-production/embedded-charts.html#development-requirements",
    "title": "Embedded visualisations in EES",
    "section": "Development requirements",
    "text": "Development requirements\nMany of the same principles that apply to creating a full R Shiny dashboard apply to creating a custom embeddable charts. These are:\n\nAccessibility;\nReliability;\nPerformance;\nCoherent styling.\n\n\n\nSpecific design recommendations for embedded plots\n\nExample code for producing an embeddable Shiny chart is given in the template tiny shiny app repository. The following recommendations should be followed in adapting this code:\n\nFigures should be produced using plotly/ggplot2\nFigure lengths and heights should be in the range 6-10cm\nText sizes in plots should be no smaller than 12pt\nPlotly overlays should be turned off\n\nExample code for creating charts using ggplot can be found in the Using explore education statistics guidance",
    "crumbs": [
      "Statistics production",
      "Embedded visualisations in EES"
    ]
  },
  {
    "objectID": "statistics-production/user-eng.html",
    "href": "statistics-production/user-eng.html",
    "title": "User engagement",
    "section": "",
    "text": "Guidance on understanding and engaging with the users of published statistics",
    "crumbs": [
      "Statistics production",
      "User engagement"
    ]
  },
  {
    "objectID": "statistics-production/user-eng.html#introduction",
    "href": "statistics-production/user-eng.html#introduction",
    "title": "User engagement",
    "section": "Introduction",
    "text": "Introduction\nUser engagement is about building sustainable relationships and feedback loops between statistics producers, users (and potential users) of statistics and other relevant stakeholders.\nBeing user centered is a key departmental priority. It’s critical that we actively engage with the users of our statistics. There are thousands of users out there and teams should be actively be getting feedback on their publications from a variety of users - policy colleagues, local authorities, schools, media, and the general public to name but a few.\nThe Code of Practice for Statistics states that:\n“users of statistics and data should be at the centre of statistical production; their needs should be understood, their views sought and acted on, and their use of statistics supported”.\nProducers of statistics will only know whether this element of quality continues to be achieved by regularly engaging with a full range of users.\n\n\nUSER hub\n\nThe User Support and Engagement Resource (USER) hub is now available on the GSS website. This is a central resource to help producers of statistics develop the knowledge, skills and techniques needed to engage effectively with their audiences.\n\n\n\nImproving your own engagmenent\n\nDownload this top tips document as a starting point for understanding how you can improve your user engagement.\nIf you have any suggestions of things for us to test on EES with users who have volunteered to do user testing, please contact explore.statistics@education.gov.uk.\nMore advice and guidance will follow in this space.\n\n\n\nCase studies\n\n\nExplore education statistics service\n\nExplore education statistics (EES) was born from a discovery project, where the emphasis of the discovery was to understand who the users of the Department’s official statistics are, what they need, and where the DfE are currently meeting, exceeding or failing to meet those needs.\nInitial user research\nWe spoke to over 90 users directly, via 1 to 1 sessions or workshops and had over 130 survey responses. We used this research to create user personas and user stories which we then assessed ourselves against to see how well we were meeting those needs - the result of this was a recommendation that we needed to do something better.\nThe alpha phase\nFollowing the discovery, we moved to the alpha phase, where we built prototypes and carried out extensive user testing to firm up the functionality that might be required - focusing on different user journeys and the user experience at each step.\nThe private beta phase\nWe then passed a GDS service assessment where our approach was tested against the GDS service standard before moving into the private beta phase where we started building the service for real.\nUser testing\nWe user-tested every bit of the journey thoroughly and used feedback from users to iterate on the functionality we were making available. User testing was primarily through 1 on 1 sessions where users would be set activities to work through and we would watch how the user clicked through each part of the relevant journey and to ask what they might expect to see at any different steps of the process - both when creating/publishing statistics releases and using releases after publishing. We also ran workshops and carried out user surveys to collect as much evidence as possible, for more information on this take a look at the write up of our private beta phase on Hive IT’s website.\n\nEES goes live\nIn March 2020 we were given the green light via a GDS service assessment to move into our public beta phase. That’s when we started using the service for real and teams started publishing statistics via EES only. Ongoing user feedback has been really important throughout public beta as it has helped us move the service from a minimal viable product to one that works in the best way possible for everyone. We also use this engagement to help us understand how well we are meeting our KPIs, which we review regularly to see how well we are meeting the original aims of the service thinking back to our original discovery and what users told us they needed.\nOngoing feedback routes\nThere are a number of ongoing feedback routes open to users during the public beta phase of the service:\n\nBeta banner (service focus) Users can submit feedback through the beta banner from any EES page, and responses are managed by the statistics development team. Feedback so far has mostly been aimed at the service functionality more generally but we do regularly get publication specific feedback which we share with the relevant teams.\nFormal user testing (service focus) Our EES team will speak to users via user testing sessions to help inform future iterations to functionality.\nIn-release feedback requests (publication focus) Some teams have been including specific calls for feedback within their release pages e.g. the apprenticeships and traineeships publication\nGoogle analytics We collect user analytics to help us understand what users are doing with our statistics. This information is shared with production teams via our EES analytics app.\n\nFor more information, get in touch with the team at explore.statistics@education.gov.uk.\n\n\n\nSchool places local authority scorecard\n\nIn 2021 the annual Local Authority School Places Scorecard publication was cancelled, due to Covid- 19, allowing time for user-engagement and development of the scorecard. This is a dashboard publication allowing users to select a specific local authority in England, to view it’s progress in providing quality school places.\nFirstly, internal feedback was sought and received, and generally users found the Scorecard a useful tool. Longer-term additions were identified, however short-term development was also needed to improve the accessibility of the Scorecard so this was the first developmental focus. The Scorecard has been previously published as an excel-based dashboard, however this method of publication was no longer viable due to new rules and regulations for official statistics publications.\nA new platform was therefore needed and R Shiny was suggested as an accessible platform. A new version of a previously published scorecard was produced in R shiny. This was then published externally to gather feedback from users to check they could obtain the required information.\nThe scorecard contained a link to a short survey and over 90 internal and external users responded (after email invitations were sent allowing a month to complete). Most users found the R Shiny version easier to use compared to the excel based dashboard which was positive. Suggestions for improvement were gathered and quite a few users mentioned they missed the one-page format of the excel scorecard (the R Shiny version has tabs to get to some visualisations) and found some of the newer charts slightly confusing. Therefore solutions were proposed e.g. a downloadable one page PDF summary and alternative charts/clearer guidance. Development work is still ongoing to ensure users can gather the information they need from the scorecard.\nFor any further information contact the Pupil Place Planning Data team.\n\n\n\nCIN-CLA outcomes statistical release\n\nIn 2020 we created a new statistical release which incorporated changes to the annual Outcomes for children looked after by LAs statistical release and outcomes within the Characteristics of children in need statistical release. Improving these statistics was a commitment made at the end of the CIN review.\nAs part of the review of the National Statistics, an ad-hoc release was published (Outcomes of children in need including looked after children) and we asked users to submit feedback on the proposals through the running of a consultation via Gov.UK to which people could respond (Children in need and looked after children statistics: proposed changes).\nWe involved policy colleagues (including policy analysts) in the proposed changes ahead of publishing. We engaged with colleagues across the department to identify known users and user groups and notified them of the request for feedback on the proposals. We also followed up with a reminder as we got closer to the deadline.\nAs part of this process, we had a meeting with the main stakeholders (the board of the National Association of Virtual School Heads). These key stakeholders were very keen to respond to the proposals but were having difficulty with resource and time to provide a comprehensive response, so we agreed to have a meeting to talk through everything and we took notes.\nReaching out to users and actively engaging with them produced a great result - we received a wide range of feedback from a range of sources including previously unknown users. We received external and internal feedback. We published the results of the feedback on the proposals on gov.uk, including a summary of the feedback received and what we were able to incorporate into the new publication. This was not an official Government consultation and we had to negotiate with gov.uk on how to publish the documents. We were advised that going forward, it would be preferable for user engagement of this type to be published alongside the statistical releases as part of the new EES platform. For any further information contact Bree Waine.",
    "crumbs": [
      "Statistics production",
      "User engagement"
    ]
  },
  {
    "objectID": "statistics-production/user-eng.html#ons-strategy",
    "href": "statistics-production/user-eng.html#ons-strategy",
    "title": "User engagement",
    "section": "ONS strategy",
    "text": "ONS strategy\nThe ONS published a user engagement strategy in February 2021.\nThe strategy sets out that many producers of statistics are already carrying out regular and effective user engagement. The strategy aims to build on this good work to embed user engagement as an essential part of a statistician’s role. The strategy sets out a vision and addresses the barriers to effective user engagement via three goals:\n\nCollaboration – collaborate across boundaries to offer a more coherent user experience\nCapability – build capability and equip producers of statistics with the practical skills and tools to deliver effective user engagement activities\nCulture – strengthen our culture and ensure user engagement is always an ongoing and essential part of a statistics producer’s role\n\nThe launch of the strategy, as well as two further videos on the value of user engagement are embedded below:\n\n\n\n\n\n\nWhat is user engagement?\n\n\n\n\n\nThe ONS have also published ten top tips to help improve user engagement.\n\n\n\nUser engagement successes",
    "crumbs": [
      "Statistics production",
      "User engagement"
    ]
  },
  {
    "objectID": "statistics-production/user-eng.html#dfe-user-engagement-strategy",
    "href": "statistics-production/user-eng.html#dfe-user-engagement-strategy",
    "title": "User engagement",
    "section": "DfE user engagement strategy",
    "text": "DfE user engagement strategy\nDfE are working with ONS and others to consider how to take forward work in this area. DfE have put together an initial statement (note that this is in draft and should only be used as a guide internally):\n\nWork to understand and engage our users is vital to ensuring statistics are suitably and best communicated, and help the department in continuously reflecting and improving products and its services. DfE welcomes the ONS strategy and collective focus on user engagement.\nAs part of our efforts to meet wider user needs, the Department has undertaken extensive user research over the past few years in development of the new explore education statistics service to disseminate our statistics. We continue to develop this platform in line with user research and feedback. Through the intelligent use of supporting analytics, we are learning more about how our wide range of users find and use statistics, and how best to tailor our outputs to meet their needs. This will help us embed user need even more strongly into our statistics development.\nIn addition and more broadly, the department is working with ONS Best Practice Team to consider guidance around user engagement and how best to take forward the strategy for the department. In line with this, as part of these developments we will be reviewing and refreshing our current public-facing user engagement strategy which will then be reviewed annually.\n\nTeams are encouraged to already start to think about who their users are, the channels they use to understand their needs and the user experience.\nPublication teams should keep abreast of user analytics to understand more about how users use and access their statistics and whether further improvements could be made.\nIn addition, teams should also think about the ways they can glean feedback on their publication(s), this may include internal checks regards enhancements such as participating in content design scrum(s).\n\n\nConsultations\n\nIf you are considering changes to your publication and are contemplating a formal consultation do get in touch with Statistics HoP office early on to discuss.\nFor background information about consultation, please see:\n\nCode of Practice on Consultations - please bear in mind this was written back in 2008 so certain aspects may have been updated but the background and principles do remain the same.\nConsultation principles: guidance\n\nExamples of previous consultations:\n\nOutcome and experience data\nConsultation on Data Futures and data collection\nSurveys on childcare and early years in England\nProposed cessation of ‘Income related benefits: estimates of take-up’ statistics\nProposals for a new statistical series to count unemployed claimants\n\n\n\n\nSurveys\n\nWe routinely publish statistical releases, but who is using these and why? It’s important to understand your user types and need, so we (public and department) can get best value from what we do. Using user insights can help us to continually improve our products and services.\nHaving a survey with your releases is a great way to glean this feedback.\nThe ONS has published 10 top tips for creating a user feedback survey if you want more information on creating a survey. You can also preview an example of results from a survey of statistics users to see how useful it can be to gain insights into the users of your statistics.\nIt would be great to include your examples too, as well as capture your lessons learned and tips, so do send these to us.\n\n\nExamples of surveys\n\nHigher Education Institutions Enrolments and Qualifications statistical bulletins - readership survey September 2019\nFurther Education & skills/Apprenticeships and traineeships release feedback survey\nHigher Education Statistics - Department for Education - Citizen Space\nUser feedback survey for statistics and data published by Land and Property Data Team at Registers of Scotland",
    "crumbs": [
      "Statistics production",
      "User engagement"
    ]
  },
  {
    "objectID": "statistics-production/ees.html",
    "href": "statistics-production/ees.html",
    "title": "Explore education statistics (EES)",
    "section": "",
    "text": "Guidance for how to use the features in the explore education statistics (EES) platform\nExplore education statistics (EES) is the Department’s official statistics dissemination platform, designed to make DFE’s published statistics and data easier to find, access, use and understand.\nThe EES platform consists of two applications:\nMaintenance and use of the platform is supported by the explore education statistics platforms team.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "statistics-production/ees.html#accessing-ees",
    "href": "statistics-production/ees.html#accessing-ees",
    "title": "Explore education statistics (EES)",
    "section": "Accessing EES",
    "text": "Accessing EES\n\nEnvironments\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are bookmarking links, please be careful to bookmark the links below exactly as they are shown. Often when signing in you will be redirected via other URL’s as a part of the authentication process, and bookmarking those may lead to errors.\n\n\nEES consists of two parts. We also have four versions (environments) of EES, the banner for the admin part of each environment will inform you which environment you are on, this is also colour coded.\nThere is no overlap between the environments and content created on one cannot be moved to any other. The core functionality across the environments is identical except for new changes, which are deployed through the different environments before they make it to production.\n\nDevelopment - Green\nWhere changes are first merged in, and often the first time different pieces of work from different developers will interact properly\nTest - Pink\nWhere our developers can carry out manual testing of any new features to make sure things work as expected\nPre-production - Yellow\nA sandbox area for analysts to carry out functionality testing.\nTeams can use the pre-production environment to familiarise themselves with the platform and test out what is possible. All analysts have full permissions to create publications and releases, and can see everything else that other analysts are making. This is unique to the pre-production environment.\nThere are two separate pre-production sites to mirror the public sites so that teams can familiarise themselves with both sides of EES:\n\nAdmin pre-production: where teams can test out creating publications and releases.\nPublic pre-production: where teams can test out how their releases might look on the public site.\n\nWhen accessing the pre-production environment you may be asked for a username and password, these are as follows: dfe, dataresearch.\n\n\n\n\n\n\n\nWarning - unpublished data\n\n\n\nThe pre-production environment is not suitable for unpublished data. Unpublished data should only be uploaded to the production environment.\n\n\n\nProduction - Red\nThe real service, anyone creating real releases that they intend to publish to the public should be using this environment.\nAnalysts will only have access to releases that they have been granted specific access to.\nAs mentioned, there are two public sites:\n\nAdmin production: where teams will create their releases\nPublic production: where any member of the public can access the published releases\n\n\n\n\nGetting access to admin\n\nAccess to both the production and pre-production admin services is limited to DfE AD accounts only and users have to have been invited to the service by either the explore education statistics platforms team (for full access) or an existing user (for pre-release access). Invites to the service are sent out via email using GOV.UK Notify.\nTo be invited to the service for full access teams need to email the explore education statistics platforms team, stating who needs access, what permissions they require (analyst or approver), and for which publications these apply. This email should be sent by the Team Leader, or accompanied with the relevant Team Leader’s approval. Once access has been granted you will receive an email inviting you to use the platform.\n\n\n\n\n\n\nNote\n\n\n\nJobshare emails can not be used to access EES. Specific personal emails should be used instead, in the same way as you sign in to windows on your machine.\n\n\n\n\n\n\nRoles and permissions\n\nThe Statistics Development Team are responsible for setting up and maintaining user permissions during the beta phase. Change requests will be monitored by the explore education statistics platforms team.\nThe following roles exist within EES admin and are assigned to a specific release:\n\nPre-release\nAny user invited for pre-release is given a pre-release user role. During the hours of pre-release they can:\n\nPreview the release page, including downloading files\nPreview the table tool page for that release\n\n\n\n\nAnalyst - contributor\nAny analyst working on a release within EES admin will have the contributor role. They can:\n\nEdit release details\nUpload data and other files\nCreate footnotes, datablocks and charts\nEdit and comment on release content\nInvite PRA users\nMove the release status between draft and ready for higher review\n\n\n\n\nAnalyst - approver\nThe responsible statistician for a statistics release will have the approver role, allowing them to sign off releases for publication. This will usually be the responsible G5 or G6 for the statistics publication. They can do everything a contributor can, as well as:\n\nMove the release status between draft, ready for review and approved\nSchedule the release for publication (either immediately or for a specific date)\n\n\n\nThere is also a publication owner role, which is assigned at publication level to a users account:\n\nPublication owner\nThis permissions level gives publication owners control over their publications. They can do anything a contributor can for all releases within the publication. On top of this the publication owner also has access to:\n\nManage publication level details\nCreate new releases within a publication\nCreate an amendment of the latest published release within a publication\nCancel an amendment before it is published\n\n\n\n\nBAU\nThe administrative role:\n\nThis role is assigned to a user and gives full access including administrative tools, typically only used by the Statistics Development Team and EES developers.\n\n\nSee below for a diagram of the responsibilities of each role as part of the publication process in EES:\n\n\n\n\nManaging permissions\n\nIf you are a publication owner then you can add other analysts in your team to specific releases to allow them to edit and contribute to the release.\nTo do this, select your publication from the admin dashboard.\n\nThen go to the ‘Team access’ tab.\n\nIn here you can see everyone who currently has access, and invite your team to contribute to the release, as well as manage and remove access. To give access the release must already exist, and you’ll need to give individual access for each new release you do in the publication.\nIf you are inviting someone who hasn’t used EES before, make sure to add their DfE email. Jobshare email addresses and mailboxes will not work, each user must be given access with their individual DfE account. The external user access section has more details on how to give access to external contributors.\nTo change Publication Owners, or Approvers, contact the explore education statistics mailbox.\n\n\n\nRequesting a new publication\n\nIf you have not published on EES before, or if you’re creating a brand new publication, you will need to contact us to create the publication for you before you can get started.\nIf you want to request a new publication, please be prepared to provide us with the following information:\n\nTheme that you want the publication to sit under in our site (use existing ones, or suggest new ones)\nTitle of the publication (exactly as you want it to appear on the site)\nPublication summary (see our guidance on publication summaries for more information on what to include)\nIf the methodology already exists elsewhere and you have a URL for it, or if you want to create one\nTeam name and email address\nLead statistician name and contact number (contact number is optional)\nTime periods of the releases you’re creating\nBadging (whether they are National statistics, an ad-hoc publication, or official statistics)\nWho needs access to the publication (list of email addresses of analysts who need access to the release and at what permission levels)\n\nPublication details can be managed by publication owners or via requests to the explore education statistics platforms team.\nThe hierarchy of content within EES is as follows:\n\nMethodology documentation is attached at a publication level within EES - meaning one standalone piece should be written to cover all releases for the given publication within the service.\n\n\nThemes\n\nPublications are organised into themes (as shown on the EES Find Statistics page), and then within each publication there are releases - where the latest release includes the latest statistics for that publication.\nFor example:\n\n\n\nLevel\nExample\n\n\n\n\nTheme\nPupils and Schools\n\n\nPublication title\nPupil absence in schools in England\n\n\n\n\n\n\nPublication summaries\n\nPublication summaries are a key tool in helping users find the statistics that they’re looking for. We use them on gov.uk pages and in the EES find statistics page.\nYou only have 160 characters – to make sure you are fully utilising these, have a look through the following advice:\n\nUse plain language to use terms and phrases that users are likely to use e.g. gender pay gap versus Annual Survey of Hours and Earnings.\nIs it clear what the geographical coverage of this publication is e.g. England?\nIs it clear how frequently the releases are published?\nIs it clear what breakdowns you cover? E.g. Ethnicity, Sex, SEN?\nInclude the abbreviations but make sure to also write them out in full so that people can search for either, e.g. Free School Meals (FSM)\nAvoid phrases like “This release covers” as this wastes characters and delays users getting to the main information.\nHave you looked at the EES analytics to see what users key search terms on your publication are? Are key words front loaded In your summary?\nDon’t waste space on including definitions of a topic within the summary.\n\n\n\n\n\nAdmin dashboard\n\n\n\n\n\n\n\nImportant\n\n\n\nUse Google Chrome or Microsoft Edge to access and use the admin part of the explore education statistics service.\n\n\nWhen you enter the admin website you’ll see the admin dashboard. What you can see here will be dependent on your access permissions i.e. you’ll only see the publications that you have been granted access to.\nThe Statistics Development Team will be responsible for setting up and maintaining user permissions during the beta phase. Change requests will be monitored via the explore education statistics mailbox.\nWithin the admin dashboard you can view and manage existing publications, including creating and editing their releases. You can use the drop down lists to find releases by theme/topic/publication or use the draft and scheduled releases tabs to see releases that are in progress.\n\n\n\n\nExternal user access\n\nIt is possible to request that external users from other government organisations have access, and they can then be added as collaborating analysts on a release or as pre-release viewers. Each external user must complete a declaration form. When you email the explore education statistics platforms team to request external user access, you should attach copies of the declaration forms for each external user. Temporary external accounts are usually set up to expire after three months, with an option to extend this if required.\n\n\nAdding external users as pre-release viewers\n\nThis process has two distinct steps: getting a temporary account set up per user in the DfE Azure Active Directory, and then adding lists of users requiring pre-release access (PRA) onto EES itself. The diagram below shows the responsibilities of the publication team, the Statistics Development Team, and the Identity Access Management Team in this process:\n\n\n\n\n\n\n\nAdding external users as collaborating analysts\n\nThe process for adding external users as collaborating analysts is similar to adding external users for PRA, except for the final step, in which the Statistics Development Team add the external users via the EES admin interface. The process can be seen in the diagram below:\n\n\n\n\n\n\n\nWhat to include in an external access request\n\nIf you have external users you’d like to request access for, please send the following details to the explore education statistics platforms team\n\nEmail addresses of users to be added\nReason for access\nLength of time access is needed (the standard time period is three months)\nCopies of declaration forms for each user\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to contact us at least two weeks in advance of requiring access, but ideally as early as possible. Due to the dependency on DfE’s digital security, we cannot guarantee access or how long it may take.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "statistics-production/ees.html#creating-releases",
    "href": "statistics-production/ees.html#creating-releases",
    "title": "Explore education statistics (EES)",
    "section": "Creating releases",
    "text": "Creating releases\nNew releases can be created by publication owners. To create a new release in a publication series, navigate to the relevant publication and click ‘Create new release’, you will then need to enter the following information:\n\nTime identifier for the release\nTime period for the release\nType of release (National, Official, Ad-hoc etc)\n\nOnce you have created your release you will see the dashboard to edit your release, here you need to work through the following tabs to create your release:\n\nData and files (uploading data files, supporting files and creating data guidance)\nFootnotes (creating footnotes and assigning to relevant data)\nData blocks (creating summary tables, charts and key stats indicators)\nContent (drafting release content)\nSign off (moving through the approval process)\nPre-release access (invite users to pre-release and create public pre-release list)\n\n\nIf you are experiencing issues or need help with creating a new release, you can get in touch with the explore education statistics platforms team.\n\n\nData and files\n\nYou need to make sure that the data files and accompanying metadata have passed through our data screener checks before trying to upload it.\nAll data files uploaded will be available to download for users to explore (this will be in the same format as they are uploaded).\n\n\n\n\n\n\nWarning\n\n\n\nReleases cannot be published without a completed metadata document. If not filled in an error will be flagged during sign off.\n\n\n\n\nSubject titles\n\nYou’ll need to give a ‘Subject title’ to each data file you upload. This is what users will see whenever the file is referenced within EES so it should be a simple user-friendly title. The actual file name and data guidance can include more technical / coverage information.\nWhen adding a Subject title, think about the general user and how they will appear in the service:\n\nYou don’t need to include the publication name in the title as this is always already implied within EES\nYou don’t need to list what filters are in each file in the title, users can see this in the data guidance\nYou don’t need to include the date ranges covered in each file in the title, users can see this in the data guidance\nYour Subject titles should be short and snappy and clearly explain what is in each file. Some good examples of this in practice are included below:\n\nEarly years provision by provider type\nTeacher vacancies\nITT new entrants by subject and training route\nAverage pay of further education workforce\nQualified entrants to teaching\n\n\nYou can make changes to the Subject title for your data file after it has been uploaded using the ‘Edit title’ option.\n\n\n\n\nUploading files\n\nWhen uploading files you have a choice between uploading as separate CSV files or as a combined ZIP file.\n\n\n\n\n\n\nTip\n\n\n\nFor data files greater than 80mb we recommend uploading as a ZIP file.\n\n\nOnce you click to upload the file a ‘Status’ will be visible that shows the progress of the import process. This may take a little while depending on the size of your file and if there are numerous files queued for import. You cannot view the dataset or use it to create tables/charts until this status is ‘COMPLETE’.\nIf you are having any issues uploading a file, please contact the explore education statistics platforms team.\n\n\nBulk uploads\n\nYou can upload your data files in bulk by using a single compressed ZIP folder.\n\nYou must ensure that the ZIP folder contains:\n\nAll of your data files\nAll of your metadata files with a matching name to a data file (e.g. dataset_name.csv / dataset_name.meta.csv)\nA dataset_names.csv file with two columns (file_name, dataset_name)\n\nThe two columns in dataset_names.csv should contain:\n\nfile_name must be the name of a matching CSV data set, without the file extension, e.g. dataset_name\ndataset_name is where you can provide the human readable version to use in the service, e.g. My awesome dataset\n\nFor example, your file may look like the following table:\n\n\n\n\nfile_name\ndataset_name\n\n\n\n\nteacher_pay\nTeacher pay\n\n\nteacher_retention\nTeacher retention\n\n\nworkforce_characteristics\nWorkforce characteristics\n\n\n\n\nOnce uploaded and processed, each data file will then proceed to import as normal, and you will be able to edit them as if you had imported them separately.\nIf anything in this ZIP folder doesn’t match up with this, you will receive an error message explaining why and what has happened.\n\n\n\nOrdering filters and indicators\n\nYou can save custom orders for your filters and indicators. This can help to save time when creating tables and charts, and to aid users who view your data themselves via the table tool.\nTo reorder your filters and indicators:\n\nGo to the data and files page after uploading underlying data files\nGo to the ‘Reorder filters and indicators’ tab (within the data and files page)\nChoose which file you want to reorder and then click and drag the items until they’re in the order you want them to show in the table tool.\n\nThis then becomes the default order for this filter or indicator and will apply in all charts and tables automatically (including the order that the options will be shown to users in the table tool on the public website).\n\n\n\n\n\nReplacing data\n\nIf you just need to change the Subject titles for your data file(s) you do not need to go through the whole replacement process, just click the ‘Edit title’ option.\nHowever, if you notice a mistake in your data file you can replace it with another. When replacing a data file the platform will attempt to recreate any data blocks and footnotes that were built using the previous file.\n\n\n\n\n\n\nWarning\n\n\n\nThe replacement file must contain the exact same column names and types as the original. For example, a character column named “date” must also be replaced with a character column named “date”. A numeric column named “date” will not work in the replacement.\n\n\nNavigate to the file you wish to replace, and you should see a ‘Replace data’ option in the ‘Actions’ row.\n\nThe first step is to upload the new file.\n\nOnce you’ve chosen and uploaded your replacement file it will need to go through the usual import process before it can check if retaining existing data blocks and footnotes will be possible.\n\nOnce the upload is finished a report will appear which highlights whether existing data blocks and footnotes can be replaced successfully. If you want to keep any data blocks and footnotes you’ve built you will need to make sure that your replacement data file still contains the information (indicators, filters, geographic_levels and time_periods) that was used to create them.\n\nIf it’s not possible for a data block or footnote to be recreated using the replacing data file a warning will appear and you’ll be prompted to either edit or delete them before completing the replacement.\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember to double check any data blocks or footnotes that were recreated by the platform before publishing your release.\n\n\n\n\n\nSupporting file uploads\n\nSupporting files should not be used as the default. Wherever possible you should upload your data as data files that can then be used in the table tool.\nAny files you want to make available for users to download but aren’t intended for the table tool should be added as a supporting file upload. These files will need to meet all requirements of the new accessibility regulations before they can be published.\nExamples of supporting files may be:\n\nInfographic pages\nSupplementary data that isn’t intended for the table tool\n\nTo ensure that spreadsheets are accessible, see the guidance from gov.wales, and this Analysis Function guidance.\nIf you are unsure of whether you should be using supporting files, contact the explore education statistics platforms team for advice.\n\n\n\nPublic data guidance\n\nWithin the ‘Data and files’ page, you can also create your public data guidance. This replaces the information that would have previously been uploaded as a PDF on GOV.UK and is designed to help users understand and use the data they download from your release. See the Permanent and fixed period exclusions data guidance for an example.\nThe ‘Data files’ section of the document will automatically update as you add new data files to your release, however you will need to add an overview of all the data included in the release and short summaries for each data file before the release can be published.\n\nA list of variables in each file with an associated label (taken from metadata uploads) and associated footnotes will also be displayed for each file.\n\n\n\n\n\n\n\nWarning\n\n\n\nReleases cannot be published without a completed data guidance page. If not filled in an error will be flagged during sign off.\n\n\n\n\n\nFootnotes\n\nWe generally advise against using footnotes, unless they’re absolutely necessary (think about how many people will actually be reading them!). However, if you do want to add footnotes to your data, this should be done using the EES footnote function and not added in-text.\nFootnotes can be added via the footnotes tab. Rather than writing multiple tables and assigning individual footnotes, you write footnotes and assign them to certain indicators and filters so they appear when users select them in the table builder.\nFor example in the below, the footnote “This is a footnote” is assigned to the “Headcount” indicator for all options within the “School type” filter.\n\nIf you would rather, you can assign a footnote to the whole data file by ticking this box.\n\nYou can assign the same footnote across multiple data files.\n\n\n\n\n\n\nTip\n\n\n\nWe recommend that you only add footnotes once you are certain the data file is final. If you have to delete the data file, all the assigned footnotes will be deleted alongside it.\n\n\n\n\n\nData blocks (tables and charts)\n\nA data block is a smaller cut of data from your original file that you can embed into your publication as a presentation table, build charts from, and link users directly to.\nThe ‘Data blocks’ tab will list all the data blocks you have created, highlighting which have charts, are used in content and are saved as highlight tables. Here you can also choose to create a new data block.\n\n\n\nUsing data blocks effectively\n\nAim for fewer tables, and keep them small. As a guide, we suggest no more than a couple of tables per accordion section.\nPresentation tables are the tables you include within the accordion sections of your release to quickly visualise numbers. Unlike the underlying data files, the presentation tables focus on specific parts of the story you are telling. They are distinct from, and should never be a copy of an underlying data file.\nDo include them where they add value to your release.\nDon’t include them as a straight copy of the ready-made Excel tables previously published on gov.uk\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to recreate all of the old excel tables, users can find the numbers they are interested in using the table tool, or analyse the underlying data if they want the data behind the release.\n\n\n\n\n\n\nTables\n\nBy default, every data block will include a table.\nYou will be taken through 4 steps to create the data block:\n\nData set: select the data set you want to use\nNational / Regional: choose the geographic level you want to display\nTime period: select which time periods you want to include\nIndicators / Filters: select the indicators and filters to be included\n\n\nOnce you have configured the data source for the data block, you can then preview the table displaying the chosen variables.\nYou can reorder table headers with the ‘Move and reorder table headers’ button to restructure your table however you want it before you save.\nOnce you are happy with your table, you can then fill in the Data block details:\n\nName of the datablock - this won’t be visible to users, it is for your own reference so you can differentiate between your data blocks\nTable title - this is the title of the datablock that will be visible to users\nSource - this should be the source of the data used to create the data file (not the data file itself) e.g., ‘School Census’.\n\n\n\nFeatured tables\n\nYou can also choose to highlight a data block table as a ‘featured table’ which means it will show in a list of featured tables within the table tool. This is designed to help users get to tables of interest more quickly (without having to create tables themselves).\nThere is an option to choose if a table is a ‘featured table’ when saving each data block, here you can name the table and add a description giving the table coverage (please don’t just repeat the table name in the description):\n\nEach featured table will then be listed to the user within the table tool. Featured tables do not have to be embedded within your release content to be included in this list.\n\n\n\n\nFast track links\n\nAny data block tables that are created and saved will be assigned a ‘fast-track link’, this URL can then be used throughout your release as a way to direct users to specific tables within the table tool more quickly so they can interact with and explore the data further. It will appear at the top of the page like this:\n\nWhen your release is published, any embedded data block tables within the release will have an ‘explore data button’ beneath them which will utilise these fast track links to quickly direct users to the table within the table tool so they can explore the data further. You can also use fast-track links as a hyperlink within release commentary (without having to embed the data block).\nIn your fast-track titles, you don’t want to overload information, but still want to direct the user to the right place. Remember they can go back to the table tool through your featured tables to change filters and indicators as needed.\nFast-track titles should explain:\n\nWhat the table is showing in the simplest terms\nWho/Where the data covers (e.g. characteristic groups and geography levels)\nWhen the data in the table is reported for\n\nHere are a few examples of good fast-track titles in EES:\n\nNumber of Schools and Pupils, by School Type, 2015/16 to 2020/21\nAbsence Rates by School Type, 2016/17 to 2020/21\nFree School Meals, by Region, 2015/16 to 2020/21\n\n\n\n\n\n\n\nWhat is the difference between a fast-track link and a permalink?\n\n\n\nEES also offers ‘permalinks’ for any table created in the table tool which allows a user to save a link to a permanent, static, version of a table they have created. Analysts can make use of these permalinks when answering queries or in PQ and FOI responses.\nFast-track links are similar to permalinks however instead of linking to a static version of a table they link to an ‘active’ version of the table within the table tool - meaning users can interact and change what’s shown in the table from within the table tool if they choose to.\n\n\n\n\n\n\nCharts\n\nAfter building and saving a data block table you will see a ‘Chart’ tab appears. This tab will take you to the EES chart builder, where you can choose to add a chart to your data block.\nThe first step to creating a chart is choosing the chart type, currently the EES chart builder can build line charts, horizontal/vertical bar charts (including stacked and clustered) and maps.\nAfter choosing your chart type you then need to work through the following stages to build your chart. In each stage you’ll be shown a live preview as you make changes.\n\n\nChart configuration\n\nWithin the ‘chart configuration’ tab you can add a title, alt text, move the legend and change chart dimensions.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to review your chart dimensions before you publish. Users should be able to read the labels on the axes and see the legend without having to scroll.\n\n\n\nNote, within the vertical and horizontal bar chart types you can also create stacked bar charts by clicking the ‘Stacked bars’ option within the chart configuration tab.\n\n\n\nData sets\n\nHere is where you add data to the chart. You can add each series one at a time or all together.\n\n\n\n\nLegend\n\nYou can edit the chart legend, and styling of your series via the Legend tab.\n\n\n\n\n\n\n\nTip\n\n\n\nTo select specify custom colours outside of the defaults, you can double click on the colour codes at the bottom of the colour picked until you get to the type of code you’re wanting to input (e.g. hex code) and then enter the code manually.\n\n\n\n\n\nX axis (major axis)\n\nHere is where you configure the x-axis: You can alter gridlines, labels, sort, limit and add reference lines.\n\n\n\n\n\n\n\n\nOption\nWhat it does\n\n\n\n\nSize of axis\nChange the width of the space given to axis tick labels\n\n\nShow grid lines\nTurn grid lines on and off\n\n\nShow axis\nTurn the axis on and off, you can also add a unit to the axis tick labels\n\n\nSorting\nChange how the data within the chart is sorted\n\n\nTick display type\nAlter how often axis tick labels are shown, labels will automatically skip values where there are too many to show without overlapping\n\n\nAxis range\nAlter the range of data shown in the chart\n\n\nLabel\nAdd an axis label, you can also choose the width for the space given to it\n\n\nReference lines\nAdd/remove reference lines to the chart\n\n\n\n\n\n\nY axis (minor axis)\n\nThen follow a similar process for the y axis configuration, play around until the chart looks how you want it to.\n\n\n\n\n\n\n\n\nOption\nWhat it does\n\n\n\n\nSize of axis\nChange the width of the space given to axis tick labels\n\n\nGroup data by\nChange how the data within the chart is grouped\n\n\nShow grid lines\nTurn grid lines on and off\n\n\nShow axis\nTurn the axis on and off, you can also add a unit to the axis tick labels\n\n\nSorting\nChange how the data within the chart is sorted\n\n\nTick display type\nAlter how often axis tick labels are shown, labels will automatically skip values where there are too many to show without overlapping\n\n\nAxis range\nAlter the range of data shown in the chart\n\n\nLabel\nAdd an axis label, you can also choose the width for the space given to it\n\n\nReference lines\nAdd/remove reference lines to the chart\n\n\n\n\n\n\nChanging chart type\n\nIf you create your chart and then change your mind as to what chart type would be best you can just click to change it and it will try to save all the options that you had applied previously.\n\n\n\n\n\n\n\nNote\n\n\n\nRemember to save your chart when you’re done.\n\n\n\n\n\nMaps\n\nYou can create maps too, currently this is possible for regional, LA and LAD data.\nYou can change the boundaries you are plotting onto via the “chart configuration” tab, the latest boundary file will automatically be selected, but if you are visualising historic data, you may want an older boundary file. Please contact us if the boundary you want to plot is unavailable.\n\nThen to create a map, add the cuts of data you want to display in the “data sets” tab of the chart builder.\n\nYou can change the colour scale of the chart in the “legend” tab.\n\n\n\n\nBreaks in a series\n\nWe recommend including any missing data from breaks in a time series in your data file using the appropriate GSS symbol, such as in this example table:\n\n\n\n\n\n\n\n\n\n\n\n\n2013/14\n2014/15\n2015/16\n2016/17\n2017/18\n\n\n\n\nNumber of pupil enrollments\n3,627,985\n3,713,774\n3,796,146\nx\n3,885,774\n\n\nNumber of schools\n16,705\n16,723\n16,736\nx\n16,739\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere may be times when including missing data increases the file size too much, or becomes unwieldy, if you’re unsure and would like advice on your data contact explore.statistics@education.gov.uk.\n\n\nBy including the missing data in your open data files you can then create charts in EES that represent this. Start off by creating a data block with the data you want to build the chart from.\n\nOn the chart configuration tab there is a toggle for visualising non-numeric values in the data as 0. On the first chart configuration tab, there’s a check box that will toggle between showing and hiding them.\n\nWhen selected, you will then see that this data appears in the chart as if the indicator values are 0.\n\n\n\n\nOther chart types\n\nYou should only use complex charts where there is a clear user need, as simple charts are the easiest for users to understand. If you have a an infographic or a chart that is too complex to build within our chart builder you can use the ‘Choose an infographic as alternative’ option to upload an image to your data block or use the Add embed block feature to embed an R Shiny based plot (see the section on embedded blocks for further information).\nInfographic alternatives should be .svg format and you can use the sizing options within the data block editor to size your image appropriately.\n\n\n\n\n\n\nNote\n\n\n\nAccessibility matters for infographics too, consider the following if you do need to upload an image -\n\nKeep them simple\nUse colours that are available in EES - see our visualisation guidance for more details\nDon’t put borders around images\nRead the GSS guidance on the use of colour in visualisations and follow the steps provided to check your visualisations\nTry to avoid adding text to your images other than axis labels and limited annotations. Titles and headings can be added in the “chart configuration” tab instead\n\n\n\nYou can use R to create infographics and the following code gives an example of how to create a basic line chart or stacked bar chart using the appropriate GSS sequential colour palette.\n\n# Load the necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# This is the GSS standard categorical colour palette\ngss_categorical_palette &lt;- data.frame(\n  names = c('Dark Blue', 'Turquoise', 'Dark pink', 'Orange', 'Dark grey', 'Light purple'),\n  hex = c(\"#12436D\",    '#28A197', \"#801650\", \"#F46A25\", \"#3D3D3D\", \"#A285D1\"),\n  id = c(1,2,3,4,5,6))\n\n# Set up a dummy data-set.\n# Note the line factor(...,levels=...) allows you to order your filter values in\n# the final plot based on the ordering entered into the levels keyword. If left,\n# it'll default to alphabetical.\ndata &lt;- data.frame(\n  time_period=c(\"2018/19\",\"2018/19\",\"2018/19\",\"2018/19\",\"2018/19\",\"2018/19\",\n                \"2019/20\",\"2019/20\",\"2019/20\",\"2019/20\",\"2019/20\",\"2019/20\",\n                \"2020/21\",\"2020/21\",\"2020/21\",\"2020/21\",\"2020/21\",\"2020/21\"),\n  Filter=c(gss_categorical_palette$names,\n            gss_categorical_palette$names,\n            gss_categorical_palette$names),\n  indicator1=sample(4:16,18,replace=TRUE) +\n    c(gss_categorical_palette$id,gss_categorical_palette$id,gss_categorical_palette$id)*3\n  ) %&gt;%\n  mutate(\n    time_period=as.factor(time_period),\n    Filter=factor(Filter,levels=gss_categorical_palette$names)\n    )\n\n# Create a line chart\nggplot(data, aes(x=time_period, y=indicator1, group=Filter, colour=Filter )) +\n  geom_line(size=1.2) +\n  scale_color_manual(values=gss_categorical_palette$hex) +\n  theme_classic() +\n  theme(\n    legend.position = \"bottom\",\n    text = element_text(size = 14, family = \"Arial\"),\n    strip.text.x = element_text(size = 20)\n  ) +\n  ylab(\"Indicator 1\") +\n  xlab(\"Time period\")\n\n# Create a stacked bar chart\nggplot(data, aes(x=time_period, y=indicator1,  fill=Filter )) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values=gss_categorical_palette$hex) +\n  theme_classic() +\n  theme(\n    legend.position = \"bottom\",\n    text = element_text(size = 14, family = \"Arial\"),\n    strip.text.x = element_text(size = 20)\n  ) +\n  ylab(\"Indicator 1\") +\n  xlab(\"Time period\")\n\nThe above code should produce something along the lines of the following plots:\n \n\n\n\nEmbedding R Shiny blocks\n\nIf you need to include a type of chart that isn’t possible using the standard EES chart options, for example an interactive chart with filters, you can embed a block to display a custom R Shiny produced chart.\nTo embed an R Shiny chart, you’ll first need to create an R Shiny application containing the chart using the DfE tiny-shiny template and get it hosted on our DfE GitHub and ShinyApps accounts. More details on both of these are available in the tiny-shiny section of this guidance.\nOnce you’ve got the R Shiny app set up and hosted, you can embed it using the Add embed block button in the content area. This will give you the option to enter a URL, where you can enter the URL of the shiny app. Valid URLs to use in the embed block dialogue box are limited to only those on the DfE ShinyApps server (and the internal POSIT Connect servers when needed for draft publications).\n\n\n\nThe EES interface for embedding a Shiny chart\n\n\n\n\n\n\nContent\n\nIn the content tab you can now start creating your release, embedding the data blocks you’ve created as you go.\nYou can use the page view toggles, in the blue bar at of the page, to jump between edit and preview mode for the release and to view a preview of the table tool.\n\nAdd any headline and/or key stats and figures for your release in the headline facts and figures section.\nYou can then create accordion sections to start adding your main release commentary. These sections are made up of text blocks and data blocks which can be reordered as needed.\n\n\nHeadline facts and figures\n\nUse the ‘Add secondary stats’ button to add a data block to your headline stats section.\nUse the ‘Key stats’ options to add key statistic tiles to your release. For each tile you first have to have created a data block that contains only one number.\n\nAfter embedding a key stat tile you can then edit it to add trend information and a description of what the indicator is.\n\n\n\n\n\n\n\nElement\nContent\n\n\n\n\nIndicator name\nAutomatically generated from your data\n\n\nLatest value\nAutomatically generated from your data\n\n\nTrend\nA short one-sentence description of the trend; try to avoid only stating the change from the previous year and talk about the longer-term trend where appropriate\n\n\nGuidance title\nE.g. ‘What is NEET?’ or ‘What are permanent exclusions?’\n\n\nGuidance text\nA simple description in plain English of what the indicator is\n\n\n\n\n\n\nAccordion section content\n\nYou should split your release into sections that each focus on one or two key messages, with a recommended maximum of 10 sections in the release. The whole release should take no more than 10 minutes to read. Our analytics app contains insights on how long it takes the average user to read your release.\nTo keep the release short only include information if there is something interesting to say - the commentary is there to tell a story, people looking for specific figures will use the table tool, or download the underlying data instead. Do not try to summarise every number in the commentary.\nAvoid having large blocks of text as they are hard to read and users scan them and miss the detail. Graphs and tables break up the content but only include these where they add value; you do not need a graph or chart in every section.\nUse plain English and shorter sentences, aim for an average of 15-20 words per sentence. Do not overload sentences with numbers and avoid ‘elevator commentary’ that describes small movements in the whole series without giving any insight (use a summary table instead if it is interesting, or leave it out entirely). Be impartial and objective; avoid using sensationalist terms or terms that reflect a judgement such as “very few” “only” or “strong”.\nExplain complex concepts in plain English within the text. Remember that for many of our users, confidence intervals and significant differences are complex concepts that need explaining.\n\n\n\nAbout these statistics\n\nWe recommend that the first accordion section in each release should be ‘About these statistics’. Do not assume that users will read it, the nature of interactive pages means that the reader may start at any accordion section so remember to include essential information in the section to which it refers.\nAvoid filling this section with:\n\nCrucial caveats or information necessary for accurate interpretation of the statistics, these should be included within the main commentary next to the point they relate to\nTechnical information that is not relevant to the interpretation of the statistics should be saved for the methodology\nDefinitions should be included within the main commentary when they are first mentioned\nLists or descriptions of what is in the commentary as these should be clear from the contents and accordion headings\n\nTry to focus the ‘About these statistics’ section on:\n\nSay why the data are collected and what they are used for or could be used for, including relevant policies and targets\nThe different geographical levels for which data is published\nNon-essential but important information about the statistics, such as:\n\nThe data collection window, mode of data collection and response rates in terms of whether these are representative\nClarification of the population coverage\nChanges in data collection or analysis methods from previous releases\n\nLinks to relevant related statistics including the cause of any differences\nFurther useful detail on specific measures used in the release\nNon-urgent notices to users such as minor error corrections and delays or changes to the next publication. If there is something important which you want all users to know about then give it its own accordion section.\n\n\n\n\nTables in content\n\nAny data tables should be included as data blocks, however, there might be some instances where you aren’t able to add the table you need within a data block. If this is the case, you can embed static html tables within a text box. These should only be used to present textual tables or for any small presentations of data that are not possible to do in a data block at the moment.\nNavigate to the ‘Content’ tab. In the text editor, you can add in a table using the table icon.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that all of the data included or referred to in your content should be available (or able to be created) from the downloadable open data files.\n\n\n\n\n\nWriting about characteristics\n\nThere is a wide range of guidance available from the GSS, ONS and the Cabinet Office around writing about characteristics. Statistics content published on EES should adhere to the principles outlined by the above. The data harmonisation champions group are in the process of collating the most recent guidance from all these sources and summarising it below.\nIf you need some steer on how to report on a particular characteristics, the below links provide some useful starting points:\n\nAnalysis Function Data Harmonisation\nCabinet Office guidance on writing about statistics\n\nYou can also get in touch with the DfE Data Harmonisation Champions Group via explore.statistics@education.gov.uk.\n\n\n\nWriting about ethnicity\n\nFor the official names of ethnicity filters to use in data files, please see our guidance on common harmonised variables. The below outlines some key points in writing about ethnicity in publication content.\nEthnic minorities and not BAME\nGrouping ethnicity at a higher level than the 5 major ethnic groups should be avoided. In cases where this is not possible, the group ‘ethnic minorities’ should be used exclusively to refer to all ethnic groups except the White British group. For further information see our guidance on reporting on broad ethnic minorities categories.\nOrdering of ethnic groups\nWe recommend ordering ethnic groups either alphabetically, or in expected order of size (with largest first). For further information see our guidance on GSS ethnicity categories.\n\n\n\nFootnotes\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not use footnotes in the text of your content. They’re designed for reference in print, not web pages. If the information in the footnotes is important, include it in the body text. If it’s not, leave it out.\n\n\nIf you are including a table in text that needs footnotes, it’s generally advised to include this in the commentary surrounding the table. However, if you think a footnote is still necessary, then we advise writing out the word ‘note’, with the number of the note you need to refer to, and put it in square brackets, for example: ‘Number of people in employment [note 1]’. For more guidance on footnotes outside of EES, see the Analysis Function guidance on symbols, footnotes and codes.\n\n\n\n\nReviewing content\n\nWhile a release is in draft mode, comments can be added to text to help teams collaborate. Simply highlight the text you want to comment on and click on the speech bubble in the editing bar to add a comment.\n\nComments can be edited at a later date, and can also be marked as resolved so that you can see which comments have been addressed and which are still outstanding.\nWhen someone is editing a text box, it will now be instantly frozen for all other users preventing two users from editing the same block of text at the same time. You will be able to see the name of the user who is making edits, and will see the edits coming through every few seconds as their changes autosave.\n\n\n\n\nGetting found online\n\nSearch engine optimisation (SEO) makes it easier for users to find your data through search engines like Google. Some top tips include:\n\nKeeping your release title shorter than 50-60 characters. This means the full title can be displayed on the search engine results\nAvoid listing key words: search engines penalise anything not recognised as a full sentence.\nMake use of our analytics app to explore what your users are doing: what accordions are they clicking on, what are they searching for? This could give an idea of what content you should focus on in future, and which areas are no longer of interest to most of your users.\n\nFollowing best practice in writing about statistics is of increasing importance. As shown in the below example any sentence could be pulled out into a snippet and shown in a search engine to users who are searching for related information:\n\nWe should make a concerted effort to ensure that we are answering the questions people are interested in as search engines are getting smarter and pulling this information directly out of webpages. See the following example of a google search using a snippet from one of our publications as an answer in the search engine results itself:\n\n\n\n\nGlossary\n\nThe glossary in the explore education statistics service is a growing page that helps us to standardise how refer to key terms and phrases across all of Official statistics.\n\n\nAdding new definitions\n\nContact the explore education statistics platforms team, with the title and definition and we can add this for you. It’s worth having a check on the glossary for similar or related terms, and whether you should be looking to harmonise with other teams.\n\n\n\nLinking to definitions\n\nYou can link to any term in the glossary by appending the glossary url with #name-of-definition, replacing any spaces with hyphens.\nFor example, to link to the definition for ‘Respite care’, you would use the following link:\nhttps://explore-education-statistics.service.gov.uk/glossary#respite-care\nYou can test this works by typing the url into your browser, it should take you to that specific definition on the glossary page. If you’re unsure at all, or have special characters in the title of your glossary entry, please ask us for support on getting the right link.\n\n\n\nLinking from your release\n\nWhen writing your release content, highlight a word or phrase and click the link icon in the text editor bar.\n\nThen, paste your glossary url into the box that appears.\n\nThat’s it, the system will automatically recognise that the link is for the glossary and will do the rest. You can then change to the preview mode to see how this would then appear to public users, and test that the box appears with the definition.\n\n\n\n\n\n\nSign off\n\nOnce you’re happy with your release you need to go to the sign off page and change its status in order to move it through the release pipeline.\nThere are three statuses:\n\nIn draft (where the production team work on drafting the release)\nReady for higher review (where the senior statistician checks over the release before approving)\nApproved for publication (after approval has been given, releases in this status will be published on their scheduled date)\n\n\n\n\n\n\n\nNote\n\n\n\nOnly users with approver permissions (usually G6 or above) can sign off the release for publication.\n\n\n\n\nErrors and warnings\n\nThere’s quite a few things to remember to do as you build your release so to help ensure you haven’t missed anything a release checklist is also available via the sign off page\nThe checklist can be accessed by clicking edit release status in the Sign off page.\nRemember to check over it before you submit your release for approval as a release that has outstanding errors on it will not be able to be published.\n\n\n\n\nHow to approve a release\n\nReleases are approved via the sign off tab. The release date is also set during this stage of the process. All releases scheduled for a specific date will be published at 9:30am on that day.\nThe approver has the ability to approve the release to be published on a specific date or to publish as soon as possible. Publishing as soon as possible is useful for publishing amendments to existing releases.\n\n\n\n\n\n\n\nNote\n\n\n\nThis page also gives you the expected release url which may be useful to know for other things, for example, sending to the web team to add to your gov.uk announcement page.\n\n\n\n\n\nNext release expected\n\nYou also have the option to add a date for when the next release is expected. This will appear at the top of a release page and give users an idea of when to expect the next release in the publication series. You can provide the planned month and year of the next release within this publication.\nIf this is added but then needs to be changed or removed later in the year it is easy to do so, by creating an amendment and setting a new next release expected date on sign-off.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "statistics-production/ees.html#pre-release-access",
    "href": "statistics-production/ees.html#pre-release-access",
    "title": "Explore education statistics (EES)",
    "section": "Pre-release access",
    "text": "Pre-release access\nPrior to each release going live the production team are also able to grant pre-release access to a named group of users 24 hours before it goes live. These users do not require full access to the whole admin service. They will be able to see preview versions of any releases they have been granted pre-release access to.\nThis preview is only accessible for the 24 hours before the publication date, although the emails may go out to users before then. We expect teams will continue to send an email at 09:30 on pre-release day, including any additional briefing and the link to the pre-release area.\n\n\nGranting pre-release access\n\n\n\n\n\n\n\nNote\n\n\n\nJobshare emails and shared mailboxes should not be sent invites for pre-release. The personal emails for the specific individuals should be used instead, as jobshare emails / shared mailboxes do not have active accounts with DfE to access EES.\n\n\n\n\n\n\nEES PRA one pager\n\n\n\n\n\n\n\n\n\n\n\nInviting users for pre-release access and building the public pre-release list can be found within the ‘Pre-release access’ tab on the dashboard.\nOnce the release has been marked as approved, go to the ‘Pre-release access’ tab and add the relevant email addresses to grant pre-release access. All invited users will receive an email to say that they have been given pre-release access and will get a url where the preview release will be available.\n\n\n\n\nPublic pre-release access lists\n\nYou should also create the public facing pre-release access page by clicking the ‘Public access list’ tab.\n\n\n\n\n\n\nImportant\n\n\n\nAll official statistics (that is, official statistics, national statistics and ad-hoc statistics) are required to publish a public facing pre-release access page.\n\n\n\nAfter creating your pre-release access list a text editor will appear where you can list the roles that have been given early access to the release. This list will then appear in the list of file downloads at the top of each release.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "statistics-production/ees.html#making-amendments",
    "href": "statistics-production/ees.html#making-amendments",
    "title": "Explore education statistics (EES)",
    "section": "Making amendments",
    "text": "Making amendments\nIf you need to make an amendment to release that has already been published this is possible to do from within the admin dashboard if you are a publication owner. If you need to create an amendment and your publication owner is not available, please contact the explore education statistics platforms team to do this for you.\nFirst, find your publication in the admin dashboard.\n\nThen, from the releases tab, select to ‘amend’ the release you want to make a change to. This will then create a new version of that release that you can make changes to and then choose to republish. You’ll also have the option to view the existing publication that is currently published incase you want to compare between them, or to cancel the amendment if you want to discard all changes made in an amendment while it is still in draft.\n\n\n\nRelease notes\n\nWhen publishing a new amendment you should add a ‘release note’ to your release so users can be informed of what has changed. Try to keep this brief while remaining informative - it’s important to include detail of what has changed, so that a user can see if there is any data they are interested in has changed.\nIf a user is subscribed to your release, they are notified of any amendments you make and the public release note that you complete for your amendment also goes into this email.\nSee our guidance on Good examples in EES for a best practice example of a public amendment note.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "statistics-production/ees.html#methodology",
    "href": "statistics-production/ees.html#methodology",
    "title": "Explore education statistics (EES)",
    "section": "Methodology",
    "text": "Methodology\nAppropriate methodological information must be made available for all published Official Statistics releases. Methodologies can follow our recommended methodology template, to ensure all the essential details are covered.\nIf you have not created or published a methodology page on EES before, the first thing your publication owner will need to do is create a methodology page.\n\nOnce created you can then edit the methodology content in a similar way to how you would create release page content.\nOnce the drafting has finished, a methodology should be set as ‘approved for publication’.\n\n\n\n\n\n\nWarning\n\n\n\nAny approved methodology changes will be published immediately so make sure you are ready for the changes to go live before approving.\n\n\n\n\nMethodology amendments\n\nIf you want to revise an existing methodology page you can amend the methodology which will generate a new version for you to edit.\n\nOnce you’re happy with your amendment it just needs to be approved in the usual way for it to be published.\n\n\n\nMethodology subheadings\n\nSubheadings make it easier for your users to navigate through your methodology. When editing text blocks in your methodology, you can do this by highlighting your subheading and selecting “Heading 3” in place of “Paragraph”:\n\nYou can add further subheadings underneath this but try to limit the number of subheadings per accordion to 10 at the most. Too many subheadings will make it tricky for users to find what they are looking for.\n\n\n\nMethodology images\n\nYou can add images to methodology pages via the content editor.\n\nWhen uploading an image you will need to add alt text via by clicking the eye symbol, you may also choose to add an image caption.\n\n\n\n\n\n\n\nNote\n\n\n\nAccessibility matters for uploaded images too, consider the following if you do need to upload an image -\n\nKeep them simple\nUse GDS colours\nDon’t put borders around images\nEnsure there is a high enough colour contrast ratio between elements. You can use a colour contrast checker to check the colours you’re using\nTry to avoid adding text to your images other than axis labels and limited annotations.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "statistics-production/ees.html#managing-publications",
    "href": "statistics-production/ees.html#managing-publications",
    "title": "Explore education statistics (EES)",
    "section": "Managing publications",
    "text": "Managing publications\nWe have a type of role within EES admin called ‘Publication Owner’. This permissions level gives publication owners control over their publications.\nSee roles and permissions to see what options are available to publication owners.\n\n\nPublication management\n\nUse this page to change the details for your publication, such as the contact information or to add links to legacy releases on gov.uk.\nAny updates here will affect all releases in the series and will be published immediately.\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to change the publication title via this page, however this should be used rarely and will not affect the publications url if one or more published releases exist.\n\n\n\n\n\n\nCreate new release\n\nAfter finding the publication you want to create a release for, just press the button to create a new release.\nCurrently the following types of release can be created in EES:\n\nAccredited official statistics (includes OSR tick mark logo at the top of the release page)\nOfficial statistics\nAd hoc statistics\nManagement information\nOfficial statistics in development\n\nWhen creating a release you will be asked to fill in some release summary fields. If you are unsure of what type of release you are publishing, please contact the statistics HoP office for advice immediately.\n\n\n\n\n\n\nNote\n\n\n\nThe time period for the release should reflect the time period of the data that this latest release adds to the time series.\n\n\n\n\n\nManaging legacy releases\n\nPast publications on other services can be added to the previous releases links that appear on the top right of a release page by using the ‘Manage legacy releases’ section. In here you can add links to previous releases and choose the order in which they appear.\nThis section can be found by publication owners in the publication management section.",
    "crumbs": [
      "Statistics production",
      "Explore education statistics (EES)"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html",
    "href": "writing-visualising/dashboards.html",
    "title": "Public dashboards",
    "section": "",
    "text": "Guidance for publishing public statistics dashboards",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#when-to-use-a-dashboard",
    "href": "writing-visualising/dashboards.html#when-to-use-a-dashboard",
    "title": "Public dashboards",
    "section": "When to use a dashboard",
    "text": "When to use a dashboard\nThe primary route for publishing official statistics is using the explore education statistics service (EES). In addition to publishing on EES, there may be times where you want to compliment official statistics with a dashboard. This may be to do a deeper piece of analysis for a specific user group, or to make use of functionality that isn’t available within the platform.\nIt is important that all dashboards should have a clear use case and reasoning for why they are required. If you are considering developing and publishing a dashboard, please get in touch with the explore education statistics platforms team who can advise and assist with the process, and make use of the R Shiny statistics dashboards Microsoft Teams channel to ask questions and share what you’re working on with other teams.\nThis guidance is aimed primarily at dashboards published to supplement Official Statistics to the wider public. See the how to publish an R Shiny dashboard section below for more information on the guidance for internal only R Shiny dashboards, or public facing R Shiny dashboards via shinyapps.io.\nThis page includes specific guidance for R Shiny dashboards, for teams using PowerBI, the department has a set of PowerBI dashboard standards that can be found on the DfE’s analytical services GitHub area.\nThere is also a dashboards version of the content design checklist. This checklist runs through a number of things to think about when developing dashboards to compliment official statistics and is worth using to design and also review any dashboards you own - Dashboards checklist (.xlsx). We also have a dashboard procedure checklist in this guidance page that outlines the steps required to set up an R Shiny dashboard.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#standards-to-follow",
    "href": "writing-visualising/dashboards.html#standards-to-follow",
    "title": "Public dashboards",
    "section": "Standards to follow",
    "text": "Standards to follow\nWe expect all dashboards to follow a minimum set of standards to ensure coherence between our products and a minimum standard of quality for our end users.\nThese standards are constantly evolving, and all feedback and contributions are welcome, contact the explore education statistics platforms team.\nAll dashboards should have a clear use case and reasoning for why they are required. They also need to meet the latest accessibility regulations for public sector services.\nDashboards, along with the Official Statistics that they supplement, are a digital service that we provide, and we should ensure we follow the Service Standard. The standard covers a number of aspects that all producers should be considering when developing a dashboard, and strongly influence the guidance on this page.\nAll data must be published on EES as open CSV files so that it is accessible in one place and we are not publishing anything new via dashboards.\nWhen published, dashboards can be directed to from existing EES releases using the related links at the top right of a release page, or by being included as a url anywhere in the release content.\nThe following sections cover the considerations in more detail, though when developing a dashboard you need to commit to thinking about:\n\nUser needs\nSoftware - long term sustainability, knowledge and resource\nAccessibility guidance\nConsistent styling\nUser engagement and analytics",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#governance",
    "href": "writing-visualising/dashboards.html#governance",
    "title": "Public dashboards",
    "section": "Governance",
    "text": "Governance\nAll teams developing dashboards to accompany Official Statistics should contact the explore education statistics platforms team for advice on the level of approval required as there may be times when HoP approval is needed in addition to team / unit leader approval. Approvals should be sent to the Statistics Development Team when asking for a new dashboard to be hosted on shinyapps.io.\nThe standards for dashboards, and department strategy are maintained by the Central Statistics Standards Unit and governed by the Statistics Leadership Group, which is made up of all senior statisticians owning Official Statistics publications in the department.\nRegular reflection on how teams are doing and finding ways to improve is an important part of good governance. We recommend that Senior Statisticians responsible for dashboards ensure that they are regularly reviewed, covering:\n\ntesting and quality assurance\naccessibility\nperformance against user needs\noverall coherence with central standards and strategy found on this page",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#user-needs",
    "href": "writing-visualising/dashboards.html#user-needs",
    "title": "Public dashboards",
    "section": "User needs",
    "text": "User needs\nWhen designing a government service, always start by learning about the people who will use it. If you do not understand who they are or what they need from your service, you cannot build the right thing.\nUnderstanding as much of the context as possible gives you the best chance of meeting users’ needs in a simple and cost effective way.\nThe real problem might not be the one you originally thought needed solving. Testing your assumptions early and often reduces the risk of building the wrong thing.\nServices designed around users and their needs:\n\nare more likely to be used\nhelp more people get the right outcome for them - and so achieve their policy intent\ncost less to operate by reducing time and money spent on resolving problems\n\nSee the service manual for more information on learning about your users and their needs.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#software-choices",
    "href": "writing-visualising/dashboards.html#software-choices",
    "title": "Public dashboards",
    "section": "Software choices",
    "text": "Software choices\nWhen you make a decision about technology, you’re making a significant investment. The choices you make will have a huge impact on your ability to create, iterate and operate the service in a sustainable way. We should be choosing tools and technology that let us create a high quality service in a cost effective way and minimise the cost of changing direction in future.\nFor public facing dashboards we recommended using R Shiny.\nWe feel that it best meets the service standards, it aligns with the departments RAP strategy for the use of R in in Official Statistics, and the general direction of the government analytical community.\nR Shiny is incredibly customisable, and ideal for providing everything in one place - interactivity, user customised printable PDFs (replacing old mail merge solutions), and ease of use on a phone!\nR Shiny takes advantage of transferable knowledge and shared resources that already exist to minimise the costs and maximise the shared benefits.\nWhile we recommend R Shiny, teams may use other tools if it can be justified against the Service Standard 11. Choose the right tools and technology.\nThe most common dashboard options available to us are:\n\nR Shiny\nPython Dash (or other Python-based alternatives)\nPowerBI\n\nSome considerations to think about when choosing the tooling / software for a dashboard:\n\nwill it be open source? (Service standards 12 and 13)\nwhat skills do your team already have, or are already developing?\ncan you reuse anything that someone else has already done?\ndo you have flexibility in formatting and styling?\nwill you be able to maintain it long term?\ndoes it give the flexibility required to meet accessibility requirements?\nwhat costs will it involve? (Consider learning and development time and courses as well as hosting)\nwill it allow you to develop automated testing and CI to QA our dashboards?\ndoes it align with the AF expectation to move towards R and Python for analysis?\n\nThe guidance on this site has a focus on R Shiny. However, if you are using PowerBI you should also make use of the department’s PowerBI Dashboard standards, and aim to make use of reusable configurations where possible to ensure that duplication of effort across the department is minimised.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#assessing-engagement",
    "href": "writing-visualising/dashboards.html#assessing-engagement",
    "title": "Public dashboards",
    "section": "Assessing engagement",
    "text": "Assessing engagement\nWork out what success looks like for your service and identify metrics which will tell you what’s working and what can be improved, combined with user research. For dashboards this will likely be things like the number of users and interactions with what you have created.\nDefining what “good” looks like and identifying appropriate metrics means that you’ll know whether the service is solving the problem that it’s meant to solve.\nCollecting the right engagement data means you’ll be alerted to potential problems with your service. And when you make a change to the service, you’ll be able to tell whether it had the effect you expected. In practice this will often mean setting up analytics and feedback surveys for dashboards and monitoring the data you get back.\nAt a minumum you should be requesting feedback from users via a survey hosted on the dashboard and reviewing this on a regular basis. An example of this kind of feedback survey is the beta banner survey on the explore education statistics service.\n\nGoogle Analytics is a free service that collects information on who visits your webpage and how they interact with it.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#accessibility",
    "href": "writing-visualising/dashboards.html#accessibility",
    "title": "Public dashboards",
    "section": "Accessibility",
    "text": "Accessibility\nIn line with recent legislation for public sector websites, all dashboards need to meet the latest Web Content Accessibility Guidance.\nAs a minimum we expect all dashboards to be checked using Lighthouse and have an accessibility statement before being published. In addtion, R Shiny dashboards should be tested with shinya11y.\nYour statement should be written in line with the accessibility statement guidance, and you can make use of a template accessibility statement provided by .gov.uk.\nLighthouse in Google Chrome is an easy way to quickly rate your accessibility. Open your app in Chrome, right click anywhere on the page and select “Inspect”. From there, navigate to “Lighthouse” in the top grey bar, and click “Generate report”. This generates scores for accessibility, best practices and SEO for your application or web page:\n\nAutomated tools can’t check for everything and there’s no substitute for giving your dashboard a manual check. You should consider what users need from your dashboard and ensure that that information is accessible to all. For example, can someone using a screen reader get the same information as you can see on the screen from a downloadable csv or from the alt text that you have provided? Everything that you can see, you should also be able to read with a screen reader. Use the Edge narrator tool to test out how your dashboard works in practice.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#code-testing",
    "href": "writing-visualising/dashboards.html#code-testing",
    "title": "Public dashboards",
    "section": "Code testing",
    "text": "Code testing\nTesting is a way to capture desired behaviour of your code, in such a way that you can automatically verify that it keeps working the way you expect over time. It is essential for making sure that code works the way that you intend it to, and keeps working even after you make changes to the code so that your users have access to a stable service. You need to test your service regularly as part of quality assurance (QA) to make sure that it:\n\nis easy to use for anyone who needs to use it, regardless of the device they’re using\nis stable, secure and works quickly, regardless of how many people need to use it\ncan be iterated quickly to meet changes to user needs or the political environment\n\nTests can come in a variety of shapes and sizes, good starting points for analysts new to testing are the Duck Book, and DfE good code practice.\nYou should aim to automate as much of your testing as possible and run your test suite as part of continuous integration (where your tests form part of your codebase). By testing your code automatically every time you make a change, you’ll be able to find defects more quickly. Getting fast feedback means you can respond to any problems quickly and make changes when you need to. You can also spot defects before they develop into bigger problems that are more complicated and expensive to fix.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#peer-review",
    "href": "writing-visualising/dashboards.html#peer-review",
    "title": "Public dashboards",
    "section": "Peer review",
    "text": "Peer review\nPeer review is a quality assurance activity, where an analyst other than the original author, views and tests the usage of a product or specific piece of code. This allows a fresh pair of eyes to take a look at your work. It validates that you have taken a correct approach and may highlight errors. This constructive feedback helps you to improve the quality. It provides confidence in your work, and ensures that it is fit for purpose.\n\nDashboards must always be peer reviewed within the team they are created.\nDashboards should also be peer reviewed by analysts outside of the subject area of the team.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards.html#iterative-development",
    "href": "writing-visualising/dashboards.html#iterative-development",
    "title": "Public dashboards",
    "section": "Iterative development",
    "text": "Iterative development\nUsing agile methods means getting your service in front of real users as soon as possible. Then observing and generating data on how they use it and iterating the service based on what you’ve learned. Because you’re not specifying everything up front before you’ve developed an understanding of what users need, agile methods reduce the risk of delivering the wrong thing.\nTeams should aim to get a working version of their dashboard out to users as soon as possible to prototype the design and content.\nIteration isn’t just for the early stages of a service’s development and services are never ‘finished’. Using agile methods means getting real people using your service as early as possible. Then making improvements throughout the lifetime of the service.\nMaking improvements means more than doing basic maintenance like fixing bugs in code, deploying security patches and keeping call centre scripts up to date. If that’s all you do, you’ll be fixing symptoms rather than underlying problems. And over time, the service will stop meeting user needs.\nContinuous improvement means you can respond to changes in user needs, technology or government policy throughout the lifetime of the service. So rather than having to be replaced, the service stays relevant until it’s ready to be retired.",
    "crumbs": [
      "Writing and visualising",
      "Public dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/writing.html",
    "href": "writing-visualising/writing.html",
    "title": "Writing about data",
    "section": "",
    "text": "Things to consider when writing about data",
    "crumbs": [
      "Writing and visualising",
      "Writing about data"
    ]
  },
  {
    "objectID": "writing-visualising/writing.html#introduction",
    "href": "writing-visualising/writing.html#introduction",
    "title": "Writing about data",
    "section": "Introduction",
    "text": "Introduction\nCommentary should do much more than just describe the statistics in words. It should help the user to understand the meaning of patterns, trends and limitations, and build on any factual and public information already known about the subject matter.\nClear, insightful and professionally sound commentary supports informed decision-making and democratic debate.\n\nStatistics and data should be presented clearly, explained meaningfully and provide authoritative insights that serve the public good.\n(Code of Practice for Statistics, V3: Clarity and insight, United Kingdom (UK) Statistics Authority, 2018)",
    "crumbs": [
      "Writing and visualising",
      "Writing about data"
    ]
  },
  {
    "objectID": "writing-visualising/writing.html#key-messages",
    "href": "writing-visualising/writing.html#key-messages",
    "title": "Writing about data",
    "section": "Key messages",
    "text": "Key messages\nStart by introducing the topic of your work and the questions you seek to answer with the numbers that follow. To help set the scene for your statistics, begin with a topic sentence that introduces the variables and the W’s (when, where and what).\nExamples (NB. Fictional):\nPoor: (No introductory sentence) “In 2020, there were 11,000 gun-related homicides (Figure 1)”\nThis jumps directly to presenting the data without orientating the reader to the topic and objectives.\nBetter: “What factors explain the observed rise and fall in overall homicides in England in the 2000s?”\nThis uses a rhetorical question to introduce the context (where and when) and the pattern to be investigated (the time trend). However, this does not specify the possible explanatory factors.\nBest: “Was the substantial rise and fall in the number of homicides in the 2000s in England (Figure 1) observed across all age groups and types of weapons (Figure 2)?” There have been no numbers presented yet, just a statement that establishes the purpose of the statistics. Introducing your topic is important especially when presenting a series of charts or tables.\nBelow is a session by Robert Cuffe from the BBC talking to DfE statisticians about writing our publications and highlighting key messages in a way they can be used by journalists and the media.\n\n\n\n\n\n\nHeadline sections\n\nIt is common to use bullet points to draw out key headline messages, either for policy lines or for general interest. Here are some top tips for writing headlines:\n\nAsk yourself if this is the most important, useful and relevant point to make? Why? What is new?\nWe recommend a maximum of six top headlines\nYou shouldn’t be trying to summarise all the findings in the publication\nFor regular publications, headlines won’t necessarily be the same every time\nHeadlines should be a single sentence making a single point, and be able to stand alone from the publication\nHeadlines should make sense to everyone and anyone (no jargon)\nStructure headlines as: what has happened; why is this important - don’t give numbers without context\nAt least of your headlines should put the latest figures in the context of the longer-term change\nRound figures in headlines, you don’t need lots of decimal places\nIf there is essential context for the headline facts then put this here\n\n\n\n\nActive subheadings\n\nAs with active titles for charts, active subheadings are descriptive and tell the trend by highlighting the main story. Active subheadings not only help to structure your writing, but also help users by highlighting the main message(s) you want to convey in your data.\nSee a good example of active subheadings in practice.\n\n\n\nReporting and interpreting numbers\n\nReporting the numbers you work with is an important first step toward writing effective numeric descriptions. By including numbers in text, table or chart, you give the user the raw materials with which to perform comparisons across time, places or groups. However, if you stop there, you leave it to your readers to figure out how those data answer the question at hand.\n\nPoor: “In 2010, there were 20, 370 overall homicides related to crime, 13,000 which were related to gun incidents, 7,370 related to other weapons. In 2020, they were 18,900 overall homicides, 11,000 which were related to gun incidents, 9,900 related to other weapons (Figure 1)”.\n\nThe description above simply lists statistics from charts without explaining how they relate to one another or how the statistics address the initial question in the opening paragraph.\n\nBetter: “The total number of homicides rose until the mid-2000s and then declined until 2020. As shown in Figure 1, the increase and subsequent decrease in homicides were driven by trends in gun-related homicides. In 2020, there was roughly 1.5 times as many homicides were committed with guns as with other types of weapons (11,000 versus 7265; Figure 1); whereas in 2010, roughly 2 times as many homicides were committed with guns versus other weapons, 13000 and 6500, respectively. Figure 2 examines whether gun-related homicides showed the same time trend among all age groups. As shown in Figure 2 in the two youngest groups of offenders, gun-related homicides increased substantially between 2000 and 2010, and then decreased steadily until 2020. In contrast, the number of gun related homicides committed by older offenders decreased slowly throughout the time-period shown”.\n\nTry to use prose to summarise the patterns so your user can see the general relationship in the table or chart – the forest not the individual trees. Try not to report every number from the table or chart or pick a few arbitrary numbers to contrast in sentence form without considering whether or not those numbers represent an underlying general pattern. Paint the big picture rather than reiterating all the little details. This will help you tell a clear story with numbers as evidence.",
    "crumbs": [
      "Writing and visualising",
      "Writing about data"
    ]
  },
  {
    "objectID": "writing-visualising/writing.html#accessibility",
    "href": "writing-visualising/writing.html#accessibility",
    "title": "Writing about data",
    "section": "Accessibility",
    "text": "Accessibility\nYou should always write with your audience in mind. If you are writing something that will be available publicly, make sure that you write content that is accessible for everyone.\nThe Analysis Function have create a guide for what you need to know about accessibility as an analyst.\nFor tips on creating accessible content have a look at our accessibility top tips (.docx).\n\n\nCreating hyperlinks\n\nAvoid using full URLs in text. Hyperlinks should be used and they should provide a clear description of the destination. Avoid using ‘For more information click here’. Screen readers often collate all links on a page into one list, so having numerous ‘click here’ links listed is confusing to the user and gives no description of the destinations. ‘For more information see Guidance to support the summer 2021 exams’ is an example of a good hyperlink.\nBest practice on creating hyperlinks, particularly how to name them and common pitfalls to avoid can be found on this introduction to html guide. Those of us using EES don’t need to worry about writing the raw html for the anchor links, and should instead focus on the section referring to how to name and title links.",
    "crumbs": [
      "Writing and visualising",
      "Writing about data"
    ]
  },
  {
    "objectID": "writing-visualising/writing.html#advice-and-support",
    "href": "writing-visualising/writing.html#advice-and-support",
    "title": "Writing about data",
    "section": "Advice and support",
    "text": "Advice and support\nMore detailed advice and guidance on content design best practice is available from a number of resources across Government and beyond:\n\nGSS best practice guidance\nHow to get readability scores in Microsoft Word\nSimilarly, the Hemingway app is an online tool to assess readability. Whilst we might not want to use this directly for final drafting, it is useful to highlight areas of particular complexity in our current narrative.\nAs an illustrative example of work in other Departments - DWP’s Fraud & Error statistics before (128 page release), and DWP’s Fraud & Error statistics after their own modernisation work are worth a look.\nContent design: writing for GOV.UK\nGOV.UK Technical Content Style Guide A-Z\nGOV.UK Style Guide Terms A-Z",
    "crumbs": [
      "Writing and visualising",
      "Writing about data"
    ]
  },
  {
    "objectID": "RAP/rap-support.html",
    "href": "RAP/rap-support.html",
    "title": "RAP support",
    "section": "",
    "text": "This page outlines the support available for RAP in DfE",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP support"
    ]
  },
  {
    "objectID": "RAP/rap-support.html#support-on-offer",
    "href": "RAP/rap-support.html#support-on-offer",
    "title": "RAP support",
    "section": "Support on offer",
    "text": "Support on offer\nLearning resources and materials for SQL, R, and Git\n\n\nEmail support\n\n\nstatistics.development@education.gov.uk\n\n9-5, aim to reply between 1-2 days.\n\n\n\n\nTechnical workshops\n\n\nIn person workshops covering specific technical skills in practice\n3 hours long, with people working in small groups\nWe currently offer two workshops: introduction to git and Azure DevOps, and introduction to R and RAP. We can travel to your site to deliver these.\nContact statistics.development@education.gov.uk to register interest in workshops happening at your site or to request new topics!\n\nWe’re also considering a dedicated G6 / G7 programme to build confidence and set expectations. This may include:\n\nA 2-hour workshop explaining what teams are expected to do when creating and publishing statistics. Including tools and skills required, RAP, using EES, content design, release clearance, UE + ongoing publishing and where teams can go for help.\nThere will be take away materials that cover lines to take and where to get support.\nAll G6s and G7s are expected to do this, with repeats running to ensure everyone gets a chance to attend.\n\nKeep an eye out on Teams for this programme being advertised and contact statistics.development@education.gov.uk if you’d like to discuss it.",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP support"
    ]
  },
  {
    "objectID": "RAP/rap-support.html#partnership-programme",
    "href": "RAP/rap-support.html#partnership-programme",
    "title": "RAP support",
    "section": "Partnership programme",
    "text": "Partnership programme\nThe Statistics Development Team invites teams to take part in our partnership programme. The programme can help individuals with:\n\nUsing a relevant project to develop new coding skills and improving current confidence\nStreamlining data production processes to free up time for secondary analysis\nImproving the presentation and consistency of statistical products for users\n\nThe partnership programme is a great opportunity to work with the Statistics Development Team, using protected time to work on things that are otherwise usually deprioritised. We understand the pressures of BAU work mean that development time is often hard to fit in, but this programme offers designated support and clear project planning from our team so that these improvements can be achieved. Putting in the work at these early stages will save more time and resource in the long run, and we are keen to support as many teams as possible to free up time in the future for even more interesting analysis.\nThe Ask\nWe ask that you/a designated member of your team are given protected time to work on a specific project, with support from us. Some examples of previous projects we have helped support are:\n\nThe automation of the SEN2 release, creating automated QA reports and all EES outputs at the click of a button.\nThe automation of the HoP rolling brief document, removing arduous tasks like copying, pasting, and formatting in Word.\n\nThe time commitment of the programme will be dependent on the size of the work and what level of support is required. Please get in touch with us if you are interested and would like to discuss further.",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP support"
    ]
  },
  {
    "objectID": "RAP/rap-expectations.html",
    "href": "RAP/rap-expectations.html",
    "title": "RAP expectations",
    "section": "",
    "text": "Expectations for analysts and senior colleagues around RAP",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP expectations"
    ]
  },
  {
    "objectID": "RAP/rap-expectations.html#rap-expectations",
    "href": "RAP/rap-expectations.html#rap-expectations",
    "title": "RAP expectations",
    "section": "RAP Expectations",
    "text": "RAP Expectations\nAs government analysts working with statistics, we are required to ensure that our analysis is reproducible, transparent, and robust, using coding and code management best practices (source GSG competency framework). Reproducible Analytical Pipelines (RAP) are a cross-government requirement to help analysts adopt best practices.\nWe expect any analyst to know and be able to implement RAP principles, using the recommended tools to meet at least good and great practice. You can see examples of good, great, and best practice on our RAP in statistics guidance page.\nWe expect managers of analysts working in statistics production to support and prioritise the development required to build the skills needed to implement RAP using the recommended tools. Managers should ensure that the processes for any publications they own meet at least good and great practice.\nAlthough RAP is often discussed in the context of statistics production, RAP principles can also be applied to other analysis work. The cross-government RAP strategy states a number of explicit expectations for analysts involved in the process, which are detailed in the following sections: analyst leaders, analyst managers and analysts.\n\n\nAnalyst leaders\n\nThose giving senior sign off on publications and running analytical functions, usually G6 and SCS, analyst leaders will:\n\nensure their analysts build RAP learning and development time into work plans\nhelp their teams to work with DDaT professionals to share knowledge\npromote a “RAP by default” approach for all appropriate analysis\nwrite and implement strategic plans to develop new analyses with RAP principles, and to redevelop existing products with RAP principles\nlead their RAP champions to advise analysis teams on how to implement RAP\nhelp teams to incorporate RAP development into workplans\nidentify the most valuable projects by looking at how much capability the team already has and how risky and time-consuming the existing process is\ncommunicate the benefits of RAP to analysts, managers, and users\n\n\n\n\nAnalyst managers\n\nRoughly equivalent to Team Leaders and G7, analyst managers will:\n\nwork with security and IT teams to give analysts access to the right tools\nwork with security and IT teams to develop platforms that are easy for analysts to access, flexible and responsive to the needs of analysts\nwork with security, IT, and data teams to make sure that the tools data analysts need are available in the right place and are easy to access\nbuild extra time into projects to adopt new skills and practices where appropriate\nlearn the skills they need to manage software\nevaluate RAP projects within organisations to understand and demonstrate the benefits of RAP\nmandate their teams use RAP principles whenever possible\n\n\n\n\nAnalysts\n\nAnalysts working on analysis in government will:\n\nuse open-source tools wherever whenever appropriate\nopen source their code\nwork with data engineers and architects to make sure that source data are versioned and stored so that analysis can be reproduced\nlearn the skills they need to implement RAP principles\nengage with users of their analysis to demonstrate the value of RAP principles and build motivation for development\ndeliver their analysis using RAP",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP expectations"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html",
    "href": "learning-development/learning-support.html",
    "title": "Learning support",
    "section": "",
    "text": "A collection of useful learning resources, and information on support and resources to get you started",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#tools-and-support-channels",
    "href": "learning-development/learning-support.html#tools-and-support-channels",
    "title": "Learning support",
    "section": "Tools and support channels",
    "text": "Tools and support channels\nAs analysts and statistics producers, we require a variety of tools to efficiently and reliably work with our data. Below are the recommended tools that give us the most power to do what we need. These have large user communities in DfE, and are already working in our current IT setup.\nFor best practice when using software and coding in our process, see our guidance on RAP expectations and the DfE Good Code Practice guide.\nYou can also access support on a variety of topics on the cross-government Digital Slack instance. Some useful channels to join are #accessibility, #ai, #data-science and #datavis.\nThere is also a cross-government data science Slack instance. Some useful channels to join are #chat-r, #chat-python, #info-events and #community-rap.\nIn DfE, you can also seek advice via Teams. Some useful groups are Statistics production and publishing, DfE R community, DfE Python community and DfE Analyst Network.",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#email-support",
    "href": "learning-development/learning-support.html#email-support",
    "title": "Learning support",
    "section": "Email support",
    "text": "Email support\nFor questions about publishing statistics, the statistics Code of Practice or how your statistics publication should be badged, please contact the HoP Office.\nFor support in data harmonisation and data structuring standards, or support with analytical digital services including the explore education statistics service, and deploying and maintaining R Shiny applications contact the explore education statistics platforms team.\nFor any technical questions about R, Git, SQL, RAP or statistics production, contact statistics.development@education.gov.uk. This mailbox is monitored between 9-5 and we aim to reply within 2 days. No question is too small! We are always happy to help.",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#technical-workshops",
    "href": "learning-development/learning-support.html#technical-workshops",
    "title": "Learning support",
    "section": "Technical workshops",
    "text": "Technical workshops\nWe currently offer two in person workshops covering specific technical skills in practice. Each workshop is around 3 hours long, and you will work in small groups. We travel to deliver workshops based on demand, so please get in touch and let us know if you’d like to join the waiting list in your area for one or both of the sessions.\nOur current workshops are:\nIntroduction to Git and Azure DevOps\nIn this workshop, we cover:\n\nWhat are git, Azure DevOps and GitHub and what are the differences?\nHow can they help your work?\nHow to start working with git and DevOps in the DfE ecosystem\nManaging projects and tasks with DevOps Boards\nBasic version control with git – add/stage, commit, push and pull\nWorking with branches\nTracking changes and who made them\nMerging, pull requests and merge conflicts\n\nIntroduction to R and RAP\nIn this workshop, we cover:\n\nSetting up an R project\nWriting your own functions\nAggregating & filtering data\nUsing if statements\nJoining data\nUsing R to run SQL\nCreating plots (with ggplot2)\n\nContact statistics.development@education.gov.uk to register interest in workshops happening at your site or to request new topics!",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#learning-and-development-resources",
    "href": "learning-development/learning-support.html#learning-and-development-resources",
    "title": "Learning support",
    "section": "Learning and development resources",
    "text": "Learning and development resources\n\nExplore education statistics show and tells\nA monthly opportunity to find out about our upcoming development plans and ask us any questions. You can catch up on previous show and tell sessions on our EES intranet page\nPlease contact statistics.development@education.gov.uk if you would like to be added to the invite list.\n\n\nAI Knowledge shares\nJoin the Artificial Intelligence (AI) Knowledge Shares to hear from volunteers about their AI projects and use of AI in their work. You can catch up on past sessions in the AI knowledge share folder.\nPlease contact statistics.development@education.gov.uk to be added to the mailing list.\n\n\nCoffee and coding\nCoffee & coding is a chance to learn about and discuss coding tools and techniques with guest presenters.\nAnyone in DfE can volunteer to lead a session, and we cover a wide range of topics! You can see all of our previous Coffee & Coding session recordings and slides on the Coffee and Coding Intranet page, as well as details of upcoming sessions.\nPlease contact coffee.coding@education.gov.uk if you would like to sign up to the mailing list, have an idea for a session, or would like to volunteer to lead a session.\n\n\nRAP Champions Network\nReproducible Analytical Pipelines (RAP) Champions act as a point of contact for their division for everything RAP-related: improving the awareness and use of RAP in the department, improving the RAP support available to analysts and keeping track of progress made and areas requiring support.\nYou can find more information on the RAP Champions Network page. Please contact statistics.development@education.gov.uk if you would like to learn more or sign up to be a champion.\n\n\nFreestyle RAP sessions\nFreestyle RAP is a monthly open mic for all things Reproducible Analytical Pipelines! Come along for an hour of collaborative problem-solving, where our Statistics Development Team and a RAP champion are on hand to answer all your RAP questions. There is no set agenda and anyone who has an interest in RAP is welcome!\nContact statistics.development@education.gov.uk to be added to the mailing list!",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#partnership-programmes",
    "href": "learning-development/learning-support.html#partnership-programmes",
    "title": "Learning support",
    "section": "Partnership programmes",
    "text": "Partnership programmes\nThe Statistics Services Unit offers bespoke partnership programmes for analyst teams across the department, designed as dedicated resource to support development work, either building skills or filling resource gaps to allow improvements in analytical or statistical outputs.\nPartnerships are designed as short-term projects, agreed in conjunction with the team and their Grade 6. The maximum length for the majority of partnership programmes will be three months, and the project must be clearly defined at the outset.\nThe aim of partnership programmes is always to upskill the team throughout the project. For this reason, we require any teams taking part to commit time to upskilling during the partnership and being willing to independently continue the work after the partnership is complete, if appropriate. Any agreed projects must be able to be handed back to the team that owns the work, and they must have the skills to maintain it independently by the time the partnership finishes. To enable this, we can offer a variety of support including signposting to resources, recommending relevant L+D, running team workshops and knowledge share sessions, or peer review and pair programming sessions to work side by side to build up skills during the partnership.\n\n\nWhat could a partnership programme involve?\n\n\nFlexible skilled resource for your team to support with R, Git, SQL or Databricks\nHelp with dedicated upskilling of individuals within your team to enable handover of code at the end of the programme\nKnowledge sharing, including support to make decisions on best practice and accessibility, and peer review\nSupport with RAP implementation to allow your team to meet the RAP baseline or best practice criteria\n\nFor example, this might be:\n\nAutomating processes such as data manipulation or QA\nCreation of or improvements to R Shiny dashboards\nReviewing existing analytical code\nSupport with moving to Azure DevOps or GitHub\nCreation of interactive QA reports\n\n\n\n\nPartnership programme tiers\n\nWe have four levels of support available:\n\nOne-off bespoke support via a team meeting or a workshop, or one-off peer review\nTier 1: occasional check-in meetings to provide best practice guidance\nTier 2: hands on support for teams with skills or resource gaps\nTier 3: designated resource working within your team to write code / complete a project for you, designed for complex projects and with the aim of handing the project back to the team at the end of the partnership for continued iteration and maintenance as appropriate\n\nTeams participating in tiers 1-3 will be assigned a named person from the Statistics Services Unit who will support you in completing the project. During the partnership, there will be regular update meetings and progress reports to help keep the work to the agreed timescale.\n\n\n\nHow to get started\n\nOnce you have a clear, defined piece of work that you think would be appropriate for a partnership programme, contact statistics.development@education.gov.uk.\nYou will be sent a form to complete so that you can outline your project and requirements from us. Once we have received your completed form, we’ll have an introductory meeting with your team to discuss the project and agree an appropriate tier and timescales. We will then assess your needs, determine who will be best placed to complete the work, and prioritise it according to our existing workload. Your team will be kept updated throughout the process.\n\n\n\n\n\n\nTier 3 programmes\n\n\n\nPlease note: all new tier 3 partnerships will require sign off at Grade 6 level to ensure that time for skills development within your team can be prioritised in parallel with the work, and to ensure a clear handover plan at the end of the project.\n\n\nIf you are interested in a one off / bespoke workshop or team meeting, please contact statistics.development@education.gov.uk and we will work with your team to meet your development needs.",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#r-sql-and-git-resources",
    "href": "learning-development/learning-support.html#r-sql-and-git-resources",
    "title": "Learning support",
    "section": "R, SQL and Git resources",
    "text": "R, SQL and Git resources\nWe also have specific learning resources and materials for SQL, R, and Git",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#general-resources",
    "href": "learning-development/learning-support.html#general-resources",
    "title": "Learning support",
    "section": "General resources",
    "text": "General resources\nGoogle and other web-search engines are the single most powerful learning resource out there, whether it leads you to a Stack Overflow answer to your problem, or to an online training course, it will have your needs covered. We appreciate that it can be daunting and overwhelming at first though, which is why we’re pooling together links to particularly relevant resources on this page. Let us know if there’s any you’d like to see added!\n\nThe DfE Analytics Academy online R training course - walks you through how to get set up with R, as well as setting tasks using DfE data to learn fundamental skills in R.\nHow to QA - a general guide to quality assurance best practice in DfE.\nDfE Data Science resource tool - a bank of learning materials, from coffee and coding talks to online guidebooks.\nESFA guide to using R and Git - take you from the beginning, teaches you how to use RStudio and later on even includes some information on Shiny apps and more complex topics. For more information on RShiny apps, join the R Shiny Developers teams group.\nConnor Quinn’s resources for Data Engineering - for more advanced SQL and database resources this is a fantastic place to go.\nGeneral data science resources - includes plenty of open-source resources for R and SQL, with Python, Git, and shell included in there too.\nAwesome lists and awesome R - another great bank of learning resources.\nData Science week streams - a variety of talks across Data Science within the department.\nDfE Quality Assurance wiki - much of this is aimed at areas that use modelling, plenty of it is applicable to how we QA our data too.\nDavid Sands’ Boons of R and Git - a helpful guide on why R and Git are worth your time.",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/learning-support.html#further-information",
    "href": "learning-development/learning-support.html#further-information",
    "title": "Learning support",
    "section": "Further information",
    "text": "Further information\nFor more information on how to use these tools in analysis and statistics publications, please see the processes and RAP page.\nThis is by no means a finite list of resources, we want this to be added to and for it to develop over time - if you have any resources that you think could be added then we’d love to hear from you, so please email us at statistics.development@education.gov.uk.\nIf you are stuck at all, have any questions, want to find a resource, or even just want a second pair of eyes to double check something, contact us using the envelope in the top left corner and we’ll be more than happy to help.",
    "crumbs": [
      "Learning resources",
      "Learning support"
    ]
  },
  {
    "objectID": "learning-development/accessibility.html",
    "href": "learning-development/accessibility.html",
    "title": "Accessibility",
    "section": "",
    "text": "Some quick links and tips for increasing awareness of accessibility in digital content",
    "crumbs": [
      "Learning resources",
      "Accessibility"
    ]
  },
  {
    "objectID": "learning-development/accessibility.html#accessibility-in-dfe",
    "href": "learning-development/accessibility.html#accessibility-in-dfe",
    "title": "Accessibility",
    "section": "Accessibility in DfE",
    "text": "Accessibility in DfE\nEveryone has a part to play in ensuring that the content and digital services the DfE provides to its customers and colleagues are accessible. Everyone should have an understanding of their responsibilities to comply with the Public Sector Bodies Accessibility Regulations 2018 and the Web Content Accessibility Guidelines 2.2. Moreover, everyone should also understand the important and critical difference this makes to the everyday lives of millions of people.\nDfE have our own Digital Accessibility Hub filled with helpful information on making the digital world more accessible.\nThere is also the DfE design manual, which has been created by the digital teams at DfE and includes DfE’s digital accessibility guidance.\nIf you want to estimate or try to appreciate just how many people might have specific needs, there is the How many people? tool, simply enter the size of your expected audience, and it will give you best estimates of how many people might have a disability, impairment or other characteristics which might affect how they use your service.",
    "crumbs": [
      "Learning resources",
      "Accessibility"
    ]
  },
  {
    "objectID": "learning-development/accessibility.html#colour-accessibility-tools",
    "href": "learning-development/accessibility.html#colour-accessibility-tools",
    "title": "Accessibility",
    "section": "Colour accessibility tools",
    "text": "Colour accessibility tools\nContrast:\n\nWebAim (comparing two colours)\nColour contrast matrix (comparing multiple colours\n\nColour blindness:\n\nAdobe colour accessibility (gives a pass / fail)\nCoblis (simulates colour blindness for uploaded images)\nColourBlindCheck (R package for simulating colour blindness for a colour palette)\n\nGrey scale:\n\nYou can often check colours in greyscale by using a print preview and switching the colour mode\nColToGrey function within the DescTools R package\n\nFor more advice about colour in charts and visualisations specifically, including more tools and resources for checking colours yourself, see the Analysis function colours in charts guidance.",
    "crumbs": [
      "Learning resources",
      "Accessibility"
    ]
  },
  {
    "objectID": "learning-development/accessibility.html#free-tools-for-reviewing-web-pages",
    "href": "learning-development/accessibility.html#free-tools-for-reviewing-web-pages",
    "title": "Accessibility",
    "section": "Free tools for reviewing web pages",
    "text": "Free tools for reviewing web pages\n\naxe DevTools, free Google Chrome extension and paid versions, we recommend this (and Government Digital Service tend to recommend too)\nGoogle Lighthouse, built into Google Chrome browser, and will catch some basic accessibility things amongst other web related issues\n\n‘Bookmarklets’ are bookmarks that instead of saving a URL to a page, save a piece of Javascript code that executes on the page you are looking at. There’s some nice accessibility focused bookmarklets that highlight specific types of mark up such as headers and lists so you can easily check if it matches what you’d expect while on any webpage. Very nifty and low effort too!",
    "crumbs": [
      "Learning resources",
      "Accessibility"
    ]
  },
  {
    "objectID": "learning-development/accessibility.html#assistive-technology",
    "href": "learning-development/accessibility.html#assistive-technology",
    "title": "Accessibility",
    "section": "Assistive technology",
    "text": "Assistive technology\nNo automated tools fully cover accessibility. Manual testing is almost always required to make sure that your content is compliant with WCAG 2.2 and accessible to as many users as possible, so if you want to do manual testing have a look at testing out the assistive software commonly used by users yourself.\nMagnifiers / advanced zoom:\n\nWindows Magnifier comes as standard with Windows (guidance for how to turn on Magnifier)\nZoomText, available through the experience lab in DfE\n\nScreen reader:\n\nNon-visual Desktop Access (NVDA) free download\nJob Access With Speech (JAWS), available through the experience lab in DfE\n\nVoice activation:\n\nDragon, available through the experience lab in DfE\n\nCognitive load:\n\nWhile there isn’t commonly used specific assistive technology for aiding users in reducing cognitive load, you can take many steps to reduce the cognitive load for users on your service, making it simpler to use for all. Have a read of Cognitive Load as a Guide: 12 Spectrums to Improve Your Data Visualisations as a starting point if you want to learn more.\n\nAlong with devices and the software mentioned above, the experience lab in the Sheffield DfE office also has a range of other equipment available, including access to a set of vision emulating glasses that you can wear to emulate different visual impairments.",
    "crumbs": [
      "Learning resources",
      "Accessibility"
    ]
  },
  {
    "objectID": "learning-development/accessibility.html#making-spreadsheets-accessible",
    "href": "learning-development/accessibility.html#making-spreadsheets-accessible",
    "title": "Accessibility",
    "section": "Making spreadsheets accessible",
    "text": "Making spreadsheets accessible\nPublic sector spreadsheets must meet the AA level of the Web Content Accessibility Guidelines (WCAG) 2.2, as required by law under The Public Sector Bodies (Websites and Mobile Applications) Accessibility Regulations 2018. This guidance distinguishes between mandatory actions needed to comply with legal accessibility standards and best practices that enhance usability. Non-compliance can lead to legal complaints, making it crucial for content creators to understand and mitigate these risks.\nWhen creating an accessible spreadsheet, it’s important to consider the needs of all users, including those with disabilities. To support analysts in making spreadsheets more accessible, apply the following strategies when creating spreadsheets:\n\n\nConsult the spreadsheet accessibility guidance\n\nBefore diving into the spreadsheet creation process, it’s essential to familiarise yourself with the official spreadsheet accessibility guidance available on the Analysis Function Guidance Hub. This resource offers comprehensive insights into best practices for ensuring that your spreadsheets are accessible to all users.\n\n\n\nAccessibility and consequences\n\nIn addition to accessibility, the guidance also addresses usability and machine readability. While these areas often overlap with accessibility, there are instances where they may conflict. Depending on user needs, it might be necessary to produce separate versions of a spreadsheet (one for human use and another optimised for machine readability) to ensure both accessibility and functionality are maintained.\n\n\n\nAutomate accessibility during the coding process\n\nAutomation is a powerful way to ensure that accessibility is consistently applied throughout the spreadsheet creation process, especially when working with large datasets or producing numerous spreadsheets. By integrating accessibility into the coding process, you can streamline your workflow and reduce the chances of missing critical accessibility features.\nHere’s how you can automate accessibility using specific tools in Python and R:\n\nWhy automate?\n\nConsistency: Automation helps maintain a consistent approach to accessibility across all spreadsheets, reducing the likelihood of human error.\nEfficiency: Automating the process saves time, especially when dealing with large datasets or generating numerous tables.\nCompliance: These tools help ensure that your spreadsheets meet legal accessibility requirements without extensive manual adjustments, reducing the risk of non-compliance.\n\nBy incorporating tools like gptables or a11ytables into your coding workflow, you can enhance the accessibility of your spreadsheets with minimal extra effort, making your data more inclusive and easier to use for everyone.\n\n\nProducing accessible tables in R using a11ytables:\n\nOverview: The a11ytables package in R serves a similar purpose by helping users create accessible tables within their spreadsheets. The package includes functions that facilitate the creation of tables with correct headers, descriptive text, and appropriate structures, making them accessible to all users, including those relying on assistive technologies.\nBenefits: Using a11ytables ensures that your spreadsheets are automatically optimized for accessibility, minimising the need for manual intervention. It’s particularly useful when producing multiple tables, as it maintains consistent formatting and accessibility standards throughout your documents.\n\n\n\nProducing accessible tables in Python using gptables package:\n\nOverview: The gptables package is designed to help Python users create accessible tables within spreadsheets. This package automates the formatting of tables to meet accessibility standards, ensuring that key elements such as table headers, data descriptions, and structures are correctly implemented.\nBenefits: By using gptables, you can automatically generate tables that are readable by screen readers, properly formatted for navigation, and compliant with accessibility regulations. This reduces the manual effort required to adjust table formats and enhances consistency across multiple spreadsheets.\nNote on automation packages: Neither of these packages are guaranteed to automatically produce perfectly accessible spreadsheets. The aim is to help you automate some of the edits.\n\n\n\n\n\nRun an accessibility check\n\nBefore finalising your spreadsheet, it’s crucial to run it through an accessibility checker to identify potential issues. Here are two options you can use:\n\nExcel Built-in Accessibility Checker: If you’re using a newer version of Excel, take advantage of the built-in accessibility checker. To use it, go to the ‘Review’ ribbon and select ‘Check Accessibility’. This tool will flag potential accessibility issues, such as missing alt text for tables. However, it’s important to treat this checker like a spelling and grammar tool—it might miss certain issues or flag irrelevant ones. For instance, if your tables are correctly marked up and named, you don’t need to worry about adding alt text. Additionally, if you save your spreadsheet in Open Document Spreadsheet (ODS) format (recommended if your publishing platform supports it), any alt text may be removed automatically.\nCustom-built XLSX Accessibility Checker: You can also use the custom-built XLSX accessibility checker, which is specifically designed using the Analysis Function’s ‘making spreadsheets accessible’ checklist. This experimental tool helps identify a range of accessibility issues based on the checklist’s criteria. While it’s a powerful tool, keep in mind that it’s still under development, so some aspects of accessibility will need to be checked manually.\n\nBy using one or both of these tools, you can ensure that your spreadsheet is as accessible as possible, though a final manual review is recommended to catch any issues that may have been overlooked.\n\n\n\nChecklist\n\nYou should check your spreadsheet against the Accessible Spreadsheets Checklist before publication.\nBefore publishing your spreadsheet, ensure that it meets accessibility standards by reviewing it against the following key points:\n“If a point in the checklist has ‘(E)’ after it, it means it has been interpreted as essential to passing the accessibility regulations”.\n\n\nTables\n\n\nMark Up Tables (E): Ensure all tables are properly marked up to assist screen readers in interpreting the content. Meaningful Names: Assign meaningful names to each table.\nRemove Complexities (E): Eliminate merged, split cells, and nested tables as they hinder accessibility.\nSimplify Structure (E): Remove any blank rows or columns within tables and ensure that each table has a tagged header row. Avoid filters and freeze panes unless absolutely necessary, providing clear instructions on their use.\nText and Cell Management (E): Ensure text within cells is wrapped, and only leave cells empty when absolutely necessary, explaining any blank cells with a note.\nAvoid Hidden Elements (E): Refrain from hiding rows or columns, or provide guidance if needed. Column Width: Adjust column widths to a sensible size to improve readability.\n\n\n\n\nFootnotes\n\n\nAvoid Symbols and Superscripts (E): Instead of symbols or superscript text, use plain text or shorthand within square brackets. Use ‘note’ with numbers in square brackets for footnotes.\nPlacement: Place footnote markers in titles, column headings, or a notes column on the right. Avoid placing them directly in cells.\nNotes Worksheet: Include all footnotes in a ‘Notes’ worksheet rather than below the table.\n\n\n\n\nFormatting\n\n\nWritten Content (E): Ensure all written content follows accessibility guidelines.\nLinks (E): Use descriptive text for hyperlinks rather than URLs.\nText Formatting: Use a sans-serif font, size 10 or larger, avoid italics and cell borders, and ensure text is horizontal. Avoid highlighting text with a background fill unless contrast is checked.\nWorksheet Titles (E): Each worksheet should have a descriptive title in Cell A1, formatted appropriately. Avoid Symbols: Use symbols sparingly and avoid headers, footers, floating text boxes, or toolbars (E).\nColour Use (E): Do not rely on colour alone to convey information, and ensure adequate contrast if colour is used.\nAvoid Images: If images are necessary, ensure they have alternative text.\nRemove Macros: Macros should be removed to avoid accessibility and security issues.\n\n\n\n\nStructure\n\n\nWorksheet Names (E): Give each worksheet a unique name or number.\nRemove Blank Sheets (E): Eliminate any blank worksheets.\nColumn A Usage (E): Place essential information in column A, above tables, including explanations of symbols, filters, or shorthand.\nPositioning (E): Align tables against the left-hand edge of the sheet and avoid placing content below tables.\nMultiple Tables: If multiple tables are needed on one sheet, refer to the specific guidance. Before Publishing\nSpelling and Grammar (E): Run a spelling and grammar check to ensure accuracy.\nAccessibility Check: Use Excel’s accessibility checker, though be aware it might not catch everything. Consider using additional tools to identify issues.\nDocument Information (E): Ensure the document’s title and language information are completed.\nFinal Save (E): Position the cursor in cell A1 of the first worksheet before the final save to ensure it opens correctly.",
    "crumbs": [
      "Learning resources",
      "Accessibility"
    ]
  },
  {
    "objectID": "learning-development/python.html",
    "href": "learning-development/python.html",
    "title": "Python",
    "section": "",
    "text": "Guidance and tips for using Python",
    "crumbs": [
      "Learning resources",
      "Python"
    ]
  },
  {
    "objectID": "learning-development/python.html#what-is-python",
    "href": "learning-development/python.html#what-is-python",
    "title": "Python",
    "section": "What is Python",
    "text": "What is Python\nPython is a high-level, general-purpose programming language. It has a number of uses, some of which are statistical analysis and data visualisation. Python code has a strong emphasis on readability and formatting. Although the majority of analysts in DfE tend to use R or SQL, Python is also available as an option.",
    "crumbs": [
      "Learning resources",
      "Python"
    ]
  },
  {
    "objectID": "learning-development/python.html#how-to-install-python",
    "href": "learning-development/python.html#how-to-install-python",
    "title": "Python",
    "section": "How to install Python",
    "text": "How to install Python\nYou can download Python (language) and PyCharm (IDE) from the DfE software center.",
    "crumbs": [
      "Learning resources",
      "Python"
    ]
  },
  {
    "objectID": "learning-development/python.html#best-places-to-start",
    "href": "learning-development/python.html#best-places-to-start",
    "title": "Python",
    "section": "Best places to start",
    "text": "Best places to start\nFor getting set up, and help through any initial issues we recommend making use of the DfE Python community on Teams.\nThere is guidance on the Python Wiki for programmers and non-programmers. Each of the guides includes a list of interactive courses that you can work through to improve your skills.\nThe ONS Data Science Campus offers training courses on Reproducible Analytical Pipelines (RAP) in Python, but be aware that there is less Python expertise in the Department which might make QA of code more difficult.",
    "crumbs": [
      "Learning resources",
      "Python"
    ]
  },
  {
    "objectID": "learning-development/python.html#best-practice",
    "href": "learning-development/python.html#best-practice",
    "title": "Python",
    "section": "Best practice",
    "text": "Best practice\nPython is different to other programming languages in that it has its own style guide, PEP-8. PEP-8 is one of many guidance documents known as Python Enhancement Proposals (PEPs) which govern the style of the language, any future changes or updates, and backwards compatibility features. PEP-20 outlines the guiding principles for Python’s design.\nPython can be a little fussy about the layout of your code, especially whitespace, and your code may fail if it is not indented correctly.",
    "crumbs": [
      "Learning resources",
      "Python"
    ]
  },
  {
    "objectID": "learning-development/python.html#how-to-work-with-python",
    "href": "learning-development/python.html#how-to-work-with-python",
    "title": "Python",
    "section": "How to work with Python",
    "text": "How to work with Python\n\nLibraries and packages\nPython packages work slightly differently to R packages. You might hear reference to libraries in Python. A library is a collection of related packages that perform similar functions. One of the most popular libraries is called NumPy and it allows you to work with numerical data more easily.\nTo access the features of a library and use them within your code, you need to install the library. Using NumPy as the example, you can do this using the syntax\n\npip install numpy\n\nPIP is the standard package installer for Python and so must be mentioned at the start of the line of code.\nOnce you have installed your library, you then need to import it so that you can use it within your code. To make your life easier, you can give it a shortened name so that you don’t have to write NumPy every time you want to use one of the features, like this:\n\nimport numpy as np\n\nIf you only install a library and then don’t import it, you won’t be able to use its features. You can either import a library as a whole, or you can just import certain functions from it if you know you won’t need to use them all.\nNumPy contains an array function. If you want to use it, you would do so as follows:\n\narr = np.array( [[ 1, 2, 3],[ 4, 2, 5]] )\n\nNote that you must use np. before array to get it to work, otherwise Python won’t know where to look for the array function. When you imported NumPy, if you gave it a different shortened name, you’d use that name instead.",
    "crumbs": [
      "Learning resources",
      "Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysts’ Guide",
    "section": "",
    "text": "This website is a guide aimed at anyone working in analysis or statistics in the Department for Education (DfE). It includes tips on best practice and learning resources across a number of areas.\nWe hope it can prove a useful community driven resource for everyone from the most experienced analyst right through to those just starting out. If you have any feedback, suggested additions, or wish to challenge any of the guidance, feel free to use the GitHub links to suggest changes directly, or contact us at the email you can find at the bottom of the page.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#learning-and-development",
    "href": "index.html#learning-and-development",
    "title": "Analysts’ Guide",
    "section": "Learning and development",
    "text": "Learning and development\nLearning support - Useful learning resources, and support to get you started\nSQL - Guidance and tips for accessing data via databases with SQL\nR - Guidance and tips for using R\nGit - Guidance and tips for version control with Git\nPython - Guidance and tips for using Python\nAnalytical Data Access (ADA) - Information on the ADA project and guidance for analysts on how to interact with and use data stored in ADA using Databricks\nDatabricks fundamentals - Fundamental concepts in Databricks that will help you navigate and understand the platform\nDatabricks notebooks - Guidance on Notebooks in Databricks\nDatabricks workflows - Guidance on Workflows in Databricks\nAccessibility - Tools and resources for digital accessibility",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#statistics-production",
    "href": "index.html#statistics-production",
    "title": "Analysts’ Guide",
    "section": "Statistics production",
    "text": "Statistics production\nRoutes for publishing - Guidance for how to publish different types of statistics\nRAP in statistics - Detailed RAP guidance for statistics publications\nOpen data standards - Guidance on how to structure data files\nStatistics API data standards - Guidance on the standards to meet for API data sets\nExplore education statistics (EES) - Tips on using the explore education statistics service\nGood examples in EES - Good practice examples in the explore education statistics service\nEmbedded visualisations in EES - How to embed R Shiny charts in EES publications\nPublication scrums - Information on the scrums we run and tips for writing statistical commentary\nUser engagement - Guidance on understanding and engaging with the users of published statistics\nEES analytics - Understanding how users are interacting with your publications",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#writing-and-visualising",
    "href": "index.html#writing-and-visualising",
    "title": "Analysts’ Guide",
    "section": "Writing and visualising",
    "text": "Writing and visualising\nPublic dashboards - Guidance for publishing public facing statistics dashboards\nPublishing an R Shiny based dashboard - Guidance for publishing public facing statistics dashboards using R Shiny\nVisualising data - Resources and best practice to guide you when visualising data\nWriting about data - Resources and best practice for writing about data",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#reproducible-analytical-pipelines-rap",
    "href": "index.html#reproducible-analytical-pipelines-rap",
    "title": "Analysts’ Guide",
    "section": "Reproducible Analytical Pipelines (RAP)",
    "text": "Reproducible Analytical Pipelines (RAP)\nRAP in statistics - Detailed RAP guidance for statistics publications\nRAP expectations - Guidance for all analysts on expectations of RAP\nRAP support - Details on support available for RAP in DfE\nRAP FAQs - Frequently asked questions about RAP",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#databricks-setup-guides",
    "href": "index.html#databricks-setup-guides",
    "title": "Analysts’ Guide",
    "section": "Databricks setup guides",
    "text": "Databricks setup guides\nSQL Warehouse with RStudio - Guidance for analysts on how to connect to a Databricks SQL Warehouse from RStudio\nPersonal cluster with RStudio and odbc / DBI - Guidance for analysts on how to connect to a Databricks personal cluster from RStudio using the odbc / DBI method\nPersonal cluster with RStudio and sparklyr - Guidance for analysts on how to connect to a Databricks personal cluster from RStudio using sparklyr\nUse Databricks with Git - Guidance for connecting Databricks to Git repositories\nScript workflows in Databricks - Guidance for scripting workflows in the Databricks interface\nScript workflows in RStudio - Guidance for scripting workflows in RStudio",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact-us",
    "href": "index.html#contact-us",
    "title": "Analysts’ Guide",
    "section": "Contact us",
    "text": "Contact us\nOur mailbox is always monitored and is available for anyone in DfE to ask questions about statistics, whether that is about RAP, building dashboards, coding support, learning and development or statistics publications.\n\nstatistics.development@education.gov.uk\n9am-5pm, Monday-Friday, aim to reply within 1-2 days",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to the Analyst’s Guide",
    "section": "",
    "text": "Thank you for investing your time in contributing to the analyst’s guide! All contributions are very welcome - we want this to be a useful resource for the whole analytical community in DfE :sparkles:.\nRead our Code of Conduct to keep our community approachable and respectable.\nThe main branch of the site is protected and can only be updated via pull requests. All pull requests must be approved by a repository admin before merging.\nIn this guide you will get an overview of the contribution workflow from opening an issue, creating a PR, reviewing, and merging the PR.\n\n\nTo get an overview of the project, read the README. Here are some resources to help you get started with open source contributions:\n\nFinding ways to contribute to open source on GitHub\nSet up Git\nGitHub flow\nCollaborating with pull requests\n\n\n\nFor more information specific to how a quarto site works, and for examples of what is possible within a static quarto site, see the main Quarto website.\nHadley Wickham’s book, R for Data Science, gives the following intro to Quarto:\n\n\"Quarto is an open-source scientific and technical publishing system. It allows you to create dynamic documents that weave together narrative text, code, and output, such as plots and tables.\"\n\n\n\nThe Visual Editor in RStudio offers a WYSIWYM (What You See Is What You Mean) interface for creating Quarto documents. These documents, saved as .qmd files, are written using Markdown—a lightweight syntax for formatting plain text. Specifically, Quarto employs Pandoc Markdown, an enhanced version of Markdown, which supports a variety of elements such as tables, citations, cross-references, footnotes, divs/spans, definition lists, attributes, raw HTML/TeX, and more. Additionally, Quarto allows for the execution of code cells with inline output display. Although Markdown is designed to be straightforward, it still necessitates learning some new syntax.\nFor those who are new to computational documents like .qmd files but have experience with tools like Google Docs or MS Word, the visual editor in RStudio is the most user-friendly way to begin working with Quarto. In the visual editor, you can use the toolbar buttons to insert images, tables, cross-references, and other elements.\n\n\n\nThe visual editor also simplifies inserting images and customizing their display. You can paste an image directly from your clipboard into the visual editor, which will save a copy of the image in your project directory and link to it. Alternatively, you can use the Insert &gt; Figure / Image menu to browse for the image you want to insert or paste its URL. This menu also allows you to resize the image, add captions, alternative text, and links.\nThe visual editor offers many additional features that become evident as you gain more experience with it.\nMost importantly, while the visual editor displays your content with formatting, it saves everything in plain Markdown. You can easily switch between the visual and source editors to view and edit your content in either format.\n\n\n\nIn Quarto, you can add links using the following Markdown syntax:\n\nInline Links: [Link Text] (https://example.com)\nReference Links: [Link Text][1]\n\n[1]: https://example.com\nIn Quarto, you can also include links within various contexts, such as code blocks or embedded within other content structures, depending on the complexity of your document. However, the basic Markdown link syntax will generally cover most use cases.\n\n\n\nExample for adding links\n\n\n\n\n\n\n\nYou can embed videos in Quarto documents using HTML, Markdown, or specialised Quarto syntax. Here are some methods:\n\nUsing a shortcode:\n\n{{&lt; video url &gt;}}\n\nFor example, here we embed a YouTube video:\n\n{{&lt; video https://www.youtube.com/embed/wo9vZccmqwc &gt;}} \n\n\nEmbedding videos in other formats:\n\n\nIn HTML formats the video will be embedded within the document.\nFor other formats, a simple link to the video will be rendered.\n\n\n\n\n\nAudio Files: You can embed audio files using the HTML  tag.\nInteractive Content with HTML Widgets: Quarto supports various HTML widgets that can add interactive content to your document.\n\n\n\n\n\nRelative Paths: Use relative paths for local files to ensure your document remains portable.\n\nIn R, relative paths refer to file or directory paths specified in relation to the current working directory. Relative paths help manage current working files.\nIf you’re struggling with running your files across different paths in your working directories, then the here package can sometimes help you solve those types of issues.\n\nTest Across Browsers: Ensure multimedia content works across different browsers and devices.\nOutput Formats: Verify that multimedia elements render correctly in the intended output formats (HTML, PDF, etc.).\n\nBy following these methods, you can effectively add videos, images, audio, and interactive content to your Quarto documents, enhancing their informativeness and engagement.\n\n\n\n\nRedirecting pages in a Quarto project for R involves creating a _quart.yml configuration file where you can define redirects.\n\nCreate/Edit_quarto.yml : Ensure you have a _quarto.yml file in your Quarto project.\nAdd redirects: In _quarto.yml, define redirects like this: - From: old-page.html to: new-page.html\nRender Project : Run quarto render in your project directory to apply changes.\n\n4.Verify: Test old URLs to ensure they redirect correctly.\nThis sets up URL redirects for pages that have changed or been removed in your Quarto project.\n\n\n\nWhen changing headings in Quarto project, it’s crucial to ensure that all links referencing these headings are still correct. Here’s a quick guide to check for broken links:\n\nChange Headings: Update your headings in the relevant .qmd files.\nRun Quarto’s Built-in Link Checker: Quarto has a built in feauture to check for broken links.\n\nRun the following command in your project directory:\nquarto check\nThis command will scan your project for broken links and report any issues.\n\nManually Update Links: If any broken links are reported, update them to match the new headings. Use the correct anchors, which typically follow the format #new-heading-text.\nRe-render Project: After updating links, re-render your project: quarto render\nDouble-Check: Manually verify a few pages to ensure that the links work as expected.\n\n\n\n\n\nIf you changed a heading from ## Old Heading to ## New Heading, make sure all links like [link text](#old-heading) are updated to [link text](#new-heading). By following these steps, you can efficiently check for and fix broken links when changing headings in your Quarto project.\n\n\nHere’s a streamlined guide for building your Quarto project locally before raising a pull request (from the terminal):\n\nPull Latest Changes: Make sure your local branch is up-to-date with the main branch. git pull origin main\nMake Your Changes: Edit your .qmd files or other relevant files in the project.\nBuild the Project Locally: Render the entire project to catch any errors. quarto render\n\n\n\n\nCheck for Errors and Warnings: Review the console output and fix any issues.\nPreview the Project: Open the generated files in a browser to ensure everything looks correct. quarto preview\nCheck for Broken Links: Use Quarto’s link checker to find and fix broken links. quarto check\nCommit Your Changes: Add and commit your changes to your local repository. git add . git commit -m \"Describe your changes\"\nPush Your Changes: Push your changes to your feature branch. git push origin your-feature-branch\nCreate a Pull Request: Go to your repository platform (e.g., GitHub) and create a pull request from your feature branch to the main branch.\n\nFollowing these steps ensures your changes are correctly implemented and verified locally, minimising potential issues during the review process.\n\n\n\n\nYou can modify the highlight-style element in the quarto.yml file to change the display of code blocks using predefined themes. This will change the appearance of all code snippets across the entire Analysts’ guide. The current theme is set to “printing” but there is a list of other available themes on the Quarto website.\nSome things to be aware of before you make changes:\n\nSome of the themes are adaptive, meaning that if you change the site view from dark mode to light mode then the theme will also change accordingly.\nThe appearance of code snippets will only change if the language (e.g. R, Python) is defined at the start of the snippet (e.g. ``` {r connection_example, eval=FALSE}). Including eval=FALSE stops your code snippet from actually running. If no language is defined, the snippet appearance will need to be modified manually and this can cause some issues.\n\n\n\n\n\nTo add in call out blocks for text, use the built-in Quarto callouts as follows:\n::: {.callout-note}\nText \n:::\nThere are five different types of callouts available: note, warning, important, tip and caution. The name you use will be displayed at the top of the box.\nTo customise the title of the callout box, add a title within the callout code like this:\n::: {.callout-warning}\n## Title of my warning\nText \n:::\nYou can find more information on formatting callout blocks on the Quarto website.\n\n\n\nYou can create images, flowcharts, and diagrams using draw.io. To add them to the Analysts’ guide, we recommend embedding them, by selecting File &gt; Embed &gt; HTML. The image below shows the options that you should select when embedding a diagram\n\n\n\n\n\n\nIf you spot a problem with the site, search if an issue already exists. If a related issue doesn’t exist, you can open a new issue using a relevant issue form.\n\n\n\nScan through our existing issues to find one that interests you. If you find an issue to work on, you are welcome to open a PR with a fix.\n\n\n\n\nKey things to remember when making or proposing changes:\n\nWhere guidance already exists elsewhere it should be linked to rather than duplicated\nWhen adding images, save them in the /images folder\nWhen adding files for download, save them in the /resources folder - Don’t commit rendered .html files for pages\nIf your changes remove or edit a pre-existing anchor link, consider how that will be redirected for users who may have bookmarked it\n\n\n\nClick Edit this page at the bottom of the right hand table of contents of any page to make small changes such as a typo, sentence fix, or a broken link. This takes you to the .Qmd file where you can make your changes and create a pull request for a review.\n\n\n\n\nClone or fork the repository.\nOpen the repository in your editor of choice, e.g. R Studio.\nCreate a working branch and start with your changes!\nCommit and push the changes to your working branch.\n\n\n\n\n\nAll pull requests should be made against the main branch.\nWhen you’re finished with the changes, create a pull request, also known as a PR.\n\nDon’t forget to link PR to issue if you are solving one.\n\nOnce you submit your PR, a repository admin will review your proposal. We may ask questions or request additional information.\n\nWe may ask for changes to be made before a PR can be merged, either using suggested changes or pull request comments. You can apply suggested changes directly through the UI. You can make any other changes and then commit them to your branch.\nAs you update your PR and apply changes, mark each conversation as resolved.\nIf you run into any merge issues, checkout this Git tutorial to help you resolve merge conflicts and other issues.\n\n\n\n\n\nIf you need any assistance at all, please contact statistics.development@education.gov.uk."
  },
  {
    "objectID": "CONTRIBUTING.html#new-contributor-guide",
    "href": "CONTRIBUTING.html#new-contributor-guide",
    "title": "Contributing to the Analyst’s Guide",
    "section": "",
    "text": "To get an overview of the project, read the README. Here are some resources to help you get started with open source contributions:\n\nFinding ways to contribute to open source on GitHub\nSet up Git\nGitHub flow\nCollaborating with pull requests\n\n\n\nFor more information specific to how a quarto site works, and for examples of what is possible within a static quarto site, see the main Quarto website.\nHadley Wickham’s book, R for Data Science, gives the following intro to Quarto:\n\n\"Quarto is an open-source scientific and technical publishing system. It allows you to create dynamic documents that weave together narrative text, code, and output, such as plots and tables.\"\n\n\n\nThe Visual Editor in RStudio offers a WYSIWYM (What You See Is What You Mean) interface for creating Quarto documents. These documents, saved as .qmd files, are written using Markdown—a lightweight syntax for formatting plain text. Specifically, Quarto employs Pandoc Markdown, an enhanced version of Markdown, which supports a variety of elements such as tables, citations, cross-references, footnotes, divs/spans, definition lists, attributes, raw HTML/TeX, and more. Additionally, Quarto allows for the execution of code cells with inline output display. Although Markdown is designed to be straightforward, it still necessitates learning some new syntax.\nFor those who are new to computational documents like .qmd files but have experience with tools like Google Docs or MS Word, the visual editor in RStudio is the most user-friendly way to begin working with Quarto. In the visual editor, you can use the toolbar buttons to insert images, tables, cross-references, and other elements.\n\n\n\nThe visual editor also simplifies inserting images and customizing their display. You can paste an image directly from your clipboard into the visual editor, which will save a copy of the image in your project directory and link to it. Alternatively, you can use the Insert &gt; Figure / Image menu to browse for the image you want to insert or paste its URL. This menu also allows you to resize the image, add captions, alternative text, and links.\nThe visual editor offers many additional features that become evident as you gain more experience with it.\nMost importantly, while the visual editor displays your content with formatting, it saves everything in plain Markdown. You can easily switch between the visual and source editors to view and edit your content in either format.\n\n\n\nIn Quarto, you can add links using the following Markdown syntax:\n\nInline Links: [Link Text] (https://example.com)\nReference Links: [Link Text][1]\n\n[1]: https://example.com\nIn Quarto, you can also include links within various contexts, such as code blocks or embedded within other content structures, depending on the complexity of your document. However, the basic Markdown link syntax will generally cover most use cases.\n\n\n\nExample for adding links\n\n\n\n\n\n\n\nYou can embed videos in Quarto documents using HTML, Markdown, or specialised Quarto syntax. Here are some methods:\n\nUsing a shortcode:\n\n{{&lt; video url &gt;}}\n\nFor example, here we embed a YouTube video:\n\n{{&lt; video https://www.youtube.com/embed/wo9vZccmqwc &gt;}} \n\n\nEmbedding videos in other formats:\n\n\nIn HTML formats the video will be embedded within the document.\nFor other formats, a simple link to the video will be rendered.\n\n\n\n\n\nAudio Files: You can embed audio files using the HTML  tag.\nInteractive Content with HTML Widgets: Quarto supports various HTML widgets that can add interactive content to your document.\n\n\n\n\n\nRelative Paths: Use relative paths for local files to ensure your document remains portable.\n\nIn R, relative paths refer to file or directory paths specified in relation to the current working directory. Relative paths help manage current working files.\nIf you’re struggling with running your files across different paths in your working directories, then the here package can sometimes help you solve those types of issues.\n\nTest Across Browsers: Ensure multimedia content works across different browsers and devices.\nOutput Formats: Verify that multimedia elements render correctly in the intended output formats (HTML, PDF, etc.).\n\nBy following these methods, you can effectively add videos, images, audio, and interactive content to your Quarto documents, enhancing their informativeness and engagement.\n\n\n\n\nRedirecting pages in a Quarto project for R involves creating a _quart.yml configuration file where you can define redirects.\n\nCreate/Edit_quarto.yml : Ensure you have a _quarto.yml file in your Quarto project.\nAdd redirects: In _quarto.yml, define redirects like this: - From: old-page.html to: new-page.html\nRender Project : Run quarto render in your project directory to apply changes.\n\n4.Verify: Test old URLs to ensure they redirect correctly.\nThis sets up URL redirects for pages that have changed or been removed in your Quarto project.\n\n\n\nWhen changing headings in Quarto project, it’s crucial to ensure that all links referencing these headings are still correct. Here’s a quick guide to check for broken links:\n\nChange Headings: Update your headings in the relevant .qmd files.\nRun Quarto’s Built-in Link Checker: Quarto has a built in feauture to check for broken links.\n\nRun the following command in your project directory:\nquarto check\nThis command will scan your project for broken links and report any issues.\n\nManually Update Links: If any broken links are reported, update them to match the new headings. Use the correct anchors, which typically follow the format #new-heading-text.\nRe-render Project: After updating links, re-render your project: quarto render\nDouble-Check: Manually verify a few pages to ensure that the links work as expected.\n\n\n\n\n\nIf you changed a heading from ## Old Heading to ## New Heading, make sure all links like [link text](#old-heading) are updated to [link text](#new-heading). By following these steps, you can efficiently check for and fix broken links when changing headings in your Quarto project.\n\n\nHere’s a streamlined guide for building your Quarto project locally before raising a pull request (from the terminal):\n\nPull Latest Changes: Make sure your local branch is up-to-date with the main branch. git pull origin main\nMake Your Changes: Edit your .qmd files or other relevant files in the project.\nBuild the Project Locally: Render the entire project to catch any errors. quarto render\n\n\n\n\nCheck for Errors and Warnings: Review the console output and fix any issues.\nPreview the Project: Open the generated files in a browser to ensure everything looks correct. quarto preview\nCheck for Broken Links: Use Quarto’s link checker to find and fix broken links. quarto check\nCommit Your Changes: Add and commit your changes to your local repository. git add . git commit -m \"Describe your changes\"\nPush Your Changes: Push your changes to your feature branch. git push origin your-feature-branch\nCreate a Pull Request: Go to your repository platform (e.g., GitHub) and create a pull request from your feature branch to the main branch.\n\nFollowing these steps ensures your changes are correctly implemented and verified locally, minimising potential issues during the review process.\n\n\n\n\nYou can modify the highlight-style element in the quarto.yml file to change the display of code blocks using predefined themes. This will change the appearance of all code snippets across the entire Analysts’ guide. The current theme is set to “printing” but there is a list of other available themes on the Quarto website.\nSome things to be aware of before you make changes:\n\nSome of the themes are adaptive, meaning that if you change the site view from dark mode to light mode then the theme will also change accordingly.\nThe appearance of code snippets will only change if the language (e.g. R, Python) is defined at the start of the snippet (e.g. ``` {r connection_example, eval=FALSE}). Including eval=FALSE stops your code snippet from actually running. If no language is defined, the snippet appearance will need to be modified manually and this can cause some issues.\n\n\n\n\n\nTo add in call out blocks for text, use the built-in Quarto callouts as follows:\n::: {.callout-note}\nText \n:::\nThere are five different types of callouts available: note, warning, important, tip and caution. The name you use will be displayed at the top of the box.\nTo customise the title of the callout box, add a title within the callout code like this:\n::: {.callout-warning}\n## Title of my warning\nText \n:::\nYou can find more information on formatting callout blocks on the Quarto website.\n\n\n\nYou can create images, flowcharts, and diagrams using draw.io. To add them to the Analysts’ guide, we recommend embedding them, by selecting File &gt; Embed &gt; HTML. The image below shows the options that you should select when embedding a diagram\n\n\n\n\n\n\nIf you spot a problem with the site, search if an issue already exists. If a related issue doesn’t exist, you can open a new issue using a relevant issue form.\n\n\n\nScan through our existing issues to find one that interests you. If you find an issue to work on, you are welcome to open a PR with a fix.\n\n\n\n\nKey things to remember when making or proposing changes:\n\nWhere guidance already exists elsewhere it should be linked to rather than duplicated\nWhen adding images, save them in the /images folder\nWhen adding files for download, save them in the /resources folder - Don’t commit rendered .html files for pages\nIf your changes remove or edit a pre-existing anchor link, consider how that will be redirected for users who may have bookmarked it\n\n\n\nClick Edit this page at the bottom of the right hand table of contents of any page to make small changes such as a typo, sentence fix, or a broken link. This takes you to the .Qmd file where you can make your changes and create a pull request for a review.\n\n\n\n\nClone or fork the repository.\nOpen the repository in your editor of choice, e.g. R Studio.\nCreate a working branch and start with your changes!\nCommit and push the changes to your working branch.\n\n\n\n\n\nAll pull requests should be made against the main branch.\nWhen you’re finished with the changes, create a pull request, also known as a PR.\n\nDon’t forget to link PR to issue if you are solving one.\n\nOnce you submit your PR, a repository admin will review your proposal. We may ask questions or request additional information.\n\nWe may ask for changes to be made before a PR can be merged, either using suggested changes or pull request comments. You can apply suggested changes directly through the UI. You can make any other changes and then commit them to your branch.\nAs you update your PR and apply changes, mark each conversation as resolved.\nIf you run into any merge issues, checkout this Git tutorial to help you resolve merge conflicts and other issues."
  },
  {
    "objectID": "CONTRIBUTING.html#support",
    "href": "CONTRIBUTING.html#support",
    "title": "Contributing to the Analyst’s Guide",
    "section": "",
    "text": "If you need any assistance at all, please contact statistics.development@education.gov.uk."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at statistics.development@education.gov.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at statistics.development@education.gov.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "learning-development/r.html",
    "href": "learning-development/r.html",
    "title": "R",
    "section": "",
    "text": "Guidance and tips for using the programming language R",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#what-is-r",
    "href": "learning-development/r.html#what-is-r",
    "title": "R",
    "section": "What is R",
    "text": "What is R\nR is an open-source programming language specifically aimed at statisticians and data analysts.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#what-is-r-for",
    "href": "learning-development/r.html#what-is-r-for",
    "title": "R",
    "section": "What is R for",
    "text": "What is R for\nR can be used for almost anything you can think of, notably data analysis, data visualisation, and creating reports and dashboards. It can also be used to extract data from SQL databases and run SQL queries.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#how-to-install-r",
    "href": "learning-development/r.html#how-to-install-r",
    "title": "R",
    "section": "How to install R",
    "text": "How to install R\nDownload R (language) and RStudio (IDE) from the DfE software center. We also recommend that you download RTools (a helpful R extension) at the same time.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#best-places-to-start",
    "href": "learning-development/r.html#best-places-to-start",
    "title": "R",
    "section": "Best places to start",
    "text": "Best places to start\n\nThe DfE Analytics Academy host an online R training course on SharePoint. This is a great resource full of reproducible examples using DfE data. The course takes you through initially getting R downloaded, all the way through to developing apps in RShiny.\n\nThere is also the DfE R training guide, which is a great starting point and reference to guide you through how to get started using R and RStudio.\nAs an alternative, with a number of options for beginners to R, RStudio Education provide a variety of materials to suit different learning styles.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#best-practice",
    "href": "learning-development/r.html#best-practice",
    "title": "R",
    "section": "Best practice",
    "text": "Best practice\nTips for reaching best practice in R can be found on our RAP for Statistics page, with guidance on meeting best practice in RAP for clean final code. This makes it easier to read and pick up if another person is running your code.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#how-to-work-with-r",
    "href": "learning-development/r.html#how-to-work-with-r",
    "title": "R",
    "section": "How to work with R",
    "text": "How to work with R\n\nR Projects\n\nWhenever you are using R, you should work in an RProject. This just makes sure you are set up in the correct working directory, so your code is pointing at the right folders and files.\nThis guide for using projects in R is a really useful article to help you set up a project.\nYou can check which project you are working in by looking in the top right hand corner of RStudio:\n\n\n\n\nOutlines\n\nIn RStudio you can greatly increase the navigability of your code by taking advantage of outlines. More information on folding and navigating outlines in RStudio can be found online, though when using rmarkdown reports, remember to use names first, such as ## Rows that aren't matching: r nrow(joined %&gt;% filter(matching == FALSE)), rather than having the R code first, so that they are easy to discern in the outline.\n\n\n\nrenv\n\nYou should use the renv package for package and version control in R.\nPackages and versions of R regularly update. Over time, this can cause code to break - e.g. if different dependencies are required for later versions of packages to work. Using renv creates a “snapshot” of your code and packages at the time you created it, which anyone can then recreate when they come to use your code.\nThis is really important for reproducibility, and will help you meet elements of great practice with recyclable code for future use.\n\n\nrenv::restore()\n\nSometimes renv::restore() can fail, and when in specific renv-controlled projects install.packages() will fail saying that packages aren’t available even when they clearly are. There are a couple of workarounds we have found that get around this failure.\n\nConfiguring the proxy settings by running the below in R - this also helps if you are getting timeout issues when trying to webscrape with R:\n\n\nSys.setenv(no_proxy=\"*\") \n\n\nSpecifying the renv library as the install location. It’s a bit of a fudge, though these lines are helpful to get the packages from the renv lockfile installed and you running the project when needed:\n\n\nmyPath &lt;- .libPaths()[1]\n\nforceInstall &lt;- function(pkg, path) {\nmissing &lt;- suppressWarnings(eval(parse(text= paste0(\"!require(\",pkg,\")\"))))\n\nif(missing == FALSE){\nmessage(pkg, \" is already installed.\")\n} else{\ninstall.packages(pkg, lib = path)\n}\n}\n\nforceInstall(\"jsonlite\", myPath)\n\nrenvPackages &lt;- names(jsonlite::fromJSON(\"renv.lock\", flatten = TRUE)$Packages)\n\ninvisible(lapply(renvPackages, forceInstall, path = myPath))\n\nMore manual equivalent to use for specific packages:\n\n.libPaths() # note down output 1, and reuse in the lib argument of install.packages() as below\n\ninstall.packages(\"rmarkdown\", lib = \"C:/Users/swong/OneDrive - Department for Education/Documents/stats-production-guidance/renv/library/R-4.0/x86_64-w64-mingw32\")\n\n\n\n\nUpdating packages in renv\n\nTo update a single package run:\nrenv::update(\"dplyr\")\nTo update all packages run:\nrenv::update()\n\n\n\nInstalling old package versions in renv\n\nThis is surprisingly neat to do. Let’s say you wanted to roll back to version 1.0.2 of dplyr, you would run the following:\nrenv::install(\"dplyr@1.0.2\")",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#quick-reference-lookup",
    "href": "learning-development/r.html#quick-reference-lookup",
    "title": "R",
    "section": "Quick reference lookup",
    "text": "Quick reference lookup\n\nIf you want a useful guide for R syntax or functions, then look no further than the R cheat sheets, these can be an invaluable point of reference. Below we’ve included a few particularly relevant ones:\n\nIntroduction to the RStudio environment\nBase R\ndplyr for data manipulation\nData import with the tidyverse\nstringr for string manipulation\nRegex\nRMarkdown\nRShiny\nggplot2 for data visualisations\npurrr for applying functions",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#other-resources",
    "href": "learning-development/r.html#other-resources",
    "title": "R",
    "section": "Other resources",
    "text": "Other resources\n\nHere is another free introduction to R course by Quantargo.\nR Markdown: The Definitive Guide, hopefully this one should be relatively self-explanatory!\nData science in education provides a heavily detailed guide for beginners in R learning to process data, with some well written out sections that may be of interest.\nHandy guide to collapsing and sectioning R code for easy navigation in RStudio.\nHere are 5 handy tidyverse functions that you should know if you’re using R to process data. Number two is especially useful for those processing wide data into a tidy format!\nMoJ have produced guidance on writing functions in R\nIf you’re wondering how best to make the jump to R from Excel and SQL, take a look at this coffee and coding presention by David Sands.\nMalcolm Barrett has done some slides on dplyr, ggplot2, and using purrr which may be useful if you’re looking at learning more about any of those packages.\nAlso check out the janitor package, it has some particularly powerful functions that are worth a look for tidying and QA’ing data.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#excel-functions-in-r",
    "href": "learning-development/r.html#excel-functions-in-r",
    "title": "R",
    "section": "Excel functions in R",
    "text": "Excel functions in R\nR can do everything you do in excel, but takes out the human error. The reference table below shows how you would carry out popular Excel commands in R.\nR comes in with a built-in dataset called “iris”. We’ll use this for all examples so you can recreate them in your local area.\nREMEMBER: R is case sensitive, so all references to column names/entries need to be as-is in the dataset you are looking at. Functions exist that can translate all your columns to lower or snake case for ease!\n\n\n\n\n\n\n\n\nCommon Excel Task\nExample with iris \nHow to do in R with dplyr\n\n\n\n\nSelect specific columns\nSelect only species and petal length\niris %&gt;% select(Species, Petal.Length)\n\n\nList unique entries in field (column)\nFind the unique entries for the “Species” column in iris\niris %&gt;% select(Species) %&gt;% distinct()\n\n\nFilter/select based on criteria\nFilter for sepal length &gt;4 and sepal width &lt;2.5, but NOT “versicolor” species\niris %&gt;% filter(Sepal.Length &gt; 4 &  Sepal.Width &lt;2.5 & Species != \"versicolor\")\n\n\nFilter for multiple criteria in same column\nFilter for all “setosa” and “versicolor” species\niris %&gt;% filter(Species %in% c(\"setosa\", \"versicolor\")\n\n\nIf else with OR\nCreate new column called “size_group” based on length or width of petal\niris %&gt;% mutate(size_group =if_else( Petal.Length &gt; 4 | Petal.Width &gt;1.5, \"Large\", \"Small\"))\n\n\nMultiple if else\nCreate new column called “flower_price” based on species and petal length\niris %&gt;%  mutate(flower_price = case_when(  Species == \"setosa\" & Petal.Length &gt; 1.5 ~\"top band\",Species == \"versicolor\" & Petal.Length &lt; 4 ~\"low_band\", TRUE ~ \"mid_band\"))\n\n\nCOUNTIF\nCount the number of species if they have a petal length &gt;1.5\niris %&gt;% filter(Petal.Length &gt; 1.5 ) %&gt;%group_by(Species) %&gt;% count()\n\n\nSUMIF\nSum petal width of species if sepal width &lt;3\niris %&gt;% filter(Sepal.Width &lt;3) %&gt;%group_by(Species) %&gt;%summarise(Petal.Width = sum(Petal.Width))\n\n\nVLOOKUP\nLookup to a table called “lookup”\niris %&gt;%  left_join(lookup, by.x=\"Species\", by.y =\"plant_species\")\n\n\nOrder by\nOrder dataset by descending petal width\niris %&gt;% arrange(desc(Petal.Width))\n\n\n\nMore tips for moving from using Excel to using R can be found in the excel-to-R wiki.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#sql-function-equivalents-in-r",
    "href": "learning-development/r.html#sql-function-equivalents-in-r",
    "title": "R",
    "section": "SQL function equivalents in R",
    "text": "SQL function equivalents in R\nR can do a lot of the things that are possible in SQL. The reference table below shows how you would carry out some popular SQL commands in R.\nREMEMBER: R is case sensitive, so all references to column names/entries need to be as-is in the dataset you are looking at. Functions exist that can translate all your columns to lower or snake case for ease!\n\n\n\n\n\n\n\nCommon SQL Task\nHow to do in R (with dplyr)\n\n\n\n\nSELECT * FROM TABLE\ntable %&gt;% select()\n\n\nSELECT ColA, ColB, ColC FROM TABLE\ntable %&gt;% select(ColA, ColB, ColC)\n\n\nSELECT DISTINCT ColA FROM TABLE\ntable %&gt;% select(ColA) %&gt;% distinct()\n\n\nTABLE A LEFT JOIN (TABLE B) ON TABLEA.x = TABLEB.y\ntableA %&gt;% left_join(TableB, by = c(x = y))\n\n\nCASE WHEN x = 1 THEN 1, WHEN x =2 THEN 2, ELSE 0 END AS New_column_name\n%&gt;% mutate (New_column_name = case_when (x == 1 ~ 1, x == 2 ~ 2, TRUE ~ 0))\n\n\nCONCAT(LEA, ESTAB) AS LAESTAB\n%&gt;% mutate(LAESTAB = paste0(LEA, ESTAB))\n\n\nSELECT COUNT(*) FROM TABLE\ntable %&gt;% nrow()\n\n\nSELECT COUNT(ColA) FROM TABLE\ntable %&gt;% count(colA)\n\n\nSELECT Date_column = CONVERT(DATE, Date_column) FROM TABLE\ntable %&gt;% mutate(Date_column = as.Date(Date_column))\n\n\nSELECT Number_column = CONVERT(INT, Number_column ) FROM TABLE\ntable %&gt;% mutate(Number_column = as.numeric(Number_column))\n\n\nSELECT String_column = CONVERT(VARCHAR, String_column ) FROM TABLE\ntable %&gt;% mutate(String_column = as.character(String_column))\n\n\nDROP TableA\nrm(TableA)\n\n\n\nMore tips for moving from using SQL to using R can be found in the SQL-to-R wiki.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#interacting-with-a-sql-database-from-within-r-scripts",
    "href": "learning-development/r.html#interacting-with-a-sql-database-from-within-r-scripts",
    "title": "R",
    "section": "Interacting with a SQL database from within R-scripts",
    "text": "Interacting with a SQL database from within R-scripts\nR can be used to execute SQL scripts to extract data from a database as well as querying the database directly via R. There are two primary ways to do this:\n\nexecuting a SQL script as is within R and\nusing dbplyr to create SQL code for you.\n\nWhich you use will depend on how comfortable with SQL and R and also if you already have existing SQL scripts that you want to execute or you’re writing new database queries.\n\n\nExecuting SQL scripts from within R\n\nThis is primarily useful if you already have existing SQL scripts that you want to build into an R controlled pipeline. For using R to execute a SQL script you’ll need the SQL script/s to be in your R Project and to make a connection via R to the database.\nExample code and a walkthrough guide is available in the Connecting to SQL page in the dfeR package documentation.\n\n\n\nUsing dbplyr to create database queries\n\nIf you’re familiar with dplyr then using the dbplyr package to interact with a SQL database is a good way to go. It allows you to use common dplyr functions such as select(), filter(), mutate(), group_by(), summarise() and arrange() with a SQL database.\nAs with sending a SQL script, you’ll need to create the connection to the database as shown in the dfeR package and then create piped queries as you would with dplyr:\n\n# Library calls ====\n\nlibrary(odbc)\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\n\n# DB connection ====\n\ncon &lt;- DBI::dbConnect(odbc::odbc(),\n                      Driver = \"ODBC Driver 17 for SQL Server\",\n                      Server = \"server_name\",\n                      Database = \"database_name\",\n                      UID = \"\",\n                      PWD = \"\",\n                      Trusted_Connection = \"Yes\"\n)\n\nquery_result &lt;- collect(\n  tbl(con, Id(schema='dbo', table='table_name')) %&gt;%\n  select(example_id_column, example_filter_column, example_indicator_column) %&gt;%\n  filter(example_filter_column=='Filter entry 1', example_indicator_column &lt;= 120)\n)\n\nThe key differences to just using dplyr with a data frame are:\n\nwe use tbl() to point to the SQL table on the SQL database in place of supplying a data-frame\nwe use the collect() function to tell R to collate the commands and run them on the SQL database\n\n\n\n\nCommon issues interacting with a SQL database from within R\n\n\nDBI varchar(max) column ordering bug\nA known bug in DBI is that if a SQL table contains columns of type varchar(max), then any queries including those columns will fail if those columns aren’t the last to be called in the query. This is a bit esoteric, but you can identify this issue if you’re getting the following error message when you run your query:\n\nError: nanodbc/nanodbc.cpp:3166: 07009: [Microsoft][SQL Server Native Client 11.0]Invalid Descriptor Index \nWarning message:\nIn dbClearResult(rs) : Result already cleared\n\nThere are a couple of solutions to this:\n\navoid creating tables containing varchar(max) columns (i.e. always set a field length for your character fields)\nmake sure all varchar(max) columns are placed last in ay select query that you write\n\nThis latter route can be automated using the following function:\n\ntbl_fix_nvarchar &lt;- function(con, schema, table){\n  column.types &lt;- dbGetQuery(\n    con,\n    paste0(\"SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH FROM INFORMATION_SCHEMA.COLUMNS \",\n          \"WHERE TABLE_NAME='\",table,\"'\")\n  )\n  \n  ct &lt;- column.types %&gt;%\n    mutate(cml = case_when(\n      is.na(CHARACTER_MAXIMUM_LENGTH) ~ 10,\n      CHARACTER_MAXIMUM_LENGTH == -1 ~ 100000,\n      TRUE ~ as.double(CHARACTER_MAXIMUM_LENGTH)\n    )\n    ) %&gt;%\n    arrange(cml) %&gt;%\n    pull(COLUMN_NAME)\n\n  tbl(con, Id(schema=schema, table=table)) %&gt;% select(all_of(ct))\n}\n\nThis queries the properties of the SQL table that you’re interacting with, identifies any varchar(max) columns (i.e. where CHARACTER_MAXIMUM_LENGTH equals 1) and then creates a column ordering in your query whereby any varchar(max) columns present are placed to the end.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#tips-for-using-r",
    "href": "learning-development/r.html#tips-for-using-r",
    "title": "R",
    "section": "Tips for using R",
    "text": "Tips for using R\nA selection of handy bits of code and workarounds for common issues. More useful code snippets can also be found in our github repo\n\n\nSpecifying a version of R to use\n\nThis can be done most easily by navigating in RStudio through Tools &gt; Global options &gt; General &gt; Basic &gt; R version (change). It’s likely you’ll need to restart RStudio for the changes to take affect.\n\n\n\nRounding\n\nThe base R function of round() can behave unusually due to the way numbers are stored within R itself. round() is supposed to use a ‘banker’s round’, in which mid-point values (e.g. 0.5, 1.5, 2.5) are rounded to the closest even number. For example, using round(), 1.5 should round to 2, but 2.5 should also round to 2. This is different than the way in which SQL performs rounding, which is always to round numbers at the mid-way point up to the next number (e.g. 2.5 rounds to 3).\nHowever, when investigating numbers in R, you can see that they are not always stored in the way you might think. This is due to floating point numbers. Whilst humans use base 10 (numbers 0-9), computers store numbers in a binary (base 2) system as a combinations of 1s and 0s. When translating between base 2 and base 10, sometimes numbers can be not quite what we expect. For example, asking R to print the number 1.4 to 18 decimal places shows that it is actually stored as:\n\nsprintf(\"%.18f\", 1.4)\n[1] \"1.399999999999999911\"\n\nThese slight differences in the floating point numbers behind the scenes can result in unusual rounding errors using round(). If you’re interested, there’s more information available in these forum posts.\nTo round numbers upwards in a way that is consistent with SQL and other programming languages, we recommend using the roundFiveUp() function in the dfeR R package.\n\n\n\nMaking maps\n\n\nBoundary files\n\nMost maps we’ll want to make will be based on boundaries provided from the Open Geography Portal.\nTake heed of the homepage and details on the codes for map resolution, usually the ultra generalised BUC boundaries are the best option for us as they still provide plenty of detail but are far smaller in size than the other formats.\nSome examples of commonly used boundary files from the Open Geography Portal (based on latest available at time of writing, always make sure the year of the boundaries is the most relevant for your own data!):\n\nParliamentary Constituency - Westminster Parliamentary Constituencies (July 2024) Boundaries UK BUC\nLocal authority - Counties and Unitary Authorities (December 2023) Boundaries UK BUC\nLocal authority district - Local Authority Districts (May 2024) Boundaries UK BUC\nLocal skills improvement areas - Local Skills Improvement Plan Areas (August 2023) Boundaries EN BUC\n\nWhen downloading the boundaries you’ll have a choice of file formats, we’d generally recommend the GeoPackage format as that comes as a single file, though both Shapefile (will get 4/5 files needed to read a single boundary layer) or GeoJSON (sometimes larger file size and needs more processing) are possible options too. There are numerous debates online about these that you can delve into if you so desire.\nIf you want to edit the boundaries, whether that is to edit the coordinate system, file type or to simplify them at all, have a look at the interactive website for mapshaper, there’s also an accompanying rmapshaper R package.\n\n\n\nPackages\n\nKnown package options for building maps are (note this is not an exhaustive list):\n\nbase R’s plot function\ntmap\nleaflet\nmapgl\n\nFor interactive maps in dashboard leaflet is most frequently used in DfE currently.\nThe sf package is usually needed for processing spatial data in R, be aware that the first time you install this it can take a number of minutes.\n\n\n\nExample Shiny app with map\n\n\n# Shiny app ===================================================================\n# Minimal example code for a shiny app with interactive map\n# Can copy and paste into a single app.R file and run to test\n\n# Pre-processing of data (do this in a separate script to the app) ============\n# Load dependencies -----------------------------------------------------------\nlibrary(leaflet) # using leaflet for the interactive map\nlibrary(dplyr) # used for data processing\nlibrary(sf) # used for reading geospatial data\n\n# Pre-process data before app -------------------------------------------------\n# Create an example file using the locations look ups\nexample_data &lt;- read.csv(\"https://raw.githubusercontent.com/dfe-analytical-services/dfe-published-data-qa/main/data/lsips.csv\") %&gt;% \n  mutate(example_count = sample(38)) # number of rows in data frame\n\n# Pull in boundary data from the Open Geography Portal\n# Download GeoPackage format and save in repo from:\n# https://geoportal.statistics.gov.uk/datasets/72bd510b46a7483194237ede9db0a267_0/explore\nboundary_data &lt;- st_read(\"Local_Skills_Improvement_Plan_Areas_August_2023_Boundaries_EN_BUC_3056517900557747412.gpkg\")\n\n# Join the data together and save ready for the app\nmap_data &lt;- boundary_data %&gt;% \n  right_join(example_data, by = join_by(LSIP23CD == lsip_code)) %&gt;%\n  # Convert coordinates to World Geodetic System \n  st_transform(crs = 4326)\n\nsaveRDS(map_data, \"map_data.RDS\")\n\n# R Shiny App =================================================================\n# Global.R file ---------------------------------------------------------------\nlibrary(shiny)\nlibrary(leaflet)\n\n# Load in map data\nmap_data &lt;- readRDS(\"map_data.RDS\")\n\n# ui.R file -------------------------------------------------------------------\nui &lt;- fluidPage(\n  # Show interactive map\n  leafletOutput(\"example_map\")\n)\n\n# server.R file ---------------------------------------------------------------\nserver &lt;- function(input, output) {\n\n    output$example_map &lt;- renderLeaflet({\n      # Set the color scheme and scale\n      pal_fun &lt;- colorNumeric(\n        \"Blues\", \n        domain = c(\n          min(map_data$example_count), \n          max(map_data$example_count)\n          )\n        )\n      \n      # Set a pop up\n      map_popup &lt;- paste(\n        map_data$example_count, \n        \" example count for \", \n        map_data$lsip_name\n        )\n      \n      # Create the map \n      map &lt;- leaflet(\n        map_data, \n        # Take off annoying scrolling, personal preference\n        options = leafletOptions(scrollWheelZoom = FALSE)\n        ) %&gt;%\n        # Set the basemap (this is a good neutral one)\n        addProviderTiles(providers$CartoDB.PositronNoLabels) %&gt;%\n        # Add the shaded regions\n        addPolygons(\n          color = \"black\",\n          weight = 1,\n          fillColor = pal_fun(map_data[[\"example_count\"]]),\n          popup = map_popup\n        ) %&gt;%\n        # Add a legend to the map\n        addLegend(\"topright\",\n                  pal = pal_fun,\n                  values = ~map_data[[\"example_count\"]],\n                  title = \"Example map title\"\n        )\n    })\n}\n\n# Run the application ---------------------------------------------------------\nshinyApp(ui = ui, server = server)\n\n\n\n\nR Shiny peformance\n\nMaps can be intensive and slow to process, and when placed in R Shiny dashboards this can sometimes make users have to wait longer than we’d like.\n\nPre-process as much as you can\n\nFirst, it is best to do all the joining and processing you can before the dashboard, and then save the data as an RDS file to read into the app later. This will minimise the processing R has to do in preparing the file.\n\nSimplify the map detail\n\nIf you’re using a particularly detailed map, you can sometimes simplify the files used to help reduce the size, this will make the map less precise but have a trade off of speeding up the load times. See the boundary files section for more information.\n\nOnly change the layers you need to\n\nIf the map layers on top are reactive (e.g. shaded polygons that change based on a user interaction like a dropdown), consider using the leafletProxy function from the Leaflet package to change only the layer you need to while leaving the base map static.\n\nIf all else fails, try the magical hammer of caching\n\nCaching allows you to save the outputs and then return them without having to run the process again. Caution and thorough testing is advised as this can get complicated and easily lead to bugs, though great rewards lurk beyond the risk. If you’re interested in looking at this route, take a look at the caching section of the Mastering Shiny guide.\n\n\n\n\nRendering tables\n\nThere are a lots of options for rendering tables in Quarto, RMarkdown and R Shiny, many of which are covered in this blog about beautiful tables. Think about the user needs in the first instance and go from there - there’s compelling arguments for each package mentioned.\nThere’s lots of examples to browse through from the RStudio Table Contests for inspiration.\n\n\n\nPassing variables as arguments\n\nThis can be worked around by using a combination of eval() and parse(), as shown in the below function:\n\nshowFilterLevels &lt;- function(data, meta) {\n  filters &lt;- meta %&gt;%\n    filter(col_type == \"Filter\") %&gt;%\n    pull(col_name)\n\n  levelsTable &lt;- function(filter) {\n    return(eval(parse(text = paste0(\"data %&gt;% select(\", filter, \") %&gt;% distinct()\"))))\n  }\n\n  output &lt;- lapply(filters, levelsTable)\n\n  return(output)\n}\n\n\n\n\nReverse additive filters\n\nYou might want to filter your dataset based on multiple negative conditions. Normally to filter on multiple conditions, you would use filter(condition1 & condition2). The “filter” function does not work well with negative conditions (i.e. filtering for cases where condition 1 and condition 2 are not met). Instead, you can use subset(!(condition1 & condition2).\n\n\n\nFile locations\n\nStruggling to get files to talk to one another, or get code to find and use another R script? Use here::here() and marvel at it’s wondrous ability to magic away issues.\n\n\n\nInterweaving vectors\n\nThere’s an easy way to interweave multiple vectors into one single vector using c(rbind()). The example below shows two vectors, but you can have even more if you need.\n\n#Two vectors, x and y\nx &lt;- 1:3\ny &lt;- 4:6\n\n#Run code to interweave\nc(rbind(x, y))\n\n#Output below\n# [1] 1 4 2 5 3 6\n\n\n\n\nMaking charts interactive\n\nWhen pulling ggplot charts into RMarkdown reports, you can consider making them even more user-friendly and interactive with plotly. Further information on how to make your charts interactive with plotly can be found online.\n\n#Simple ggplot chart called \"p\"\np &lt;- ggplot(dat, aes(x=xvar, y=yvar)) +\n    geom_point(shape=1)      # Use hollow circles\n\n#Apply ggplotly() to it to make it interactive\nfig &lt;- ggplotly(p)\n\n\n\n\nReplace all values with another\n\nHave you ever needed to replace every value in your data with another? This can come in handy when you are looking at suppression, e.g. converting all NAs to “z” or all values under a certain threshold to “c”.\n\ndata %&gt;% mutate_all(~ replace(., . == \"Value to replace\", \"Replacement\"))\n\n\n\n\nTemporary groups\n\nThe group_by() function in dplyr is really useful, but can be fiddly if you only want to use it for one operation in a chunk of code. The with_groups() function from dplyr lets you do this, saving you having to group and ungroup data each time.\n\ndata %&gt;% mutate(annual_average = with_groups(time_period, mean))\n\n\n\n\nFinding package dependencies\n\nOften we’ll take chunks of code and reuse them for new projects. This can lead to building up a long list of packages to install, not all of which end up being used in your new code. The NCmisc package is a really handy way to check which packages and functions are used in your code.\nFirstly, load up all the packages the code has library() commands for, then run the following:\n\nlist.functions.in.file('your-filename-here.R', alphabetic = TRUE)\n\n\n\n\nVisualise dependencies\n\nThe depgraph package allows you to plot a graph of all the dependencies in your R project, which can be a useful tool to help you cut down on the number of package dependencies. Briefly, in these graphs you can look for “hot spots” in the network (big bright dots), which represent packages that have many upstream dependencies but are potentially easy to remove because they have few downstream dependencies (that is, only your package depends on them).\n\nplot_dependency_graph(\n  pkg = multibridge_pkg\n  , suggests = FALSE\n  , option = \"cividis\"\n)\n\n\n\n\nReproducible random numbers\n\nThe set.seed() function generates a sequence of random numbers, starting from the value you define in the brackets. This ensures you get the same sequence of random numbers each time you run set.seed() with the same value, which is helpful to test that your results are reproducible.\n\n# random sampling\n&gt; sample(LETTERS, 5)\n[1] \"K\" \"N\" \"R\" \"Z\" \"G\"\n&gt; sample(LETTERS, 5)\n[1] \"L\" \"P\" \"J\" \"E\" \"D\"\n\n# reproducible random sampling\n&gt; set.seed(42); sample(LETTERS, 5)\n[1] \"Q\" \"E\" \"A\" \"J\" \"D\"\n&gt; set.seed(42); sample(LETTERS, 5)\n[1] \"Q\" \"E\" \"A\" \"J\" \"D\"\n\n\n\n\nAutomatic logging\n\nThe tidylog package is a really useful tool for providing automated feedback on dplyr and tidyr operations.\n\nlibrary(tidylog)\n\nfiltered &lt;- filter(mtcars, cyl == 4)\n#&gt; filter: removed 21 rows (66%), 11 rows remaining\nmutated &lt;- mutate(mtcars, new_var = wt ** 2)\n#&gt; mutate: new variable 'new_var' (double) with 29 unique values and 0% NA",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#troubleshooting",
    "href": "learning-development/r.html#troubleshooting",
    "title": "R",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCan’t find make error\n\nThis error will appear when trying to restore old package versions using renv::restore(). It is usually due to Rtools not being properly installed, and therefore your system is struggling to install the older version of the package that hasn’t been provided by CRAN.\nThe solution takes a few minutes, but is relatively straightforward, you can install Rtools as a direct download from the Software Centre. On there, pick the right version of RTools for your version of R and install the application.\nOnce it’s installed close and reopen RStudio and run renv::restore() again - it should hopefully now restore and install your packages successfully!\n\n\n\nPackages not downloading due to “wininet” error (renv 0.17)\n\nPackages are generally downloaded from CRAN using one of two methods: wininet or curl. Since renv version 0.17, analysts potentially hit an error associated with renv defaulting to wininet, whilst R defaults to curl, causing package downloads to fail. To fix this issue, analysts should change their default download method with renv to be curl. This can be done across all R-projects on your machine, by setting the default in your global R environment file (.Renviron). If you don’t know where to find your .Renviron file, then you can run the command usethis::edit_r_environ() (you may have to install the usethis package first) to open it in R-Studio. Once you’ve got it open, then add the following line to the .Renviron file:\nRENV_DOWNLOAD_METHOD = \"curl\"\nThen restart R-Studio and downloads with renv should now succeed.\n\n\n\nSSL certificate errors\n\nThe DfE firewall currently blocks access to external sites from DfE laptops when connecting from common analytical software such as R or Python. This can prevent analysts from accessing important external resources such as package repositories and data API endpoints. If you get an error including text along the lines of “self signed SSL certificate not allowed” or anything similar involving SSL certificates (or certs.), then contact the Statistics Development Team with a copy of the command that you’re running and the error that it’s producing. We can then work with the Network Operations team to prevent this being a blocker to your work.\n\n\n\nshinytest2 - Invalid path to Chrome (chromote) error message\n\nSometimes when attempting to run tests using shinytest2, you will get the following error:\nError in initialize(...) : Invalid path to Chrome\nShinytest2 relies on Google Chrome to run, and so it needs to know where it has been installed on your computer. Chrome is not a standard piece of software for DfE laptops, so there are a few steps you can follow to resolve this issue.\nBefore you try any of the steps below, please check that you have Google Chrome installed. You can download Chrome from the Google Chrome website. You may get an error message saying that you do not have adminstrator permissions when you run the install file. Click “No”, and you should be presented with another window that says “Google Chrome can be installed without administrator privileges. Continue?”. Click “Yes” on this screen and Chrome should install.\nIf you have Chrome installed and continue to get the chromote error when using shinytest2, then you’ll need to set an environment variable to tell R where to find Chrome.\nThe first step to resolve this is to set the path to chrome.exe (the Chrome application file itself where you’ve installed it) via the console for the current session. You can find the file path for your local Chrome installation by following these steps:\n\nClick the Start button\nRight click on the Chrome icon and click ‘Open File Location’\nWhen the folder opens, right click on Google Chrome inside the folder and click ‘Properties’\nCopy the file path inside the Target box (see image below)\n\n\nSet the CHROMOTE_CHROME environment variable to the path to the chrome.exe file on your computer. If you have followed the instructions above to copy the file path then you will need to make sure that you change the \\ to / before you run the code otherwise it will not work. As an example it should look something like this:\nSys.setenv(CHROMOTE_CHROME = \"C:/Users/jbloggs/AppData/Local/Google/Chrome/Application/chrome.exe\")\nTo set the environment variable, you’ll need to edit your .Renviron file. This will fix the error across all R projects. If you don’t know where to find the .Renviron file, you can use the usethis package to open it in R-Studio using usethis::edit_r_environ(). Then set the CHROMOTE_CHROME environment variable to the path to chrome.exe on your computer. As an example it should look something like this:\nCHROMOTE_CHROME=\"C:/Users/jbloggs/AppData/Local/Google/Chrome/Application/chrome.exe\"\nNote: remember to give the path using forward-slashes (/) rather than back-slashes (\\).\nOnce you’ve added that and saved the .Renviron file, restart your R session and that should then be loaded and shinytest2 will run. You can double check that it’s set the environment variable properly by running the following command:\nSys.getenv(\"CHROMOTE_CHROME\")\nThat command should return the path that you entered for Chrome.\n\n\n\nshinytest2 - Old headless mode error message\n\nWhen shinytest2 runs a Chrome or Edge browser window, it runs it in something called “headless mode”, which effectively means it’s running the browser in the background without you seeing the window on your desktop. The way headless mode works in Chrome and Edge recently changed, which triggers an error in shinytest2 if you haven’t updated your R settings to allow for this.\nThe resulting error should look something like the following.\nError in with_random_port(launch_chrome_impl, path = path, args = args) :\nCannot find an available port. Please try again.\nCaused by error in startup():\n! Failed to start chrome. Error:\nOld Headless mode has been removed from the Chrome binary. Please use the new \nHeadless mode (https://permanently-removed.invalid/docs/chromium/new-headless) \nor the chrome-headless-shell which is a standalone implementation of the old \nHeadless mode (https://permanently-removed.invalid/blog/chrome-headless-shell).\nTo fix this issue, you just need to let R know that you want chromote sessions to run with the “new” headless mode. This can be done by setting a system variable in your .Renviron file as follows:\n\nopen your .Renviron file for editing (for example, in the R console run usethis::edit_r_environ())\nadd the line CHROMOTE_headless=\"new\" to the .Renviron file (and make sure the file ends in an empty line)\nSave the .Renviron file\nRestart your R session\n\nNow you should be able to run shinytest2::test_app() as normal.",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/r.html#using-r-with-ada-databricks",
    "href": "learning-development/r.html#using-r-with-ada-databricks",
    "title": "R",
    "section": "Using R with ADA / Databricks",
    "text": "Using R with ADA / Databricks\nSee our guidance on how to connect to Databricks SQL Warehouse from R Studio, and how to connect to Databricks personal cluster from R studio\nYou can also view example R code using the Databricks code template notebooks",
    "crumbs": [
      "Learning resources",
      "R"
    ]
  },
  {
    "objectID": "learning-development/sql.html",
    "href": "learning-development/sql.html",
    "title": "SQL",
    "section": "",
    "text": "Guidance and tips for accessing data via databases with SQL",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#what-is-sql",
    "href": "learning-development/sql.html#what-is-sql",
    "title": "SQL",
    "section": "What is SQL",
    "text": "What is SQL\nSQL or Structured Query Language, is a programming language used to talk to relational database management systems.",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#what-is-sql-for",
    "href": "learning-development/sql.html#what-is-sql-for",
    "title": "SQL",
    "section": "What is SQL for",
    "text": "What is SQL for\nSQL servers are where most of DfE’s data is held, making it ideal for database management.\nSQL provides us with a language primarily for querying databases to extract data, though it is also capable of some basic data processing and analysis.",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#how-to-install-sql",
    "href": "learning-development/sql.html#how-to-install-sql",
    "title": "SQL",
    "section": "How to install SQL",
    "text": "How to install SQL\nDownload SSMS from the DfE software center, talk to your team about getting access to the appropriate SQL servers and databases where the data you need to access is held and start writing SQL queries.\nThere are usually a couple of different versions available for software on the software center, we’d recommend you always go for the latest (newest) version possible.\nIf you can’t find the option in the software centre, then you may need to raise a service desk request to make SQL server management studio visible in your software centre.",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#best-place-to-start",
    "href": "learning-development/sql.html#best-place-to-start",
    "title": "SQL",
    "section": "Best place to start",
    "text": "Best place to start\nAndy Brook’s excellent Introduction to SQL session, giving a visual overview of the basics of querying with SQL:",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#best-practice",
    "href": "learning-development/sql.html#best-practice",
    "title": "SQL",
    "section": "Best practice",
    "text": "Best practice\nHere are some tips to follow best practice in your SQL code, making it easier to read and pick up if another person is running your code. Following best practice guidance will help you to achieve RAP best practice with clean final code.\n\nAvoid any trailing whitespace\nAlways capitalize SQL keywords (e.g., SELECT or AS)\nVariable names should be in snake case - lower case words separated by underscores (e.g. pupil_age instead of PupilAge)\nComments should go near the top of your query, or at least near the closest SELECT\nTry to only comment on things that aren’t obvious about the query (e.g. why hardcoded filters are used, how to update them)\nWhere possible, use Common Table Expressions (CTEs) early and often, and name them descriptively (e.g. “pupil_age_table” rather than “p”)\n\nGitLab have produced a full SQL style guide, which we recommend following where possible.",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#how-to-work-with-sql",
    "href": "learning-development/sql.html#how-to-work-with-sql",
    "title": "SQL",
    "section": "How to work with SQL",
    "text": "How to work with SQL\nSSMS is the best tool to get started with writing SQL queries and saving SQL scripts that produce your desired outputs.\nOnce you have saved SQL scripts or are more familiar with writing SQL queries on the fly, you can look at running your scripts or lines of SQL code directly in R. This will streamline your process, saving copying and pasting SQL outputs into csvs, and ultimately help with reaching RAP best practice by aiding production of a single publication production script",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#quick-reference-lookup",
    "href": "learning-development/sql.html#quick-reference-lookup",
    "title": "SQL",
    "section": "Quick reference lookup",
    "text": "Quick reference lookup\n\nw3schools.com offers a useful guide through the most common SQL commands.",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#other-resources",
    "href": "learning-development/sql.html#other-resources",
    "title": "SQL",
    "section": "Other resources",
    "text": "Other resources\n\nThis tutorial script by Tom Franklin is a particularly good starting point as it includes the data you are manipulating, so you don’t need to worry about connecting to or getting access to specific databases before you can then run anything. Simply open up Microsoft SQL Server Management Studio and start playing with that query.\nAvision Ho created the this SQL training course.\nThe Khan academy offers a great free introduction to the basics of SQL.\nIt’s also worth taking a look at Jon Holman’s presentation on ‘good to know’ SQL functions.\nMoJ have produced a SQL from square one guide to using CTE’s in SQL as well as running SQL from RStudio\n\nAndy’s follow up intermediate SQL session, covering more advanced features of SQL:",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/sql.html#tips-for-using-sql",
    "href": "learning-development/sql.html#tips-for-using-sql",
    "title": "SQL",
    "section": "Tips for using SQL",
    "text": "Tips for using SQL\n\nSetting up a SQL area\n\nBefore you set up a SQL database, make sure you have the following information to pass on:\n\nThe name of the database you want to set up - Different servers will have different naming conventions, make sure to check this with the server owner before you confirm the name.\nWho the database owners should be - This will most likely be yourself, but you can have multiple (e.g. your team leader). It can be helpful to have more than one owner, so one can grant permissions when the other is unavailable.\nWho should have access, and what their access levels should be - Users can have read or read/write access. Make sure you have a list of users (with their AD names) and their access levels ready.\nThe database structure - Do you need certain schemas setting up? This will help organise your database. Without schemas, all tables will be saved under [dbo].\n\nThere are a few common servers that statistics producers (and analysts in general) make use of at DfE. Use the following contacts below to pass on the above information to get your new database set up:\n\nPDR (T1PRMDRSQL,55842) - contact the PDR team\nPDB16 (3DCPRI-PDB16) - raise a request through the service desk under “non-standard” &gt; “any other request”\nAnalyse & Modelling server (T1PRANMSQL,60125) - raise a request on the service desk under the following options:\n\n\n\n\n\nManaging access\n\nTo gain access to a SQL database, you must have written confirmation from the database owner specifying whether your access is read-only or both read and write.\nIf the area you require access to is in the T1PRMDRSQL,55842 SQL server, contact the PDR team with your permission attached, stating the name of the database you want access to.\nIf the area is in any other server, raise a request through the central IT service portal under “non-standard” &gt; “any other request”. In your request make sure you attach the written confirmation and specify:\n\nThe server name\nThe database name\nWhether it’s read or write access you need\n\n\n\n\n\nMoving data to different areas\n\nInformation on how to do this in R can be found in our RAP for Statistics page",
    "crumbs": [
      "Learning resources",
      "SQL"
    ]
  },
  {
    "objectID": "learning-development/git.html",
    "href": "learning-development/git.html",
    "title": "Git",
    "section": "",
    "text": "Guidance and tips for version control with Git",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#what-is-git",
    "href": "learning-development/git.html#what-is-git",
    "title": "Git",
    "section": "What is Git",
    "text": "What is Git\nIt is a version control software. It is by far the best of its kind and is widely used by software developers and data scientists.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#what-is-git-for",
    "href": "learning-development/git.html#what-is-git-for",
    "title": "Git",
    "section": "What is Git for",
    "text": "What is Git for\nGit is a version control software that tracks changes to files within a folder that you assign Git to track. It works best with plain text files such as flat data files, code scripts and markdown documents. These folders are known as repositories and can be held and managed securely in a central online place such as GitHub (best for public), GitLab (can be good for either public or private) and Azure DevOps (best for private). We can easily mirror our Azure DevOps repositories in the DfE Analytical Services area on GitHub.\nIt is widely used across DfE and integrates neatly with our use of Azure DevOps, as well as being the current leading version control software in the world of coding with over 87% of 74,298 stack overflow users using it.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#how-to-install-git",
    "href": "learning-development/git.html#how-to-install-git",
    "title": "Git",
    "section": "How to install Git",
    "text": "How to install Git\nDownload it from the Git website.\nGit doesn’t have an IDE, instead it will either integrate with your current IDE such as RStudio or Visual Studio Code, or run in the command line.\nWhen you first try to use Git you may be prompted for a GitHub username and password, if this happens you should generate a Personal Access Token (PAT) and use this as your password.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#best-places-to-start",
    "href": "learning-development/git.html#best-places-to-start",
    "title": "Git",
    "section": "Best places to start",
    "text": "Best places to start\n\nContact us about attending one of our in-person Git workshops\nIf you’re new to Git and are unsure of what it does, then take a look through these Git for humans slides\nDavid Sands’ guide to getting started with Git is a helpful place to start. When watching video 03, do NOT run the code to change your proxy settings! Ignore this part of the video.\nGooey Git by David Sands, provides a very neat overview of using git with R.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#how-to-work-with-git",
    "href": "learning-development/git.html#how-to-work-with-git",
    "title": "Git",
    "section": "How to work with git",
    "text": "How to work with git\n\nGit Bash\n\nGit Bash allows you to run git commands without opening another IDE. You’d often need to use Git Bash to set your user settings, amend your proxy settings and clone repositories.\n\n\n\nGit with RStudio\n\nGit with R studio is a neat user interface for git. You don’t need to use any git bash commands, and everything is done using point and click. This is useful for day-to-day version control, but does not support the full functionality of git.\nHowever, you can still run the full suite of git commands by simply typing them in the “Terminal” of RStudio.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#quick-reference-lookup",
    "href": "learning-development/git.html#quick-reference-lookup",
    "title": "Git",
    "section": "Quick reference lookup",
    "text": "Quick reference lookup\n\nGitHub have created a cheat sheet for git commands.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#other-resources",
    "href": "learning-development/git.html#other-resources",
    "title": "Git",
    "section": "Other resources",
    "text": "Other resources\n\nAvison Ho and Linda Bennett gave this coffee and coding presentation on version controlling SQL with Git.\nHappy Git is a useful (though detailed) guide to setting up and using git.\nAdam Robinson and Zach Waller have produced guidance for how to use git in Azure DevOps (formally VSTS), which gives a detailed guide on how to use version control software in DfE analysis.\nWhile also mentioned above as a resource for learning R, chapter 6 of ESFA’s guide to R and Git is also worth looking at for Git alone.\nMicrosoft have produced documentation on using Git within AzureDevOps.\nFor those wanting to go deeper into understand the variety of git commands and what they do, there is a great online visual resource.\nWe also have a number of helpful sections on using git in practice at the end of our RAP for Statistics page.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "learning-development/git.html#tips-for-using-git",
    "href": "learning-development/git.html#tips-for-using-git",
    "title": "Git",
    "section": "Tips for using Git",
    "text": "Tips for using Git\n\nCommits\n\nThe best advice is to just commit regularly, like you would if you were saving a word document.\nEach commit is a saved point in time that you can easily roll back to if needed. If you want to know more about how to do this, see the reverting a commit section below.\n\n\nCommit messages\n\nThe best advice for commit messages is to keep them clear, simple and brief.\nIt is recommended to use the imperative tone e.g. Add fix for joining pupil table rather than Added ... or Adding .... Other general recommendations include:\n\nKeep it short (max 150 characters if possible) - committing regularly can help with this\nExplain what the change is and why the change was needed. You generally don’t need to go into the detail of how the change was made as this should be visible from the code\n\ne.g., Add x to prevent y or Add x so that y can z\n\nDon’t assume the reviewer understands what the original problem was\n\nwould someone else looking at it, or even yourself in a week’s time know what the commit was doing?\n\nCapitalise the first letter and don’t end with punctuation, it is unnecessary\n\nIf you’re struggling to know what to write, remember the reader (including future you) is unlikely to have much context when looking through the version log to find the right point in time, try considering the following:\n\nWhy have I made these changes?\nWhat effect have my changes made?\nWhy was the change needed?\nWhat are the changes in reference to?\n\nNever leave a commit message blank. The information you include could really pay off later when you need to remember what you worked on, or share it with other people.\nThere are specific conventions outlined in conventional commits, that are worth familiarising yourself as you begin to use Git more. Following these conventions will help you write messages that are consistent with over 100 million other Git users around the world.\n\n\n\nIn RStudio\n\nOnce you are happy with changes and want them to be in the latest version of your branch for all of your team to see, you can push “commits” up.\n\nWhen you make a change to a file, this will pop up in the “Git” window of your R console. Select the files you want to commit by ticking the “staged” box next to them.\n\n\n\nThis will bring up a new window. Add a comment describing your additions/changes, and click commit. You will see all the staged files disappear. Then click “Push” to push the committed files up to the online repository for all to use.\n\n\n\nWhen another member of your team makes a commit and you want to pull this into your local area to check and work off the latest version, click on the blue “pull” button.\n\n\n\n\n\n\nBranches\n\nDavid Sands has produced a very helpful video on how to use branches in git, which also covers how to tackle merge conflicts if and when they arise:\n\n\n\n\nWhen you create a git project, it will automatically create a “main” (sometimes “master”) branch for you. This is where code that has been QA’d and you are happy with should sit.\n\nIt is good practice to have at least one other branch, we tend to call it “development”. This is the branch where you will be doing most of your work. To open a new branch, navigate to “branches” and click on the blue box highlighted below\n\n\n\nHaving two separate branches means that if anything goes wrong in the “development” branch, the “main” or “master” branch is still unaffected and runs without issue. This lets you test and QA the code more thoroughly before merging into your main branch.\nWhen working on your project, make sure that you are in the right branch. You can check this by navigating to the “Git” tab in RStudio as demonstrated below.\n\n\n\n\n\nPull requests\n\nWhen you have got to a place with the code and your committed changes where you are happy for it to be QA’d, you can open a pull request. This gives your team a chance to QA your changes before merging the branches together.\n\nNavigate to “Pull Requests” in the “Repos” tab of Azure DevOps and click the blue “New pull request” button.\nThis will take you to a new window. Here, you can add:\n\nTitle, tell your team what has changed\nDescription, tell your reviewer what they should check\nReviewers, add multiple if needed.\n\nAs a reviewer, to approve a pull request, follow the link in your email and click “approve” in the blue box. When all reviewers are happy for this to be the new master branch, click “complete”.\n\nCreating pull requests in GitHub follows a broadly similar process and should be intuitive from the above steps for Azure DevOps.\n\n\n\nMerging a branch into another one\n\nIf you want to merge a branch into another without doing a full pull request, you can do this using a terminal.\nThis may happen if you are working on a feature branch, and want to merge the latest changes to the master branch back into your feature branch before opening a PR to master with your own changes. Often this can let you deal with nasty conflicts up front, or allow you to keep working on your feature but update to have something new that’s on master.\nStart off by checking out your desired target branch - git checkout mybranch. Then merge in the branch you want (e.g. master) - git merge master.\n\n\n\nCherry picking\n\nIf you want to cherry pick specific commits for PR, you can do this by cherry picking the commits you want to use, and creating a new branch that has only those new commits that you want.\nTo start off you’ll need to identify the commits you want. In the terminal, run git log --oneline to get a log of commits for your current branch, use git log --online BRANCHNAME to specific the branch for the log. This gives a list of commit hashes and messages (stackoverflow response defining git hashes and commmit ID’s). You can press enter to get more commits, or q to quit\nThen go to the branch you want the commit to appear on and cherry pick your commits. Often, if this is to hop around something on a development to master pr, you would create a second development branch, cherry pick commits to there, and then PR that to master and delete the branch after merging.\nOn the branch you want to PR (i.e. your copy of a development branch purely for merging only cherry picked commits) run git cherry-pick COMMITHASH to add the specified commit, or git cherry-pick HASH1^..HASH4 for a specified list of commits (inclusive).\nHappy days!\n\n\n\nVisualising your tree\n\nYou can use gitk --all to visualise a tree of all previous commits up to this point.\n\n\n\n\nGetting commit IDs\n\nCommit IDs are the way Git identifies unique commits. They’re really helpful if you ever need to revert back to a previous commit if you’ve made a mistake.\nThere are lots of different ways to find out commit IDs:\n\nVisit the repo in Azure Devops and go to the commit of interest. At the top of the page there is a commit ID you can copy.\nNavigate to the repo in your file explorer, then open up Git Bash and type\n\ngit log --pretty=format:\"%h - %an, %ar : %\nOR\ngit log --pretty=oneline\nThere are a number of customisable versions of this, more information is available on the Git website.\n\n\n\nReverting a commit\n\nMade a mistake and need to revert? No problem! Reverting commits in git creates a new commit reversing your accidental commits, bringing you back to an earlier point in your branch.\nFor rolling back on a branch, you should revert any changes so that you’re not erasing history others might have pulled/cloned. E.g. If you want to revert the last 3 commits on master:\ngit revert --no-commit master~3..master\nThe no commit argument means that it just makes the reverts locally, and you can then commit them all as a single revert commit. If you don’t use --no-commit then it will start doing individual revert commits for you for each commit you’re reverting.\nYou can also revert back to a particular commit ID:\n\nNavigate to the repo in your file explorer, then open up Git Bash and type :\n\ngit revert [PASTE COMMIT ID HERE]\n\nThis opens up a window that asks you to write a commit message. You can skip this step as it automatically writes a revert message for you. Enter :wq which quits the writer window.\nYou can now push these changes to Azure Devops by entering git push into the git window\n\n\n\n\nTagging release versions\n\n\nIt can be useful to tag specific commits or releases at key points in time. For us a common example will be each publication cycle, to tag the version of the code used to process data for a particular release or amendment.\nGuidance on how to tag releases using git can be found on the draft version of the BPI code guidance\n\n\n\nFix: cannot resolve proxy\n\nIf you get this error when trying to pull / push to a repository from a DfE laptop:\nfatal: unable to access 'https://dfe-gov-uk.visualstudio.com/stats-development/_git/ees-analytics-new/': Could not resolve proxy: mwg.proxy.ad.hq.dept\nThen try running the following git command to clear the proxy settings in your git config file:\ngit config --global --unset http.proxy\n\n\n\nCleaning up local branches\n\nEssentially this finds all merged (old) branches, makes sure to not include master or development (can rename or add more if appropriate) and then runs those branches through the delete command. To be safe, run the first two thirds first, to print out the list of what you’ll be deleting:\ngit branch --merged | egrep -v \"(^\\*|master|development)\"\nThen if you’re happy, run the whole line:\ngit branch --merged | egrep -v \"(^\\*|master|development)\" | xargs git branch -d\nThat should be all local branches tidied up. Now to complete the job you can prune the tracking branches you have set up, usually this will just be:\ngit remote prune origin\nThough you can use git remote -v to find other remotes if you have them.\n\n\n\nCreating PATs\n\nPATs or Personal access tokens, are a preferred way to authenticate into repositories through code. Creating a PAT in GitHub is relatively intuitive.\nIn Azure DevOps, you can do this by following Microsoft’s documentation on creating a PAT in Azure DevOps.\n\n\n\nStoring secure variables\n\n\nAzure DevOps\n\nIn Azure DevOps you can securely store variables that are then used by your pipelines by making use of variable groups.\nFirst create a variable group by navigating to Pipelines &gt; Library. Enter any variables you want to store here and make sure to change the variable type to secret if appropriate (i.e. login credentials or PATs).\n\nThen, in the pipeline you want to use the variables in, go to Variables &gt; Variable groups and link the variable group as shown below. You can then call upon the variables as needed in your pipeline.\n\n\n\n\nGitHub\n\nIn GitHub you can store sensitive variables as encrypted secrets.\n\n\n\n\nMirroring a repository\n\nThere may be times when you have a repository in one place, such as Azure DevOps, but want to mirror the code and any changes to it in another place, such as GitHub. This can be for many different reasons, though commonly for us it will be to open up our code across government. Azure DevOps doesn’t provide us with easily publicly visible code repositories, however GitHub does.\nTo mirror a code repository from Azure DevOps, use the mirror git repository extension (already installed on dfe-gov-uk instance), make use of the PAT and secure variables sections above and then add a job to your pipeline and enter your parameters in the fields as per the example below:\n\n\n\n\nFixing the “JSON token 4” error\n\nThis error can appear when you are trying to push changes to Azure DevOps.\nIf it appears and you have not changed your password recently, try locking your laptop and seeing if on re-login it prompts you for a password change.\nIf you have changed changed your password recently, try signing out of Azure and then back in again.",
    "crumbs": [
      "Learning resources",
      "Git"
    ]
  },
  {
    "objectID": "RAP/rap-faq.html",
    "href": "RAP/rap-faq.html",
    "title": "RAP FAQs",
    "section": "",
    "text": "This guidance is to help answer questions about RAP and act as a tool for anyone managing those implementing RAP.",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP FAQs"
    ]
  },
  {
    "objectID": "RAP/rap-faq.html#frequently-asked-questions",
    "href": "RAP/rap-faq.html#frequently-asked-questions",
    "title": "RAP FAQs",
    "section": "Frequently asked questions",
    "text": "Frequently asked questions\nIf you have any questions you’d like answering or adding, let us know by emailing statistics.development@education.gov.uk.\n\n\nWhat is RAP\n\nGeneral questions about Reproducible Analytical Pipelines and why analysts need to care about it.\n\n\nWhat is RAP, is it just using R?\n\nNo. Reproducible Analytical Pipelines (RAPs) are automated statistical and analytical processes. They incorporate elements of software engineering best practice to ensure that the pipelines are reproducible, auditable, efficient, and high quality.\nDoing RAP is doing analysis using the best methods available to us, which is an expectation of the statistics code of practice.\nThe tools we recommend for statistics production RAP are SQL, Git and R. Other suitable alternatives that allow you to meet the core principles can also be used, but you should check this with the Statistics Development Team first. Ideally any tools used would be open source. Python is a good example of a tool that would also be well suited, though is less widely used in DfE and has a steeper learning curve than R.\nUsing R for parts of the process does get you close to a lot of the RAP principles with little effort, which is why we recommend it as one of the tools you should use.\nMore details and learning resources for the recommended tools can be found in our appropriate tools guidance.\n\n\n\nIs this specific to DfE?\n\nNo – RAP is a cross government strategy, and all departments are expected to use this way of working. See the analytical function page on RAP to see other examples of departments utilising RAP and seeing the benefits.\nHere is a great blog post from NHS digital on ‘Why we’re getting our data teams to RAP’.\nRAP is also a strategic objective of Analysis Function strategy for 22-25:\n\ndelivering the Reproducible Analytical Pipelines (RAP) Strategy and action plan to embed RAP across government, ensuring our processes are automated, efficient, and high quality which frees up resource to add value and insight in our analysis.\n\n\n\n\nI’m overwhelmed by all the steps, is RAP really necessary?\n\nThe levels within RAP good practice are in line with what a lot of teams are doing already, take time to understand what the different steps mean and don’t panic about the quantity of them as they’ve intentionally been broken down into the smallest level that was sensible. Ideally your work should meet as many of the RAP criteria as possible, but even the use of some RAP principles when you’re just starting out are better than none.\nStatistics publications are some of the most important pieces of statistics work that the department does. It’s important we get that data right; RAP can help us automate the production and QA of outputs in a way that gives us the most confidence in the statistics and causes the least burden for you.\n\n\n\nWill implementing RAP lead to a disconnect with the data and ‘black box’ processes?\n\nNot at all. When fully implemented and accompanied with the relevant skills, RAP has the opposite effect as each stage of the process is clearly written as code.\nWe recognise there is a sizeable skill gap, and that until this is addressed there can be a risk of feeling like code is a black box. This isn’t a reason to avoid RAP though, instead it’s a big reason why we need to push ahead with prioritising upskilling ourselves to implement it. An element of RAP is having documentation for your analytical processes. Having good documentation will help to support your team as they are learning new skills, particularly when you have new starters who are taking on pieces of work for the first time.\n\n\n\nTeams vary, and what my team does is different to others, if I’m happy with my approach, can I ignore some of the RAP steps?\n\nNo, you should not ignore any of the steps. If you think any of the steps aren’t applicable to you, talk to the Statistics Development Team so we can understand more about the processes in your area. It may well be that you’re meeting it without realising, there’s a misunderstanding, or there’s something more we can add to the guidance.\nIf you have unique or nuanced processes, RAP helps you document these and make your area more resilient to turnover of people. RAP is all about automating the process and documenting it in a standardised way to make it easily reproducible.\n\n\n\n\nGetting started\n\nQuestions about how to get started with RAP.\n\n\nCan I leave the R stuff to others in my team whilst I focus elsewhere?\n\nNo. Anyone undertaking any analytical processes should now be using RAP processes, and coding is a critical analytical skill. RAP is a cross-government strategy and there is an expectation in all departments for all analysts to move to this way of working.\nWe all must ensure that analysis is reproducible, transparent, and robust using coding and code management best practices (Analysis Function competency framework).\n\n\n\nImplementing RAP takes time to setup initially, how can we prioritise it?\n\nThere is a clear expectation that this is the direction for analysis in government, ultimately, it’s a part of the new digital skills required for the role of analysts. If we’re not prioritising improving our analysis in line with RAP principles, we’re not approaching our work correctly.\nWe have support from senior leadership, and this is a department priority, so you should be building in time for it. If you are having difficulties prioritising RAP after talking with your line management chain, please get in touch with the Statistics Development Team.\nIn the long term, implementing RAP will significantly reduce the time it takes to run your pipeline, and so while it requires time upfront, it creates more time in the future to prioritise other things. It also improves the quality and reproducibility of our work, giving numerous business benefits.\n\n\n\n\nTools for RAP\nQuestions about what tools and software to use when applying RAP principles.\n\n\nDo I need to rewrite existing SQL code into R?\n\nNo. In fact, we recommend you don’t necessarily do this!\nSQL is a useful tool in its own right and some processes are better using SQL code executed on the databases.\nWhat we recommend, is that all of your code is within a single R project, and the SQL scripts are executed via R, this helps with documenting the process, but keeps the SQL code and benefits of doing heavy processes on the database side. See the final question in this ‘Tools for RAP’ section for more information.\n\n\n\nI’m sometimes limited by the access and tools I have (ESFA servers, EDAP, Databricks), is there anything that can be done about this?\n\nThe first step is to let us (the Statistics Development Team) know so we can understand the wider landscape and escalate. There isn’t really a quick fix, but the first step is raising awareness.\n\n\n\nWhat happens if we can’t reproduce our current processes using R?\n\nThis is highly unlikely and is more likely to be from a lack of knowledge of what is possible in R – if you’re struggling to reproduce processes, please contact the Statistics Development Team so they’re aware of your processes and can help you implement RAP principles.\n\n\n\nR isn’t always the quickest tool for processing data, can we use SQL instead?\n\nYes. We recommend different tools for different purposes, SQL should be used for querying source data and large data processes, R should be used for more complicated data manipulation, QA, and analysis. There’s a grey area in the middle where you can do some things in either tool, sometimes you’ll need to test for yourself which way is faster (e.g., linking datasets or transforming tables).\nSQL scripts should be run in an automated way using R, instead of manually running them and copying the outputs. The difference we’re talking about here is whether you process data on the database server or on the machine where you’re running R. A simplistic rule of thumb is do large scale processing on the database side (e.g., filtering and aggregating), and then only bring the data you need for manipulation / complicated processing into R.\n\n\n\n\nImplementing RAP\n\nQuestions on how to implement RAP.\n\n\nHaving a single script for code doesn’t seem the best way to do it, why are you suggesting this?\n\nThis is a misconception of our guidance that we will be clarifying and improving the phrasing of. The two steps this refers to are:\n\nCan the publication be reproduced using a single code script?\nAre data files produced via single code script(s) with integrated QA?\n\nWe’re not suggesting all code lives in a single script. These steps in the RAP guidance are to encourage the repository being structured so that one click of a button can run the entire process from the source data through to the final output in order.\nThis means that you should have one ‘centralized’ script that details all of the different scripts in your repository. This single run script then provides a single reference point for where all of your code and scripts live, and in what order they’re executed, which is a fantastic piece of documentation to have!\n\n\n\nHow do I dual run for QA if our process is in code?\n\nYou don’t need to be dual running if your process is automated, it’s not the best way to QA. See our guidance on QA for more details on how you can approach it or talk to us if you’re unsure.\n\n\n\nShould I have separate repositories or branches in Git for each release?\n\nMore information on recommended ways of working with Git can be found in our guidance on repositories.\nIn short, a single repository should be used for all releases of your publication, there’s no need to have multiple as all the history is saved within previous commits.\n\n\n\nWe should have plain text documentation to accompany the process, code and comments don’t feel like enough?\n\nYes, you should have some plain text documentation, this is a part of the guidance on RAP and a part of the ‘documentation’ step to ‘good’ practice.\nThe README in your repository is the place for traditional ‘desk notes’ and text explanations of bits of the process and context required, we have guidance on writing READMEs on the statistics production website.\n\n\n\n\nLearning about RAP\nQuestions learning more about RAP and developing skills.\n\n\nCan I look at what other people are doing?\n\nAbsolutely. Currently the best way to do this is to ask other teams to share code directly with you. If we can all make more progress with using Git, then this will make it much easier to share repositories and have code open by default for interested analysts to browse.\nWe run a regular RAP Knowledge Share series in which we share good practice and examples of work. We record each session so that you can refer back to them at a later date if you want to. Previous recordings can be found in our RAP knowledge share Sharepoint folder. Upcoming sessions are advertised on the DfE analysts network mailing list.\nWe’d also encourage analysts to make more use of the RAP Community channel in the DfE Analyst Network Teams group to post questions and ask about other people doing similar types of analysis, no question is too big or too small!\n\n\n\nI don’t have the skills to implement RAP, how do I get them?\n\nSee the top section on the support on offer!\nPlus, if you’re ever unsure at all, you can always contact statisics.development@education.gov.uk who will be able to help you find resources that work for you or do bespoke training for your team’s needs.",
    "crumbs": [
      "Reproducible Analytical Pipelines (RAP)",
      "RAP FAQs"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html",
    "href": "RAP/rap-statistics.html",
    "title": "RAP for Statistics",
    "section": "",
    "text": "Guidance for how to implement the principles of Reproducible Analytical Pipelines (RAP) into statistics production processes",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#what-is-rap",
    "href": "RAP/rap-statistics.html#what-is-rap",
    "title": "RAP for Statistics",
    "section": "What is RAP?",
    "text": "What is RAP?\nRAP (Reproducible Analytical Pipelines) are a way to create well documented, reproducible, quality analysis using the best tools available to us as analysts. In 2022, the Analysis Function published their RAP strategy, which outlines the expectation that RAP should be “the default approach to analysis in government”. Each department is expected to publish a RAP implementation strategy to explain how they are going to support analysts to embed RAP into their day to day work. You can view the RAP implementation plans for all departments, including DfE on the Analysis Function website.\nCam ran an introduction to RAP session for DISD in December 2020. The slides can be found on GitHub, or you can watch the recording.\nRAP means using automation to our advantage when analysing data, which can be as simple as writing code so that we can click a button to execute and do the job for us. Most analysts will be using elements of RAP in their work, such as quality assurance and version control, without even thinking about it!\nCross-government RAP champions have laid out a minimum level of RAP to aim for. In DfE we have adapted these to form our own RAP baseline standards, which are described in detail on this page.\n\n\n\n\n\n\nNote\n\n\n\nThere is currently a clear expectation at DfE that any analyst working in statistics production should know RAP principles and be able to implement them using the recommended tools to meet at least the department’s definitions of “good” and “great” practice.\n\n\n\n\nBenefits of RAP\n\nAccording to the Analysis Function, using RAP should:\n\nimprove the quality of the analysis\nincrease trust in the analysis by producers, their managers and users\ncreate a more efficient process\nimprove business continuity and knowledge management\n\nIn DfE, we already have ‘analytical pipelines’ and have done for many years. The aim of RAP is to automate the parts of these pipelines that can be automated, to increase efficiency and accuracy, while creating a clear audit trail to allow analyses to easily be re-run if needed. This will free us up to focus on the parts of our work where our human input can really add value. RAP is something we can use to reduce the burden on us by getting rid of some of the boring stuff, what’s not to like! RAP can also reduce risk through human error, since you will no longer have to copy and paste numbers between different documents or make substantial manual edits to code each time you need to re-run it.\n\n\n\nOur scope\n\nWe want to focus on the parts of the production process that we have ownership and control over – so for statistics production we are focussing on the process from data sources to publishable data files. This is the part of the process where RAP can currently add the most value - automating the production and quality assurance of our outputs currently takes up huge amount of analytical resource, which could be better spent providing insight and other value adding activity. \n\nIn Official Statistics production we are using RAP as a framework for best practice when producing our published data files, as these are the foundations of our publications moving forward. Following this framework will help us to improve and standardise our current production processes and provide a clear ‘pipeline’ for analysts to follow. To get started with RAP, we first need to be able to understand what it actually means in practice, and be able to assess our own work against the principles of RAP. From there, we can work out what training is needed, if any, and where additional support can help teams to meet the baseline.\nIn other areas of analysis, we recommend that RAP principles are applied proportionately. Whilst you wouldn’t create a full RAP process for an ad-hoc, you could still version control your code so that it could be reused if similar requests came in, and you should get your code peer reviewed by someone before sending out any results.\nImplementing RAP for us will involve combining the use of SQL, R, and clear, consistent version control to increase efficiency and accuracy in our work. For more information on what these tools are, why we are using them, and resources to help upskill in those areas, see our learning resources page.\nThe collection of, and routine checking of data as it is coming into the department is also an area that RAP can be applied to. We have kept this out of scope at the moment as the levels of control in this area vary wildly from team to team. If you would like advice and help to automate any particular processes, feel free to contact the Statistics Development Team.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#core-principles",
    "href": "RAP/rap-statistics.html#core-principles",
    "title": "RAP for Statistics",
    "section": "Core principles",
    "text": "Core principles\nRAP has three core principles:\nPreparing data: Data sources for a publication are stored in the same database\nWriting code: Underlying data files are produced using code, with no manual steps\nVersion control: Files and scripts should be appropriately version controlled\nWithin each of these principles are separate elements of RAP. Each of these is discussed in detail below so that you know what is expected of you as an analyst.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#rap-in-practice",
    "href": "RAP/rap-statistics.html#rap-in-practice",
    "title": "RAP for Statistics",
    "section": "RAP in practice",
    "text": "RAP in practice\nThe diagram below highlights what RAP means for us, and the varying levels in which it can be applied in all types of analysis. You can click on each of the hexagons in the diagram to learn more about each of the RAP principles and how to use them in practice.\nThe expectation is that all statistics publications will meet the department’s baseline implementation of RAP, using the self-assessment tool to monitor their progress. It’s worth acknowledging that some teams are already working around great and best practice levels, and that we appreciate every team’s situation is unique, our guidance is designed to be applicable across all official statistics publications by DfE. Once teams achieve baseline status, their RAP process will be audited.\n\n\n\nhex-diagram",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#what-is-expected",
    "href": "RAP/rap-statistics.html#what-is-expected",
    "title": "RAP for Statistics",
    "section": "What is expected",
    "text": "What is expected\n\n\n\n\n\n\nWarning\n\n\n\nIt is expected that all teams’ processes meet all elements of good and great practice as a baseline.\nTeams are expected to review their own processes using the publication self-assessment tool and use the guidance on this site to start making improvements towards meeting the core principles if they aren’t already. If you would like additional help to review your processes, please contact the Statistics Development Team.\n\n\nTeams will start from different places and implement changes at different rates, and in different ways. We do not expect that every team will follow the same path, or even end at the same point. Don’t worry if this seems overwhelming at first, use the guidance here to identify areas for improvement and then tackle them with confidence.\nWhile working to reach our baseline expectation of good and great practice, you can track your progress in the publication self-assessment tool and contact the Statistics Development Team for help and support.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#how-to-assess-your-publication",
    "href": "RAP/rap-statistics.html#how-to-assess-your-publication",
    "title": "RAP for Statistics",
    "section": "How to assess your publication",
    "text": "How to assess your publication\nThe checklist provided in the publication self-assessment tool, shown below, is designed to make reviewing our processes against our RAP levels easier, giving a straightforward list of questions to check your work against. This will flag potential areas of improvement, and you can then use the links on the right hand side to go to the specific section of this page to find more detail and guidance on how to develop your current processes in line with best practice.\n\nSome teams will already be looking at best practice, while others will still have work to do to achieve the department’s baseline of good and great practice. We know that all teams are starting this from different points, and are here to support all teams from their respective starting positions.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#how-to-get-started-with-rap",
    "href": "RAP/rap-statistics.html#how-to-get-started-with-rap",
    "title": "RAP for Statistics",
    "section": "How to get started with RAP",
    "text": "How to get started with RAP\nCheck your analysis against the Good, Great and Best practice standards under RAP in practice.\nMeasure your publication against the RAP levels using our self assessment tool. This will give you a good starting point and initial points to work on to progress to the next level of RAP.\nOnce you’ve assessed your publication, have a look through our guidance below to narrow down how you can get started with improving those parts of your process.\nThe Statistics Development Team invites teams to take part in our partnership programme to develop their skills and implement RAP principles to a relevant project. Partnership programmes can offer additional resource and dedicated support to your team to implement specific RAP principles. Visit our page on getting started with the partnership programme for more details.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#how-does-rap-fit-into-ada-databricks",
    "href": "RAP/rap-statistics.html#how-does-rap-fit-into-ada-databricks",
    "title": "RAP for Statistics",
    "section": "How does RAP fit into ADA / Databricks",
    "text": "How does RAP fit into ADA / Databricks\nTeams can use data held on the Databricks platform to implement RAP principles like version control, automated QA and automated pipelines. If you have an existing pipeline, please see the ADA guidance on statistics publications to see how migrating to ADA and Databricks will affect your work.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#preparing-data",
    "href": "RAP/rap-statistics.html#preparing-data",
    "title": "RAP for Statistics",
    "section": "Preparing data",
    "text": "Preparing data\nPreparing data is our first core RAP principle, which contains the following elements:\n\nAll source data stored in a database\nFiles meet data standards\nSensible file and folder structure\n\nYour team should store the raw data you use to create underlying data in a Microsoft SQL Server database or in the Databricks platform. These are similar to a Sharepoint area or a shared folder, but offer dedicated data storage areas and allow multiple users to use the same file at once. This means that you can also run code against a single source of data, further reducing the risk of error.\n\n\nAll source data stored in a database\n\n\nWhat does this mean?\nWhen we refer to ‘source data’, we take this to mean the data you use at the start of your process to create the underlying data files. Any cleaning at the end of a collection will happen before this.\nIn order for us to be able to have an end-to-end data pipeline where we can replicate our analysis across the department, we should store all of the raw data needed to create aggregate statistics in a managed Microsoft SQL Server or in the Databricks platform. This includes any lookup tables and all administrative data from collections prior to any manual processing. This allows us to then match and join the data together in an end-to-end process using SQL queries.\nAs far as meeting the requirement to have all source data in a database, databases other than SQL or Databricks may be acceptable, though we can’t support them in the same way.\nWhy do it?\nThe principle is that this source data will remain stable and is the point you can go back to and re-run the processes from if necessary. If for any reason the source data needs to change, your processes will be set up in a way that you can easily re-run them to get updated outputs based on the amended source data with minimal effort.\nSQL is a fantastic language for large scale data joining and manipulation; it allows us to replicate end-to-end from raw data to final aggregate statistics output. Having all the data in one place and processing it in one place makes our lives easier, and also helps us when auditing our work and ensuring reproducibility of results. You can run SQL queries against data in SQL Server or in Databricks, although SQL Server makes use of T-SQL and Databricks makes use of Spark SQL.\nHow to get started\nFor a collection of relevant resources to use when learning SQL, see our learning resources page, and for guidance on best practice when writing SQL queries, see the writing code and documentation sections on this page, as well as the guides immediately below on how to set up and use a SQL database.\nFor resources to help you learn about Databricks and how to migrate your data into the Unity Catalog, please see our ADA and Databricks documentation. Legacy servers will be decommissioned in 2026, and all data will be migrated into the Databricks Unity Catalog instead.\n\n\nHow to set up a SQL working area\n\nThere are a few different options, depending on where you want your new area to exist. Visit our SQL learning page for details.\n\n\n\nMoving data to different areas\n\nIf your data is already in SQL Server, you can use this snippet of R code to move tables from one area (e.g. the iStore) to another (e.g. your team’s modelling area) to ensure all data are stored in a database.\n\nlibrary(odbc)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\n\n# Step 1.1.: Connect to source server -------------------------------------------\ncon_source &lt;- dbConnect(odbc(),\n                     Driver = \"SQL Server Native Client 11.0\",\n                     Server = \"Name_of_source_server\",\n                     Database = \"Source_database\",\n                     Trusted_Connection = \"yes\"\n)\n\n# Step 1.2.: Connect to target server\ncon_target &lt;- dbConnect(odbc(),\n                        Driver = \"SQL Server Native Client 11.0\",\n                        Server = \"Name_of_target_server\",\n                        Database = \"Your_target_database\",\n                        Trusted_Connection = \"yes\"\n)\n\n# Step 2.1.: Pull the table from the source database\ntable_for_transfer &lt;- tbl(con_source,in_schema(\"schema_name\", \"table_name\")) %&gt;% collect()\n\n# Step 2.2.: Copy table into target database \ndbWriteTable(con_target,\"whatever_you_want_to_call_new_table\", table_for_transfer)\n\n\n\n\nImporting data to SQL Server\n\nThere’s lots of guidance online of how to import flat files from shared areas into Microsoft SQL Server on the internet, including this guide.\nRemember that it is important to import them with consistent, thought-through naming conventions. You will thank yourself later.\n\n\n\nHow to grant access to your area\n\nMuch like setting up a SQL area, there are different ways to do this depending on the server your database is in. Visit our SQL learning page for details.\n\n\n\n\nFiles meet data standards\n\n\n\nOpen Data Standards\n\nYou can find information on our standards for Open Data, including how to assess your data against these standards, on our Open Data standards page.\n\n\n\nStandardised reference data\n\nThere will of course be cases where some of the data you use is reference data not owned by the DfE, or is available online for you to download rather than in an internal server. There are ways of incorporating this into a reproducible analytical pipeline nicely, sometimes you can even use links/URLs in your code that will always pull the latest data such that you will never need to change the link to reflect updates to the data!\n\n\n\nRaw files on GitHub\n\nIf you want to use reference data from GitHub, you can use the URL to the raw file. A common example of when you might want to use this would be to use geographic lookup tables that contain names and codes of different geographic levels for EES files (which are available in our data screener repository).\nIf you find the data you’re interested in within a repository, rather than copying, cloning or downloading the data, you should click the ‘raw’ button (see the below screenshot).\n\nThis should take you to a new window which contains the raw data from the CSV file. Copy the URL from this page, as this is what we can use to pull the latest data into our code.\nYou can now use this URL as you would use a file path in a read.csv() query. For example;\n\ndevolved_area_lookup &lt;- read.csv(\"https://raw.githubusercontent.com/dfe-analytical-services/dfe-published-data-qa/master/data/english_devolved_areas.csv\")\n\nThe above code snippet will load in the data as it would with any other CSV, however the benefit is if that data file is updated on GitHub, when you run the code it will always pull the latest version. This is especially useful for lookup tables and reference data, and is best RAP practice as it removed the need for any manual steps (like downloading, copying & pasting or manually updating the data/code!)\n\n\n\n\nSensible folder and file structure\n\n\nWhat does this mean?\nAs a minimum you should have a folder that includes all of the final versions of documents produced and published, per release, within a folder for the wider publication. Ask yourself if it would be easy for someone who isn’t in the team to find specific files, and if not, is there a better way that you could name and structure your folders to make them more intuitive to navigate?\nWhy do it?\nHow you organize and name your files will have a big impact on your ability to find those files later and to understand what they contain. You should be consistent and descriptive in naming and organizing files so that it is obvious where to find specific data and what the files contain.\nHow to get started\nSome questions to help you consider whether your folder structure is sensible are:\n\nAre all documentation, code and outputs for the publication saved in one folder area?\nIs simple version control clearly applied (e.g. having all final files in a folder named “final”?\nAre there sub-folders like ‘code’, ‘documentation’‘, ’outputs’ and ‘final’ to save the relevant working files in?\nAre you keeping a version log up to date with any changes made to files in this final folder?\n\nYou could also consider using the create_project() function from the dfeR package to create a pre-populated folder structure for use with an R project\n\n\nNaming conventions\n\nHaving a clear and consistent naming convention for your files is critical. Remember that file names should:\nBe machine readable\n\nAvoid spaces.\nAvoid special characters such as: ~ ! @ # $ % ^ & * ( ) ` ; &lt; &gt; ? , [ ] { } ‘ “.\nBe as short as practicable; overly long names do not work well with all types of software.\n\nBe human readable\n\nBe easy to understand the contents from the name.\n\nPlay well with default ordering\n\nOften (though not always!) you should have numbers first, particularly if your file names include dates.\nFollow the ISO 8601 date standard (YYYYMMDD) to ensure that all of your files stay in chronological order.\nUse leading zeros to left pad numbers and ensure files sort properly, e.g. using 01, 02, 03 to avoid 1, 10, 2, 3.\n\nIf in doubt, take a look at this presentation, or this naming convention guide by Stanford, for examples reinforcing the above.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#writing-code",
    "href": "RAP/rap-statistics.html#writing-code",
    "title": "RAP for Statistics",
    "section": "Writing code",
    "text": "Writing code\nWriting code is our second core RAP principle, and is made up of the following elements:\n\nProcessing is done with code\nUse appropriate tools\nWhole publication production scripts\nDataset production scripts\nRecyclable code for future use\nClean final code\nPeer review of code within team\nPeer review of code from outside the team\nBasic automated QA\nPublication specific automated QA\nAutomated summaries\nPublication specific automated summaries\n\n\n\n\n\n\n\nTip\n\n\n\nThe key thing to remember is that we should be automating everything we can, and the key to automation is writing code. Using code is as simple as telling your computer what to do. Code is just a list of instructions in a language that your computer can understand. We have links to many resources to help you learn to code on our learning support page.\n\n\n\n\nProcessing is done with code\n\n\nWhat does this mean?\nAll extraction, and processing of data should be done using code, avoiding any manual steps and moving away from a reliance on Excel, SPSS, and other manual processing. In order to carry out our jobs to the best of our ability it is imperative that we use the appropriate tools for the work that we do.\nEven steps such as copy and pasting data, or pointing and clicking, are fraught with danger, and these risks should be minimised by using code to document and execute these processes instead.\nWhy do it?\nUsing code brings numerous benefits. Computers are far quicker, more accurate, and far more reliable than humans in many of the tasks that we do. Writing out these instructions saves us significant amounts of time, particularly when code can be reused in future years, or even next week when one specific number in the source file suddenly changes. Code scripts also provide us with editable documentation for our production processes, saving the need for writing down information in extra documents.\nReliability is a huge benefit of the automation that RAP brings - when your data has to be amended a week before publication, it’s a life saver to know that you can re-run your process in minutes, and reassuring to know that it will give you the result you want. You can run the same code 100 times, and be confident that it will follow the same steps in the same order every single time.\nHow to get started\nSee our learning resources for a wealth of resources on SQL and R to learn the skills required to translate your process into code.\nThere are also two sections below with examples of tidying data in SQL and R to get you started.\nEnsure that any last-minute fixes to the process are written in the code and not done with manual changes.\n\n\nProducing tidy underlying data in SQL\n\nTo get started, here is a SQL query that you can run on your own machine and walks you through the basics of tidying a simple example dataset in SQL.\n\n\n\nTidying and processing data in R\n\nHere is a video of Hadley Wickham talking about how to tidy your data to these principles in R. This covers useful functions and how to complete common data tidying tasks in R. Also worth taking a look at applied data tidying in R, by RStudio.\nUsing the %&gt;% pipe in R can be incredibly powerful, and make your code much easier to follow, as well as more efficient. If you aren’t yet familiar with this, have a look at this article that provides a useful beginners guide to piping and the kinds of functions you can use it for. The possibilities stretch about as far as your imagination, and if you have a function or task you want to do within a pipe, googling ‘how do I do X in dplyr r’ will usually start to point you in the right direction, alternatively you can contact us, and we’ll be happy to help you figure out how to do what you need.\nA quick example of how powerful this is is below. The pipe operator passes the outcome of each line of code onto the next, so you can complete multiple steps of data manipulation in one section of code instead of writing separate steps for each one. In this code, we:\n\nstart with my_data\ncalculate a percentage column using mutate\nrename the percentage column we created to “newPercentageColumn”, rename “number” to “numberColumn”, and rename “population” to “totalPopulationColumn”\nuse the clean_names() function from the janitor package to ensure that columns have consistent naming standards\nuse the remove_empty() function from the janitor package to remove any rows and columns that are composed entirely of NA values\nfilter the dataframe to only include Regional geographic level data\norder the dataframe by time period and region name\n\n\nprocessed_regional_data &lt;- my_data %&gt;% \n  mutate(newPercentageColumn = (numberColumn / totalPopulationColumn) * 100) %&gt;% \n  rename(newPercentageColumn = percentageRate,\n         numberColumn = number,\n         totalPopulationColumn = population) %&gt;% \n  clean_names() %&gt;% \n  remove_empty() %&gt;% \n  filter(geographic_level == \"Regional\") %&gt;% \n  arrange(time_period, region_name)\n\nHelpful new functions in the tidyverse packages can help you to easily transform data from wide to long format (see tip 2 in the linked article for this, as it is often required for tidy data), as well as providing you with tools to allow you quickly and efficiently change the structure of your variables.\nFor further resources on learning R so that you’re able to apply it to your everyday work, have a look at the learning resources page.\n\n\n\n\nUse appropriate tools\n\n\nWhat does this mean?\nUsing the recommended tools on our learning page (SQL, R and Git), or other suitable alternatives that allow you to meet the core principles. Ideally any tools used would be open source, Python is a good example of a tool that would also be well suited, though is less widely used in DfE and has a steeper learning curve than R.\nOpen-source refers to something people can modify and share because its design is publicly accessible. For more information, take a look at this explanation of open-source, as well as this guide to working in an open-source way. In practical terms, this means moving away from the likes of SPSS, SAS and Excel VBA, and utilising the likes of R or Python, version controlled with git, and hosted in a publicly accessible repository.\nWhy do it?\nThere are many reasons why we have recommended the tools that we have, the recommended tools are:\n\nalready in use at the department and easy for us to access\neasy and free to learn\ndesigned for the work that we do\nused widely across data science in both the public and private sector\nallow us to meet best practice when applying RAP to our processes\n\nHow to get started\nGo to our learning page to read more about the recommended tools for the jobs we do, as well as looking at the resources available there for how to build capability in them. Always feel free to contact us if you have any specific questions or would like help in understanding how to use those tools in your work.\nBy following our guidance in saving versions of code in an Azure DevOps, we will then be able to mirror those repositories in a publicly available GitHub area.\n\n\n\nWhole publication production scripts\n\n\nWhat does this mean?\nThe ultimate aim is to utilise a single script to document and run off everything for a publication, the data files, any QA, any summary reports. This script should allow you to run individual outputs by themselves as well, so make sure that each data file can be run in isolation by running single lines of this script. All quality assurance for a file is also included in the single script that can be used to create a file from source data (see the dataset production scripts section)\nWhy do it?\nThis carries all of the same benefits as having a single ‘run’ script for a file, but at a wider publication level, effectively documenting the entire publication process in one place. This makes it easier for new analysts to pick up the process, as well as making it quicker and easier to rerun as all reports relating to that file are immediately available if you ever make changes file.\nHow to get started\nThe Education, Health and Care Plans production cycle is a good example of a single publication ‘run’ script. They have kept their actual data processing in SQL, but all the running and manipulation of the data happens in R.\nThe cycle originally consisted of multiple SQL scripts, manual QA and generation of final files.\n\nThe team now have their end-to-end process fully documented, which can be run off of one single R script. The ‘run’ script points at the SQL scripts to run them all in one go, and also creates a QA report and corresponding metadata files that pass the data screener. Each data file can still be run in isolation from this script.\n\n\n\nUsing ‘run’ scripts\nUtilising a single ‘run’ script to execute processes written in other scripts brings a number of benefits. It isn’t just about removing the need to manually trigger different code scripts to get the outputs, but it means the entire process, from start to finish, is fully documented in one place. This has a huge number of benefits, particularly for enabling new team members to pick up existing work quickly, without wasting time struggling to understand what has been done in the past.\n\n\n\nConnecting R to SQL\n\nIn order to create a single script to run all processes from, it is likely that you will need to use R to run SQL queries. If you are unsure of how to do this, take a look the guide on the dfeR connecting to SQL documentation.\nIf you prefer a video, Chris Mason-Thom did a coffee and coding session on this.\n\n\n\n\nDataset production scripts\n\n\nWhat does this mean?\nEach dataset can be created by running a single script, which may ‘source’ multiple scripts within it. This does not mean that all of the code to create a file must be written in a single script, but instead that there is a single ‘create file’ or ‘run’ script that sources every step in the correct order such that every step from beginning to end will be executed if you run that single ‘run’ script.\nThis ‘run’ script should take the source data right through to final output at the push of a button, including any manipulation, aggregation, suppression etc.\nWhy do it?\nHaving a script that documents the whole process for this saves time when needing to rerun processes, and provides a clear documentation of how a file is produced.\nHow to get started\nReview your current process - how many file scripts does it take to get from source data to final output, why are they separated, and what order should they be run in? Do you still have manual steps that could introduce human error (for example, manually moving column orders around in excel)?\nYou should automate any manual steps such as the example above. If it makes sense to, you could combine certain scripts to reduce the number. You can then write code in R to execute your scripts in order, so you are still only running one script to get the final output.\n\n\n\nRecyclable code for future use\n\n\nWhat does this mean?\nWe’d expect that any recyclable code would take less than 30 minutes of editing before being able to run again in a future iteration of the publication.\nWhy do it?\nOne huge benefit that comes with using code in our processes, is that we can pick them up in future years and reuse with minimum effort, saving us huge amounts of resource. To be able to do this, we need to be conscious of how we write our code, and write it in a way that makes it easy to use in future releases for the publication.\nHow to get started\nReview your code and consider the following:\n\nWhat steps might need re-editing or could become irrelevant?\nCan you move all variables that require manual input (e.g. table names, years) to be assigned at the top of the code, so it’s easy to edit in one place with each iteration?\nAre there any fixed variables that are prone to changing such as geographic boundaries, that you could start preparing for changes now by making it easy to adapt in future?\n\nFor example, if you refer to the year of publication in your code a lot, consider replacing every instance with a named variable, which you only need to change once at the start of your code. In the example below, the year is set at the top of the code, and is used to define “prev_year”, both of which are used further down the code to filter the data based on year.\n\nthis_year &lt;- 2020\nprev_year &lt;- this_year - 1\n\ndata_filtered &lt;- data %&gt;% \n  filter(year == this_year)\n\ndata_filtered_last_year &lt;- data %&gt;% \n  filter(year == prev_year)\n\n\n\nStandards for coding\n\nCode can be written in many different ways, and in languages such as R, there are often many different functions and routes that you can take to get to the same end result. On top of that, there are even more possibilities for how you can format the code. This section will take you through some widely used standards for coding to help bring standardisation to this area and make it easier to both write and use our code.\n\n\n\n\nClean final code\n\n\nWhat does this mean?\n\nThis code should meet the best practice standards below (for SQL and R). If you are using a different language, such as Python, then contact us for advice on the best standards to use when writing code.\nThere should be no redundant or duplicated code, even if this has been commented out. It should be removed from the files to prevent confusion further down the line.\nThe only comments left in the code should be those describing the decisions you have made to help other analysts (and future you) to understand your code. More guidance on commenting in code can be found later on this page.\n\nWhy do it?\nClean code is efficient, easy to write, easy to review, and easy to amend for future use. Below are some recommended standards to follow when writing code in SQL and R.\nHow to get started\nWatch this coffee and coding session introducing good code practice, which covers:\n\nkey principles of good code practice\nwriting and refining code to make it easier to understand and modify\na real-life example of code improvement from within DfE\n\nThen you should also watch the follow up intermediate session, which covers:\n\nversion control\nimproving code structure with functions\ndocumentation and Markdown\ninteractive notebooks\n\nClean code should include comments. Comment why you’ve made decisions, don’t comment what you are doing unless it is particularly complex as the code itself describes what you are doing. If in doubt, more comments are better than too few though. Ideally any specific comments or documentation should be alongside the code itself, rather than in separate documents.\n\n\nSQL\n\nFor best practice on writing T-SQL code used in SQL Server, here is a particularly useful Word document produced by our Data Hub. This outlines a variety of best practices, ranging from naming conventions, to formatting your SQL code so that it is easy to follow visually.\n\n\n\nR\n\nWhen using R, it is generally best practice to use R projects as directories for your work.\nThe recommended standard for styling your code in R is the tidyverse styling, which is fast becoming the global standard. What is even better is that you can automate this using the styler package, which will literally style your code for you at the click of a button, and is well worth a look.\n\nThere is also plenty of guidance around the internet for best practice when writing efficient R code.\nTo help you standardise your code further, you can make use of the functions contained within our dfeR package. The package includes functions to standardise formatting and rounding, to pull the latest ONS geography lookups, and to create a pre-populated folder structure, amongst many other things.\n\n\n\nHTML\n\nIf you ever find yourself writing HTML, or creating it through RMarkdown, you can check your HTML using W3’s validator.\n\n\n\n\nPeer review of code within team\n\n\nWhat does this mean?\nPeer review is an important element of quality assuring our work. We often do it without realising by bouncing ideas off of one another and by getting others to ‘idiot check’ our work. When writing code, ensuring that we get our work formally peer reviewed is particularly important for ensuring it’s quality and value. The Duck Book and Tidyteam contain detailed guidance on peer review, but we have summarised some of the information here for you as well.\nPrior to receiving code for peer review, the author should ensure that all code files are clean, commented appropriately and for larger projects should be held in a repo with an appropriate README file.\nYou should check:\n\nIs someone else in the team able to generate the same outputs?\nHas someone else in the team reviewed the code and given feedback?\nHave you taken on their feedback and improved the code?\n\nWhy do it?\nThere are many benefits to this, for example:\n\nEnsuring consistency across the team\nMinimizing mistakes and their impact\nEnsuring the requirements are met\nImproving code performance\nSharing of techniques and knowledge\n\nHow to get started\nWhen peer reviewing code you should consider the following questions -\n\nDo you understand what the code does? If not, is there supporting documentation or code comments that allow you to understand it?\nDoes the code do what the author intended?\nHave any dependencies (either on separate pieces of code, data files, or packages) been documented?\nAre there any tests / checks that could be added into the code that would help to give greater confidence that it is doing what it is intended to?\nAre there comments explaining why any decisions have been made?\nIs the code written and structured sensibly?\nAre there any ways to make the code more efficient (either in number of lines or raw speed)? Is there duplication that could be simplified using functions?\nDoes the code follow best practice for styling and structure?\nAre there any other teams/bits of code you’re aware of that do similar things and would be useful to point the authors towards?\nAt the end of the review, was there any information you needed to ask about that should be made more apparent in the code or documentation?\n\nDepending on your access you may or may not be able to run the code yourself, but there should be enough information within the code and documentation to be able to respond to the questions above. If you are able to run the code, you could also check -\n\nDoes the code run without errors? If warnings are displayed, are they explained?\nIf the project has unit/integration tests, do they pass?\nCan you replicate previous output using the same code and input data?\n\nIf you would like a more thorough list of questions to follow, then the Duck Book has checklists available for three levels of peer review, based on risk:\n\nLower\nModerate\nHigher\n\nIf you’re unfamiliar with giving feedback on someone’s code then it can be daunting at first. Feedback should always be constructive and practical. It is recommended that you use the CEDAR model to structure your comments:\n\nContext - describe the issue and the potential impact\nExamples - give specific examples of when and where the issue has been present (specifying the line numbers of the code where the issue can be found can be useful here)\nDiagnosis - use the example to discuss why this approach was taken, what could have been done differently and why alternatives could be an improvement\nActions - ask the person receiving feedback to suggest actions that they could follow to avoid this issue in future\nReview - if you have time, revisit the discussion to look for progress following on from the feedback\nOther tips for getting started with peer review can be found in the Duck Book\nThe Duck Book also contains some helpful code QA checklists to help get you thinking about what to check\n\n\n\nImproving code performance\n\nPeer reviewing code and not sure where to start? Improving code performance can be a great quick-win for many production teams. There will be cases where code you are reviewing does things in a slightly different way to how you would: profiling the R code with the microbenchmark package is a way to objectively figure out which method is more efficient.\nFor example below, we are testing out case_when, if_else and ifelse.\n\nmicrobenchmark::microbenchmark( \n   case_when(1:1000 &lt; 3 ~ \"low\", TRUE ~ \"high\"), \n   if_else(1:1000 &lt; 3, \"low\", \"high\"),\n   ifelse(1:1000 &lt; 3, \"low\", \"high\") \n)\n\nRunning the code outputs a table in the R console, giving profile stats for each expression. Here, it is clear that on average, if_else() is the fastest function for the job.\n\nUnit: microseconds\n                                         expr     min       lq     mean   median       uq      max neval\n case_when(1:1000 &lt; 3 ~ \"low\", TRUE ~ \"high\") 167.901 206.2510 372.7321 300.2515 420.1005 4187.001   100\n           if_else(1:1000 &lt; 3, \"low\", \"high\")  55.301  74.0010 125.8741 103.7015 138.3010  538.201   100\n            ifelse(1:1000 &lt; 3, \"low\", \"high\") 266.200 339.4505 466.7650 399.7010 637.6010  851.502   100\n\n\n\n\n\nPeer review of code from outside the team\n\n\nWhat does this mean?\n\nHas someone from outside of the team and publication area reviewed the code and given feedback?\nHave you taken on their feedback and improved the code?\n\nWhy do it?\nAll of the benefits you get from peer reviewing within your own team, multiple times over. Having someone external offers new perspectives, holds you to account by breaking down assumptions, and offers far greater opportunity for building capability through knowledge sharing.\nHow to get started\nWhile peer reviewing code within the team is often practical, having external analysts peer review your code can bring a fresh perspective. If you’re interested in this, please contact us, and we can help you to arrange someone external to your team to review your processes. For this to work smoothly, we recommend that your code is easily accessible for other analysts, such as hosted in an Azure DevOps repo and mirrored to github.\n\n\n\nBasic automated QA\n\n\nWhat does this mean?\nAny data files that have been created will need to be quality assured. These checks should be automated where possible, so the computer is doing the hard work - saving us time, and to ensure their reliability.\nSome teams are already making great progress with automated QA and realising the benefits of it. The Statistics Development Team are working with these to provide generalised code that teams can use as a starting point for automated QA. The intention is that teams can then run this as a minimum, before then looking to develop more area specific checks to the script and/or continue with current checking processes in tandem. If your team already use, or are working towards using, automated QA then get in touch as we’d be keen to see what you have.\nIt is assumed that when using R, automated scripts will output .html reports that the team can read through to understand their data and identify any issues, and save as a part of their process documentation.\nFor more information on general quality assurance best practice in DfE, see the How to QA guide.\nCam and Sarah ran a session introducing how to get started with automated QA in relation to RAP, slides are available on GitHub or you can watch the recording of the session.\nThe list of basic automated QA checks, with code examples can be found below and in our GitHub repository:\n\nChecking for minimum, maximum, and average values across your data\nChecking for extreme values and outliers\nEnsuring there are no duplicate rows or duplicate columns\nChecking that where appropriate, geographical subtotals add up to totals (e.g. all the numeric values for LAs in Yorkshire and The Humber add up to the regional total)\nBasic trend analysis using scatter plots, to help you spot outliers and help tell the story of your data.\n\nThe Statistics Development Team have developed the QA app to include some of these basic QA outputs.\nWhy do it?\nQuality is one of the three pillars that our code of practice is built upon. These basic level checks allow us to have confidence that we are accurately processing the data.\nAutomating these checks ensures their accuracy and reliability, as well as being dramatically quicker than doing these manually.\nHow to get started\nTry using our template code snippets to get an idea of how you could automate QA of your own publication files. A recording of our introduction to automated QA is also available at the top of the page.\n\n\n\nPublication specific automated QA\n\n\nWhat does this mean?\nMany teams will have aspects of their data and processes that require Quality Assuring beyond the generalisable basic checks above. Therefore it is expected that teams develop their own automated QA checks to QA specificities of their publications not covered by the basic checks.\nWhy do it?\nQuality is one of the three pillars that our code of practice is built upon. By building upon the basic checks to develop bespoke QA for our publications, we can increase our confidence in the quality of the processes and outputs that they produce.\nHow to get started\nWe expect that the basic level of automated QA will cover most needs that publication teams have. However, we also expect that each publication will have it’s own quirks that require a more bespoke approach. An example of a publication with it’s own bespoke QA checks will appear in this space shortly. For the time being, try to consider what things you’d usually check as flags that something hasn’t gone right with your data. What are the unique aspects of your publication’s data, and how can you automate checks against them to give you confidence in it’s accuracy and reliability?\nFor those who are interested in starting writing their own QA scripts, it’s worth looking at packages in R such as testthat, including the coffee and coding resources on it by Peter Curtis, as well as this guide on testing by Hadley Wickham.\nThe janitor package in R also has some particularly useful functions, such as clean_names() to automatically clean up your variable names, remove_empty() to remove any completely empty rows and columns, and get_dupes() which retrieves any duplicate rows in your data - this last one is particularly powerful as you can feed it specific columns and see if there’s any duplicate instances of values across those columns.\n\n\n\nAutomated summaries\n\n\nWhat does this mean?\nAs a part of automating QA, we should also be looking to automate the production of summary statistics alongside the tidy underlying data files, this then provides us with instant insight into the stories underneath the numbers.\nSummary outputs are automated and used to explore the stories of the data.\nThe Statistics Development Team have developed the QA app to include some of these automated summaries, including minimum, maximum and average summaries for each indicator.\nAt a basic level we want teams to make use of the QA app to explore their data:\n\nHave you used the outputs of the automated QA from the screener to understand the data?\nRun automated QA, ensure that all interesting outputs/trends are reflected in the accompanying text\n\nWhy do it?\nValue is one of the three pillars of our code of practice. Even more specifically it states that ‘Statistics and data should be presented clearly, explained meaningfully and provide authoritative insights that serve the public good.’.\nAs a result, we should be developing automated summaries to help us to better understand the story of the data and be authoritative and rigorous in our telling of it.\nHow to get started\nConsider:\n\nUse the additional tabs available after a data file passes the data screener as a starting point to explore trends across breakdowns and years.\nRunning your publication-specific automated QA, ensuring that all interesting outputs/trends are reflected in the accompanying text\n\n\n\n\nPublication specific automated summaries\n\n\nWhat does this mean?\n\nHave you gone beyond the outputs of the QA app to consider automating further insights for your publication specifically? E.g. year on year changes for specific measures, comparisons of different characteristics that are of interest to the general public\nAre you using these outputs to write your commentary?\n\nWhy do it?\nAll publications are different, and therefore it is important that for each publication, teams go beyond the basics and produce automated summaries specific to their area.\nHow to get started\nConsider:\n\nIntegrating extra publication-specific QA into the production process\nConsider outputs specific to your publication that would help you to write commentary/draw out interesting analysis",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "RAP/rap-statistics.html#version-control",
    "href": "RAP/rap-statistics.html#version-control",
    "title": "RAP for Statistics",
    "section": "Version control",
    "text": "Version control\nVersion control is our third core RAP principle, and is made up of the following elements:\n\nDocumentation\nVersion controlled final code scripts\nUse open source repositories\nCollaboratively develop code using Git\n\nVersion control is just a way to track changes to files. Using proper version control can avoid lots of potential problems, including running old scripts by accident, losing files, or ending up with a folder full of documents with suffixes like “final_final_FINAL”. It also makes it much easier for new people to pick up your processes.\nWhen you assume you make an ‘ass’ out of ‘u’ and ‘me’. Everyone knows this saying, yet few of us heed its warning.\nThe aim should be to leave your work in a state that others (including future you!), can pick it up and immediately find what they need, understanding the processes that have happened previously. Changes to files should be documented, and published versions should be clearly named and stored in their own folder.\nAs we work with code to process our data more and more, we can begin to utilise version control software to make this process much easier, allowing simultaneous collaboration on files.\n\n\nDocumentation\n\n\nWhat does this mean?\n\nYou should be annotating as you go, ensuring that every process and decision made is written down. Processes are ideally written with code, and decisions in comments.\nThere should be a README notes file, that clearly details the steps in the process, any dependencies (such as places where access needs to be requested to) and how to carry out the process.\nAny specialist terms should also be defined if required (e.g. The NFTYPE lookup can be found in xxxxx. “NFTYPE” means school type).\n\nWhy do it?\nWhen documenting your processes you should leave nothing to chance, we all have wasted time in the past trying to work out what it was that we had done before, and that time increases even more when we are picking up someone else’s work. Thorough documentation saves us time, and provides a clear audit trail of what we do. This is key for the ‘Reproducible’ part of RAP, our processes must be easily reproducible and clear documentation is fundamental to that.\nHow to get started\nTake a look at your processes and be critical - could another analyst pick them up without you there to help them? If the answer is no (don’t feel ashamed, it will be for many teams) then go through and note down areas that require improvement, so that you can revise them with your team.\nTake a look at the sections below for further guidance on improving your documentation.\n\n\nCommenting in code\n\nWhen writing code, whether that is SQL, R, or something else, make sure you’re commenting as you go. Start off every file by outlining the date, author, purpose, and if applicable, the structure of the file, like this:\n\n----------------------------------------------------------------------------------------\n-- Script Name:     Section 251 Table A 2019 - s251_tA_2019.sql\n-- Description:     Extraction of data from IStore and production of underlying data file\n-- Author:          Cam Race\n-- Creation Date:   15/11/2019\n----------------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------------------\n--//  Process\n-- 1. Extract the data for each available year\n-- 2. Match in extra geographical information\n-- 3. Create aggregations - both categorical and geographical totals\n-- 4. Tidy up and output results\n-- 5. Metadata creation\n----------------------------------------------------------------------------------------\n\nCommented lines should begin with – (SQL) or # (R), followed by one space and your comment. Remember that comments should explain the why, not the what.\nIn SQL you can also use /** and **/ to bookend comments over multiple lines.\nIn rmarkdown documents you can bookend comments by using &lt;!-- and --&gt;.\nUse commented lines of - to break up your files into scannable chunks based upon the structure and subheadings, like the R example below:\n\n# Importing the data -------------------------------------------------------------------\n\nDoing this can visually break up your code into sections that are easy to navigate around. It will also add that section to your outline, which can be used in RStudio using Ctrl-Shift-O. More details on the possibilities for this can be found in the RStudio guidance on folding and sectioning code.\nYou might be thinking that it would be nice if there was software that could help you with documentation, if so, read on, as Git is an incredibly powerful tool that can help us easily and thoroughly document versions of our files. If you’re at the stage where you are developing your own functions and packages in R, then take a look at roxygen2 as well.\n\n\n\nWriting a README file\n\nWhat does this mean?\nA README is a markdown file (.md) that introduces and explains a project. It contains information that is required to understand what the project is about and how to use it. Markdown (.md) files are used for READMEs because they support formatting and render nicely on platforms like GitHub and Azure DevOps, meaning that users can see them on the main page of the repository. You can find guidance on basic markdown syntax on the Markdown Guide.\nWhy do it?\nIt’s an easy way to answer questions that your audience will likely have regarding how to install and use your project and also how to collaborate with you.\nHow to get started\nFor new projects, you can use the create_project function in dfeR. Set create_publication_proj to TRUE to create a pre-populated project with a custom folder structure, including a README template. You can find more information on this in the dfeR reference.\nIf you are creating your own README for existing projects, you should include all of the sections listed below:\nIntroduction\n\nPurpose: briefly explain the purpose of the code.\nOverview: Provide a high-level summary of the contents and structure of the repository.\n\nRequirements\n\nAccess: Detail any permissions or access needed to use the repository at the top of this section, e.g. access to specific SQL databases. This is crucial for enabling new users to use the repository.\nSkills/knowledge: Outline the required skills or knowledge, such as familiarity with specific packages in R, or SQL.\nVersion control/Renv: State how version control is managed and whether Renv is being used.\n\nGetting started\n\nSetup instructions: Provide step-by-step instructions on how to set up the environment, including installing dependencies.\nData input/output: Describe the expected input data and where it can be found, as well as what output should be expected from the code.\n\nHow to run and update\n\nRunning the code: Explain how users can best run the code, for example by running a run all script.\nUpdating guidelines: Outline the process for updating and contributing to the repository, including specific scripts and lines where updates are frequently needed. Describe how to get changes reviewed.\nIssue reporting: Explain how to report issues or suggest improvements. This could be through issues if using GitHub, boards in Azure DevOps or by emailing the team.\n\nContact details\n\nMain contacts: List the names and contact information of people who maintain the repository.\nSupport channels: Provide any information on how to get support, such as email addresses or teams channels.\n\nThe Self-assessment tool and the QA app give two examples of readme files structured like this.\n\n\n\n\nVersion controlled final code scripts\n\n\nWhat does this mean?\nThis means having the final copies of code and documentation saved in a git-controlled Azure DevOps repo in the official-statistics-production area. Access to DevOps is restricted only to people in DfE with specific account permissions. This is different to GitHub, which makes code publicly available.\nIf you do not already have git downloaded, you can download the latest version from their website.\nFor now, take a look at the resources for learning git in the learning resources section.\nWhy do it?\nHaving the final copy of the scripts version controlled gives assurance around how the data was created. It also allows teams to easily record any last minute changes to the code after the initial final version by using the version control to log this.\nHow to get started\nThe first step is to get your final versions of code and documentation together in a single folder.\nWe have a specific area set up for you to host your publication code in on the dfe-gov-uk instance of Azure DevOps, entitled official-statistics-production.\nTo gain access to this area, please raise a request on service desk by navigating through the pages detailed in the animation below.\n\nOnce you have navigated to this page, fill out the form with the following details and send your request off.\n\nAccess is usually granted within a few working days. Alert the Statistics Development Team when this is confirmed, and we will set up your repository and give your team access.\n\n\nrenv\n\nWe recommend the use of the renv package to help maintain consistent versions of packages within your work. You can learn more about how to use renv on our R page.\n\n\n\n\nUse open source repositories\n\n\nWhat does this mean?\nSaving or cloning your work into a repository that is visible to the public. We currently have brilliant examples of this in our DfE analytical services GitHub area, in which all of the code used to create public dashboards is publicly available.\nFor statistics publications we expect teams to be able to mirror their proccess code on GitHub after publication, which will help open up their code for other analysts to learn from.\nWe are currently working on ways to mirror private repos (i.e. AzureDevOps) to public repos on publication of your data. If you are interested in this please contact the Statistics Development Team.\nWhy do it?\nIt’s a key part of the technology code of practice as an agreed standard for digital services across government.\nHow to get started?\nContact us to get a repository set up in Azure DevOps, to set up a mirroring process to GitHub or to set up a repository on our dfe-analytical-services area on GitHub.\nYou should consider the following principles making an Official Statistics production repository public (some examples are R specific though can be applied to other languages):\n\nFollow the guidance on writing a readme file, and add context in about what Official/National statistics are\nEnsure no data (either input or output) is included in the repository\nHave a clear and organised folder structure (such as having R scripts in an ‘R’ folder)\nCheck your code is styled according the tidyverse styling\nUse renv for package management\nUse an R project\n\nWhen naming your publication’s repository you should use the publication name fully written out, in lower case, and with dashes for spaces – ‘graduate-labour-market-statistics’.\nA single repository should be used for all releases of your publication, there’s no need to have multiple as all the history is saved within previous commits. You can make use of tagging releases in Git to help differentiate between each cycle.\n\n\nAvoid revealing sensitive information\n\nHere are some general best practice tips:\n\nUsing .gitignore to ignore files and folders to prevent committing anything sensitive\nNever committing outputs unless they’ve been checked over, even aggregates. We suggest only outputting to an output folder which is in the .gitignore file, to ensure this doesn’t happen by mistake\nKeeping datasets and secrets (e.g. API keys) outside the repository as much as possible, make use of secure variables\nChecking Git histories: if someone is planning on open-sourcing code that has previously been in a private repository or only version-controlled locally, you want to be careful not to have anything sensitive in the commit history. You can do this by following the above rules. When in doubt, you can remove the git history and start the public repo without it\nYou can remove a file from the entire commit history if you did commit anything sensitive, although you still need to follow the usual procedures if this was a data breach\n\nYou can find out more in the Duck Book’s guidance on using Git.\n\n\n\n\nCollaboratively develop code using Git\n\n\nWhat does this mean?\n\nHas code development taken place in Git, collaboratively across the team?\nAre you making use of pull requests for team members to review and comment on code updates?\nIs there a clear paper trail of changes to code (commits)?\n\nWhy do it?\nUsing Git allows multiple people to simultaneously develop the same code using branches, all with a crystal clear audit trail showing what changes were made when using commits. It makes it easy for team members to review changes via pull requests.\nHow to get started\nTo get started you should:\n\n\nGet your code into a Git controlled folder\n\nGet code into a Git controlled folder in whatever version it is currently in. Use the following steps to do so:\n\nOpen the folder where your project is saved, right click anywhere in that window, and click “Git Bash Here”.\nThis will open a black box (the terminal). Type in the following and hit enter\n\n\ngit init\n\n\nAfter hitting enter, type in the following and hit enter again after each line. You will need the URL of your Azure DevOps repository to complete this step. Contact the Statistics Development Team if you are not sure what this is or do not have one.\n\n\ngit add .\n\ngit commit -m \"first commit\"\n\ngit remote add origin YOUR_URL_HERE\n\ngit push -f origin --all\n\n\nYou may be prompted for either your Windows or Git credentials at this stage.\n\nIf prompted for your Windows credentials, enter the username and password combination you use to log into your DfE device.\nIf prompted for your Git credentials, visit your online repository, click on the blue “clone” box, and click “generate Git credentials”. This will generate a username and password for you to enter.\n\n\n\nVisit your repository online, and check that all the files have uploaded. Other members of your team will now be able to work from your code.\n\n\n\n\nBuild capability within the team\n\n\nEnsure all team members have access to your project in the Azure DevOps official-statistics-production area. Contact the Statistics Development Team if there are any issues.\nGet team members to clone your repository in to their personal area, so everyone is able to work on code at the same time.\n\nTo clone code, they will need to do the following:\n\nRun through steps 1 - 2a of getting a file into a Git controlled folder\nAfter running those lines, type in the following with your repository URL in the “YOUR_URL_HERE” space. This will clone the online repository to your local area.\n\n\ngit clone YOUR_URL_HERE\n\n\nMake use of Git and version control in your team projects regularly. Like learning anything new, putting it into practice regularly is the best way to become confident in using it.\n\nPlease refer to the other links on the Git learning resources page to learn more about how to use Git in practice.",
    "crumbs": [
      "Statistics production",
      "RAP for Statistics"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html",
    "href": "writing-visualising/dashboards_rshiny.html",
    "title": "Creating R Shiny dashboards",
    "section": "",
    "text": "Guidance for publishing public statistics dashboards with R Shiny",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#learning-r-shiny",
    "href": "writing-visualising/dashboards_rshiny.html#learning-r-shiny",
    "title": "Creating R Shiny dashboards",
    "section": "Learning R Shiny",
    "text": "Learning R Shiny\nShiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage, embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.\nThe guidance on this page assumes you have prior knowledge of R, if you are new to R, then take a look at our R resources page to get started.\nFor some initial inspiration, check out the Shiny gallery.\n\n\n\n\n\n\nTip\n\n\n\nJust because something looks clever, doesn’t mean you should do it. Don’t over-complicate your user interface or the underlying code. Your priorities should be making something that meets user’s needs, whilst also creating something that’s manageable for whoever follows you in maintaining your dashboard.\n\n\n\n\nGetting started with Shiny\n\nThere are a lot of resources already available to support you when working with R Shiny, if you’re new, then this blog post provides a gentle introduction.\nIf you like learning by reading / doing then the Posit Shiny-specific site has lots of resources that you can work through. Likewise Introduction section of Mastering Shiny is also a helpful resources.\nIf you like learning by watching then the Posit Youtube channel is also full of good resources.\n\n\n\nCreating a user interface\n\nOur template Shiny app repository is a useful starting point for all public facing dashboards in the department. Please see the dashboard template section for guidance on what the dashboard template includes and how to use it.\nShiny layouts gives a high level idea of how to structure an R Shiny app. Alongside this it is recommended you take a look at bslib which makes constructing beautiful user interfaces easy. This site is full of information on how to construct the page and how to work themeing.\n\n\n\n\n\n\nWarning\n\n\n\nPreviously a lot of dashboards used shinydashboard - if starting afresh we recommend bslib instead. shinydashboard is uses an old version of bootstrap and has not been updated in many years. It shouldn’t immediately break but a risk nonetheless.\n\n\n\n\n\nR Shiny Best Practice\n\nSome key things you should include when building an app:\n\nYour code should follow the DfE Quality Assurance Framework\nYou need to Accessibility Test regardless of internal or external deployment\nYour code should be tested\nYour file structure should be well-organised\nYou should use dependency management\nIf using databases with Shiny, you should use the pool package. You should also be conscious of SQL Injection which is where code could be maliciously used to attack a database backend - see SQL injection prevention for best practice\nFunctions with Shiny make your life much easier\n\n\n\n\nLeveling up your Shiny\n\nMastering Shiny is a really useful read, especially the Best Practice section.\nThere are a lot of packages out there to use. Always worth checking the features they bring are accessible and that they are from reliable sources i.e. from CRAN / somewhere you trust.\nFor more custom user interface:\n\nhave a read of Outstanding User Interfaces with Shiny for how the web stuff works with Shiny\nfor easy theme customisation Bootstrap Live Customiser\nfor more context of the web bits, HTML, CSS and Bootstrap give a better understanding of what’s under the hood.\n\n\n\n\n\n\n\nCaution\n\n\n\nHTML, CSS and Bootstrap are a slippery slope into web development. Remember that if you’re working with Shiny then good analysis comes first!\n\n\nFor interactive maps, refer to this section on interactive maps.\nFor improving your tables, check out this section on data tables.\nFor more advanced Shiny knowledge it’s worth taking a look at the guide to engineering production-grade Shiny apps. It’s worth reading and considering how much to adopt, remember that just because it sounds clever doesn’t make it easy for other analysts to pick up. Read the Don’t sacrifice readability section and then work from there.\n\n\n\nSupport within the department\n\nYou should seek to make use of the community that is already out there, see what others are doing, ask them questions and for advice on any decisions or problems that you’re facing, and share what it is that you’re doing.\n\nThe Statistics Development Team are experienced with R Shiny and happy to help or offer advice;\nThe DfE R community on Teams is a place where you can post things you’ve learned or to ask for help;\nGoing beyond DfE there’s a wealth of resources and communities online, including Stack Overflow discussions, cross government slack channels (e.g. #R and #Shiny on govdatascience.slack.com) and even tweets about R Shiny on twitter.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#dfe-styling-and-the-dfe-r-shiny-template",
    "href": "writing-visualising/dashboards_rshiny.html#dfe-styling-and-the-dfe-r-shiny-template",
    "title": "Creating R Shiny dashboards",
    "section": "DfE styling and the DfE R Shiny template",
    "text": "DfE styling and the DfE R Shiny template\nOur template shiny app repository should be used a starting point for all public facing dashboards as it gives a consistent set up for publishing and provides template code for common parts. You can start your own repository using the ‘Use this template’ green button on the template repository. You will need a GitHub account to do this. If you are unsure or need help, contact the Statistics Development team who will be able to walk you through this.\n\n\nWhat’s in the template\n\nThe template provides code for a basic interactive dashboard, with an example line chart using drop-downs for education level and geographic levels. It also includes an example accessibility statement (this is a requirement for all public dashboards and must be filled in, see the Accessibility and Accessibility testing sections), and a ‘Support and feedback’ page with suggested contact information and links.\nView the latest version of the template dashboard.\nBy using the official DfE R Shiny template and the dfeshiny package (and keeping pace with updates), teams will be able to create a dashboard that conforms to a standard DfE styling.\nAll dashboards should have a link to the source code, and information about who to contact for feedback/issues. You should be familiar with and follow the gov.uk style guide as appropriate.\n\n\n\nConsistency in R chart styling and colours\n\nWe recommend producing interactive charts using the ggplot2 package to create the basic plots, in combination with the ggiraph package to add any required interactivity. Any ggplot2 chart type can be converted to being interactive using ggiraph by adding _interactive on to the plot type function call as illustrated with geom_point() below:\n\n# Creat an interactive chart that we can output in an R Shiny application\nlibrary(ggplot2)\nlibrary(ggiraph)\n\nchart_iris &lt;- ggplot(iris) +\n  geom_point_interactive(\n    aes(\n      x=Petal.Length, \n      y=Petal.Width, \n      colour=Species,\n      tooltip=paste(Species,'\\nWidth=',Petal.Width,'\\nLength=',Petal.Length)\n      )\n    )\n\nWhich produces the following chart with tooltips appearing when the user hovers over any given data point:\n\n\n\n\n\n\nTo render this within a Shiny application, we would then need to add the following lines to the ui.R and server.R files respectively:\n\n# Code to place ggiraph in ui.R\ngirafeOutput(\"my_ggiraph_chart\")\n\n\n# Code to render ggiraph in server.R\nmy_ggiraph_chart &lt;- renderGirafe(chart_iris)\n\nFor chart colours, teams should use the GSS colour palettes as outlined in the Anlytical Function data visualisation guidance. These are based on the gov.uk design system colours (described in the next section), but are optimised for charts to help with contrast and variations of colour blindness and are the colours used as default in the explore education statistics service.\nCharts using ggplot2 can take custom palettes using the scale_fill_manual() function, e.g. below:\n\n# Create colour palette using recommended colours (based on gov.uk design system)\n\nlibrary(ggplot2)\n\ngss_colours &lt;- c(\n  \"#12436D\", #`blue`\n  \"#F46A25\",#`orange`\n  \"#801650\",#`maroon`\n  \"#28A197\" #`turquoise`\n  )\n\n# Create chart using palette\n\nggplot(\n  mtcars,\n  aes(\n    x=gear,\n    fill = factor(cyl)\n    )\n  ) +\n  labs(x = 'Gears', fill = 'Cylinders') +\n  geom_bar(stat='count') +\n  scale_fill_manual(values = gss_colours) +\n  theme_classic()\n\n\n\n\n\n\n\n\nFurther examples are given on the Using explore education statistics guidance page.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#use-of-colour-outside-of-charts",
    "href": "writing-visualising/dashboards_rshiny.html#use-of-colour-outside-of-charts",
    "title": "Creating R Shiny dashboards",
    "section": "Use of colour outside of charts",
    "text": "Use of colour outside of charts\nIn terms of colour choices beyond charts, teams should pick from the gov.uk design system colours when creating dashboards. Colours for the charts are pulled in using the afcolours package, which is based on the GSS colours guidance. These colours can be reused elsewhere if you need to match colours with those in the charts.\nAs with charts, be careful when choosing colours, and make sure that colours have sufficient contrast to be placed next to each other to meet WCAG AA standards using the colour contrast checker.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#hosting-dashboard-code-on-github",
    "href": "writing-visualising/dashboards_rshiny.html#hosting-dashboard-code-on-github",
    "title": "Creating R Shiny dashboards",
    "section": "Hosting dashboard code on GitHub",
    "text": "Hosting dashboard code on GitHub\n\n\n\n\n\n\nWarning\n\n\n\nAnalysts should not use their own private GitHub accounts for the development of DfE dashboards.\n\n\nAll published dashboards should have their code hosted on the dfe-analytical-services project on GitHub, both for full public transparency and to enable automated approval-based deployments to the DfE publication platform. For the purposes of the internal-only review of dashboards under development, analysts may also benefit from a hosting code on the DfE Azure based DevOps platform. For both of the above, analysts should contact the explore education statistics platforms team.\nGiven the above, all analysts intending to create a dashboard should ensure they have a good working understanding of Git, GitHub and Azure DevOps. We strongly recommend our Tech Skills workshop on Git and Azure DevOps to either get you started or refresh your memory.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to read the guidance carefully, do not commit or push unpublished data to a GitHub repo before the day of the publication of the data. If you think you may have done this by accident, contact explore education statistics platforms team immediately with the full details of what has been uploaded to GitHub and when.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#publishing-your-dashboard",
    "href": "writing-visualising/dashboards_rshiny.html#publishing-your-dashboard",
    "title": "Creating R Shiny dashboards",
    "section": "Publishing your dashboard",
    "text": "Publishing your dashboard\nPublic facing DfE Shiny applications are currently published via the DfE Analytical Services shinyapps.io account, with the authorisation and deployment of dashboards performed using GitHub.\n\n\n\n\n\n\nPublishing new public dashboards\n\n\n\nYou need to alert the explore education statistics platforms team of any new dashboard publication as early in development as possible and keep us updated on the expected publication date so that we can review the dashboard against DfE standards and set up and support on the deployment to ShinyApps.io.\n\n\nIf you are publishing a new dashboard, or adding major updates to an existing one, you must:\n\nObtain authorisation via email from the relevant G6 or DD and the statistics development team\n\nForward authorisation emails to the explore education statistics platforms team\n\n\n\n\n\n\n\nUpdating existing dashboards\n\n\n\nPlease notify the explore education statistics platforms team of any planned data updates or significant functional updates at least 2 weeks before publication but preferably as soon as you know any updates are required.\n\n\nThe majority of dashboards made to support and augment our Official Statistics will be public facing. For public facing shiny apps you should publish via shinyapps.io. The explore education statistics platforms team manage a subscription for this and can help you get set up.\nYou will need:\n\nA finished app that meets the accessibility and styling standards (see our Dashboard procedure checklist)\nCode in the dfe-analytical-services GitHub repo\nApproval from your DD\nIf the data underlying the dashboard is currently unpublished, you will need to create dummy data to use in GitHub until the data becomes published (see dummy data guidance section).\n\nTo set up a new app, send the above to explore education statistics platforms team. If your code is not yet hosted in the dfe-analytical-services GitHub area you can request for the repository to be moved at the same time as sending approvals.\n\n\nDashboard procedure checklist\n\nThis checklist outlines the standard procedure for teams who are wishing to produce a public R Shiny dashboard.\nGetting set up:\n\nCreate an account on GitHub\nAsk the explore education statistics platforms team to create you a repository in the DfE analytical services area, providing the name of the dashboard and the GitHub accounts of anyone who will be contributing to the code. You should aim to have two analysts working on the code development and a line manager for review purposes. Further colleagues with review responsibilities (policy colleagues, G6 and above, etc.) can be given access to a demo-site, rather than the repository.\nClone the repo to your device so you can develop your code. Open the repo page in GitHub, click the green ‘Code’ button, and copy the URL it provides. Then, open R Studio on your device, click file &gt; new project &gt; version control &gt; Git, paste the repository URL you copied from GitHub, give your local project area a name, and choose where to save it (i.e. on your computer’s C: drive, outside of the OneDrive-synced folders).\n\nOnce you’re set up, there are certain parts of the code you need to update:\n\nIn the global.R script, update all of the site URL’s and EES publication names to your own.\nIn the ui.R script, update the tags$title(), the information in the meta_general(), and the secondary text in shinyGovstyle::header().\nGot to the .github &gt; workflows folder, and open the deploy-shiny.yaml file. At the bottom, update the appName in rsconnect::deployApp() - this is what will appear in the URLs (i.e. must align with the links in global).\nUpdate the ‘Support and feedback’ tab (in R &gt; standard_panels.R) with your teams information. We also recommend creating a feedback form for users of your dashboard, and adding a link to that on this page.\nUpdate the README.md file (you can do this easily directly in GitHub). This is the file that renders below your repo in GitHub for users to see.\nBegin adding your own dashboard content. If you copy and paste any parts of the code (i.e. to create new tabs) you must change all of the IDs so there are no repeated IDs in the UI, otherwise the app will run with no UI elements. You should add UI and unit tests as you develop your code as a form of automated QA (see our guidance on UI tests and guidance on unit tests).\n\nYou must contact the Statistics Development Team for the following:\n\nTo add the shinyapps.io secret and token to your GitHub repo, therefore enabling the app to be hosted on shinyapps.io.\nTo create an area for your team in Google Analytics, to track the user analytics of your dashboard.\n\nSetting up a development / demo dashboard area:\n\nWhile developing your dashboard, you may want a private, demo-version to share with policy or senior colleagues for review and feedback. This version must use either published data or dummy data and can not use unpublished data, since this cannot be uploaded to GitHub until the day of publication (see our dummy data guidance for public dashboards).\nEnsure that prior to contacting the explore education statistics platforms team, you have updated all of the URL’s and other items listed above.\nYou must contact the explore education statistics platforms team to add the shinyapps.io secret and token to your GitHub repo, therefore enabling the app to be hosted on shinyapps.io. Once this is done you will have a browser link you can use to access the dashboard. We can make this private such that there is a list of approved viewers who must log in to view the dashboard - please provide the email addresses of any colleagues who you wish to have access to the private version during development.\n\nYou must have done the following before a dashboard can be published (the explore education statistics platforms team must review and approve that these have been met):\n\nAccessibility testing the dashboard and updating the accessibility statement. The accessibility testing guidance later in this section outlines how teams can do this.\nYou should test how your dashboard appears and performs on mobile devices. you can do this by opening your dashboard in chrome/edge, right clicking anywhere on the page and selecting ‘inspect’. you will then see a new panel on the right hand side of your screen. To see your dashboard how a mobile user would, click the mobile/tablet button (see the image below).\n\n\n\nSetting up UI and unit tests (UI tests are a part of the automated QA process and will run via GitHub actions each time you create a pull request). See our guidance on UI tests and guidance on unit tests.\nPerformance testing your dashboard. See our guidance on performance testing.\nThe underlying data for the dashboard must still be uploaded to EES in the tidy data format to be used in the table tool (check this using our data screener)\nDecide where you are going to provide the public the link to your dashboard. You can easily add a link to your EES publications. If you have a draft release that is going out on the same day as the dashboard, you can add the link into your EES draft while the shinyapps.io page is still private. This is because the link will not change when the dashboard is made public, the log-in screen will simply be removed.\n\n\n\n\nProcedure for updating your R Shiny dashboard with new data, new functionality or both\n\nAs mentioned in the public dashboards section, a public dashboard should not be updated with unpublished data until the data is published. However, it is possible to clone the repo and run it locally with unpublished data for testing purposes (see our guidance on testing with unpublished data). This guidance applies to both adding real data to a dashboard that previously used dummy data, and to adding updated data to an existing dashboard.\nNote that wherever possible dashboard development updates should be done separately from routine data updates.\n\n\n3-4 weeks before publication / start of update work\n\n\nContact the explore education statistics platforms team providing the following:\n\nbrief outline of the planned changes\na planned publication date and time for the dashboard\nthe contact details of the lead developer\nthe contact details of the lead reviewer\nthe contact details of the approver (G6)\nask us about a review & health check on your dashboard to make sure it’s up to date with standards and is in a state to successfully deploy\n\nRun an update of all R-packages used by the dashboard - renv::update() - to check that everything works as expected\n\nOnce verified, merge this update into your main branch to update all packages on your live dashboard (this will help the publication deploy go quicker)\n\n\n\n\n\nAdding functionality / styling code updates\n\n\nCreate individual branches for specific improvements\nCreate / update UI tests for any new / changed functionality / behaviour\nMerge these into a development branch or into your main branch as each work item is completed\n\n\n\n\nAdding new data - at least 1 day prior to publication day\n\n\nCreate a new update branch on GitHub (branching from the most up to date version of your dashboard - i.e. either the main branch or the development branch if you’ve been making new updates on a development branch) where you intend to place your data updates\nCreate a Pull Request between the data update branch and main (or development if you wish to test on development first)\nPull the new update branch into your local clone of the repo\nAdd any new data and make the appropriate add / commits. Once the data has been added, don’t perform any pushes from this branch until publication time.\nRun the local tests and record new UI test results caused by the new data\n\n\n\n\nOn the day of publication:\n\n\nPush the new update branch created above (can be performed 30 mins prior to publication time)\nVerify all tests / checks pass on the pull request - fix if they don’t\nGet approval on GitHub Pull Request from appropriate reviewer (e.g. G7)\nComplete the pull request to trigger the deploy\n\nNote that completing a pull request into the main branch will be blocked until your PR is both approved and passes the automated tests\nDeploys usually take between 5-30mins, check the GitHub Actions logs for progress\n\nTest the functionality and data validity of the app once it’s deployed to the live site.\n\n\n\n\n\nDetailed walkthrough of a data update\n\nThe following provides a more detailed description of how you would do the above steps technically in R-Studio and GitHub if you need a refresher.\n\n\n(At least 1 day) Prior to publication\n\n\nCreate a local branch which you will use to update the data. To do this open your local version of the repo, navigate to the Git window, ensure the current branch is main (see 1) as this is the code producing the current live version of the app. If you’re happy the current branch is main, then do a pull to ensure it is up-to-date (see 2). Then click the new branch button (see 3).\n\n\n\nGive your branch a descriptive name, for example, for a data update for publication on a given date, incorporate the date into the branch name (i.e. data_update_01Jan_2023). Click create.\nYou can now add your unpublished data files to the data folder in your local copy of the repository (in the file explorer). If you have already added the data using .gitignore as described in the guidance on testing with unpublished data, you can now remove the file name from the .gitignore file. You can also remove any dummy data files or old data files from the folder.\nNow open the script that reads in data for your dashboard and edit the file paths to point at the new data files.\nrun tidy_code() and run_tests_locally() (see guidance on UI tests). If changes are found when running the tests locally, make sure you look through these differences and understand them. If you are happy with the changes found in the tests (i.e. they are expected due to your updates), update the .json files. These are tested via GitHub actions every time you do a pull request, so you should always run them in advance of any pull request.\nYou can run your app to test that it is working, but do not commit or push this branch yet.\n\n\n\n\nDay of publication\n\n\nAt 9:30 on publication day, commit and push the changes in your new branch to GitHub. This makes the data files publicly available via GitHub and so should not be done until 9:30 on the day of publication, however this step does not update the dashboard.\nOn the GitHub repo, you should now see a prompt to open a pull request. You should do this, following the template, ticking the boxes to show you have completed tests locally as required and provide details of the changes (i.e. that you’ve updated to include the most recent data, including dates).\nAs you have already completed tests locally (step 5), you do not have to wait for the tests to complete when you open the pull request, you can click to merge into main straight away. This should start the process of updating the data on the dashboard. You can view the progress and time taken to do this by opening the GitHub actions tab and looking at the shiny-deploy action. Once the shiny-deploy action is complete, the dashboard will have updated.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#dashboards-and-data",
    "href": "writing-visualising/dashboards_rshiny.html#dashboards-and-data",
    "title": "Creating R Shiny dashboards",
    "section": "Dashboards and data",
    "text": "Dashboards and data\nWe don’t yet have a database server set up that can be accessed by public facing dashboards, though we are working to put this in place. In the meantime, there are a few alternative options for storing data that the dashboard can run off.\n\nStore the data within the repo (e.g. CSV files in a /data folder). Note that only published data should be stored in a repo. If your working on a dashboard that uses unpublished data then please see the dummy data section for guidance.\nUse Google sheets\nUse Dropbox\n\nIf you are running an internal-only app then you can connect to our internal SQL servers, information on how to do this is in the R community teams area.\n\n\nStoring data in the dashboard repository\n\nWhilst not ideal, storing data in the dashboard repository is the current standard route to accessing data in DfE dashboards. As our systems mature, we will move away from this model and develop a more secure and optimal data storage and access process for dashboards.\nNote that the maximum allowable file size for GitHub repositories is 50 MB. Therefore if you are planning on using data in files larger than this limit, you will need to either break the data into smaller files or compress your data files before adding them to your repository. These steps should be taken before performing any commits incorporating the data.\n\n\nPreventing early release of unpublished data\n\nGive our commitment to transparency and this using a public service for code hosting, storing data within your dashboard repository must be done with care. Anything that you push to your GitHub repository will immediately be sent to the GitHub servers based in the US and thus will be stored outside DfE infrastructure. Therefore you should only ever push already published data to your repository.\nTo help with proper recording of data files and prevent accidental uploads, we have implemented a set of commit hooks in the dashboard repositories that helps you track any data files you are using (and will prevent the upload of any untracked data files).\nThe datafiles_log.csv file is a record of all of the data files saved in the repo, and whether or not the data is published. You should log all data files here and accurately record whether the data is published, unpublished or reference data. Any data file that is present in your repository, but is not listed as being published, dummy or reference data will prevent you being able to complete commits.\nIf you see any of the following errors, it is because you are trying to commit a data file to the GitHub repo that hasn’t been recorded as published:\nError: data/test_data.csv is not recorded in datafiles_log.csv.\nError: data/test_data.csv is not logged as published or reference data in datafiles_log.csv and is not found in .gitignore.\nCommit hooks run automatically whenever you try to commit changes, and prevent any unpublished data being uploaded to the GitHub repo (see the public dashboards and dummy data sections) by checking the datafiles_log.csv file.\nIf you would like to save an unpublished data file locally to test the dashboard you should use the .gitignore file to ensure Git ignores this file in your commits (see the testing with unpublished data guidance) and then list this file as unpublished in your datafiles_log.csv file.\n\n\n\n\nUsing dummy data\n\nWhen creating a public dashboard, all code and data will be hosted in the dfe-analytical-services GitHub repo. This is a public repo, meaning anything stored in here is publicly available. Therefore if you are creating a dashboard with previously unpublished data, you should provide dummy data for the GitHub repo, and only add the real data on your publication date.\nYour dummy data should:\n\nUse the exact same structure and variable names as your real data\nUse random values in place of real values (one example of how to do this is using the rnorm() function)\nSimulate the types of suppression, missing data or anomalies seen in your real data, to ensure the dashboard can account for these.\n\n\n\n\nTesting with unpublished data\n\nWhile you must use dummy data in your GitHub repo, it is understandable that we should test the dashboard works with the real data before it goes live.\nThis can be done using the .gitignore file alongside the datafiles_log.csv and commit hooks explained in the Stopping accidental data uploads guidance. You can view the example files our template repository.\nThe .gitignore file is a plain text file that tells Git to ignore specified files in commits and pushes to the repo. Therefore, the first step when wanting to test unpublished data in a dashboard is to add the unpublished data file names to the .gitignore file.\nAdding the file name alone will ensure it is ignored no matter where in the project area you save it (i.e. in any folder). Once this is done, you can add your unpublished data file to your local area, run the app locally, and make edits/commits without uploading the data to Github.\nThis .GitIgnore guidance page has great guidance on how you can utilize wildcards to capture all the files you might want to ignore.\nIf you have any questions on this process, please contact the explore education statistics platforms team.\n\n\n\nSecure variables\n\nSee our Git page for guidance on storing secure variables in repositories.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#internal-only-dashboards",
    "href": "writing-visualising/dashboards_rshiny.html#internal-only-dashboards",
    "title": "Creating R Shiny dashboards",
    "section": "Internal only dashboards",
    "text": "Internal only dashboards\nIt’s possible to publish dashboards that are only accessible to users within the DfE network. To do this you will need to publish via the department’s internal Posit Connect servers.\nYou will need:\n\nA finished app, in line with the guidance on this page\nThe code to be in a Git repository in the dfe-gov-uk Azure DevOps space\n\nTo publish the app, you’ll need to set up a pipeline in Azure DevOps, guidance for how to do this can be found in the Posit Connect Guidance (internal DfE access only).\nAccess to applications on the Posit Connect servers is locked down by default, so once the pipeline is set up and you’ve deployed the app you’ll need to request for its access to be opened up by using an Posit Connect request on ServiceNow.\nIf you’re running the app from an internal database, you’ll need to contact the database owner to set up a local login, and then store those as variables against your specific app in rsconnect. You can raise a request to do this via ServiceNow and selecting ‘Change app variables’.\n\n\nDatabases with Shiny\n\nCurrently only internal dashboards can have connectivity to databases. Check out this data article from Posit on best practice - in particular you should look to use the pool package for database connections and be careful of SQL Injection.\n\n\n\nWhen to load data\n\nOn Posit Connect (internal dashboards only), a process is started up when a user opens your application. The process loads everything in your global environment (pre-server). These environment variables are shared between users with up to 20 users sharing a process by default. This can massively save on computation time.\nHowever, it might be more efficient for some of your data loading to be moved onto the server side. For example, let’s say we have a dashboard showing school-level information. Collecting into memory data for every school is inefficient if a user is only looking for one. In this instance, we might set up a lazy connection to a database and only load the data when the user requests it on the server side.\nIn the example below, we demonstrate two examples. unlazy_data is all being used so we load it into the environment to be shared between multiple users. lazy_school_table is being used by one school at a time so is probably more efficient to load in only one school than all schools.\n\n# NOT RUN\nif(FALSE){\n  # create a pool connection\n  connection &lt;- pool::dbPool(...)\n  \n  # unlazy data collected into memory BEFORE server\n  unlazy_data &lt;- dplyr::tbl(connection, DBI::Id(...)) |&gt;\n    collect()\n  \n  # create a lazy connection to a database\n  lazy_school_table &lt;- dplyr::tbl(connection, DBI::Id(...))\n  \n  # tell shiny to close the connection when the app stops\n  shiny::onStop(function() poolClose(connection))\n  \n  # an example ui to select a school\n  ui &lt;- shiny::fluidPage(\n    shiny::selectInput(\n      inputId = \"school\"\n    ),\n    shiny::tableOutput(\"all_schools\"),\n    shiny::tableOutput(\"one_school\")\n  )\n  \n  # data collected into memory ON SERVER SIDE\n  server &lt;- function(input, output, session){\n    school_level_data &lt;- reactive({\n      lazy_school_table |&gt;\n        filter(school_name == !!input$school) |&gt;\n        collect()\n    })\n    \n    # table showing all schools\n    output$all_schools &lt;- renderTable({\n      unlazy_data\n    })\n    \n    # table showing filtered school\n    output$one_school &lt;- renderTable({\n      school_level_data()\n    })\n  }\n}",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#using-google-analytics-with-r-shiny",
    "href": "writing-visualising/dashboards_rshiny.html#using-google-analytics-with-r-shiny",
    "title": "Creating R Shiny dashboards",
    "section": "Using Google Analytics with R Shiny",
    "text": "Using Google Analytics with R Shiny\nIf you’re planning to publish a dashboard, or to set up Google Analytics for a published dashboard, please contact the explore education statistics platforms team.\nThe template has a google_analytics.html file which is set up to track all basic metrics, plus the inputs from both drop-downs. To set this up with your own app you will need a new tracking tag (the long number in the gtag()) which the Statistics Development Team can provide you with. Please contact us in order to set this up.\nIf you are hosting your dashboard at multiple URLs (i.e. to cope with expected high traffic) then all URLs will be tracked using one tracking tag and analytics for all URLs will all appear as one in the same report.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#accessibility-testing-in-r-shiny",
    "href": "writing-visualising/dashboards_rshiny.html#accessibility-testing-in-r-shiny",
    "title": "Creating R Shiny dashboards",
    "section": "Accessibility testing in R Shiny",
    "text": "Accessibility testing in R Shiny\nTo complement the general accessibility guidance provided on the dashboards page, we recommend also using the shinya11y package to look at accessibility across your dashboard. Simply install and load the package, then include use_tota11y() at the top of your UI script. This brings up an interface that helps you examine different accessibility aspects in your app, like colour contrast, alt-text and what screen readers will detect:\n\nR Shiny dashboards should also be tested using Lighthouse and via manual checks as outlined on the general dashboards page.\nOne example of a dashboard for Official statistics that meets these regulations is the SCAP LA school places scorecards app. Their accessibility statement is clearly labelled, explains what checks have been carried out, what the known limitations are and the plans in place to fix them:",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#code-testing",
    "href": "writing-visualising/dashboards_rshiny.html#code-testing",
    "title": "Creating R Shiny dashboards",
    "section": "Code testing",
    "text": "Code testing\nFor dashboards created in R Shiny, we strongly recommend a mix of unit tests and UI tests (integration), run using GitHub Actions for continuous integration (CI). All applications should have an appropriate level of test coverage before being published. More information and details on how to get started with this can be found in the testing R Shiny section below.\nTo ensure that they are reliable, dashboards should have an appropriate amount of automated tests before they can be published. We recommend using a mix of UI and unit tests, but the number and type of tests that you need to run will depend on the content of your application.\nFurther guidance on setting up testing can be found in the DfE good code practice guide. Also see our guidance on testing R code on the learning resources page.\n\n\nUI Tests\n\nUser interface (UI) tests should be used for all apps. These tests should check that:\n\nYour app loads up correctly\nContent appears as expected when you switch between tabs\nContent updates as expected when you change inputs\n\n\n\nUsing shinytest2 for UI tests\n\nThe shinytest2 package (which supersedes shinytest) is a really easy way to create these tests on an R Shiny app. Simply load in the package and run shinytest2::record_test() to open up the testing environment to get started.\nEach test should produce one “expected results” JSON file tracking the inputs and outputs used by your dashboard. You can check the naming of particular input and output elements within your app by holding CTRL and clicking on an element in the testing window.\nAn example of the use of shinytest2 on a DfE R Shiny app can be found in the KS4 Ready Reckoners tool. This has a variety of tests that check that the app functions as expected when different combinations of filters are selected.\nFor optimal readability, each test should be separated out by comments, clearly stating what is being tested. The testing uses the following functions to set inputs and record test results:\n\napp$set_inputs() tells the UI test to select an input in the application\napp$expect_values() tells the UI test which outputs to capture (i.e. which outputs you’d expect to change and want to test)\n\n\nThe expected outputs are saved in a _snaps/shinytest2 folder, and every time you run a UI test, the outputs will be compared to these expected snapshots.\nAn example of a basic shiny test to check that your app loads up correctly can be found in our shiny template repo.\n\n\n\nMigrating from shinytest to shinytest2\n\n\nPreparing for migration\n\nAs previously mentioned, the shinytest package has been superseded by the shinytest2 package. If you already have shinytest UI tests in place for your dashboard, there is a function available to migrate these tests for you so that you do not have to rewrite them all yourself.\nThe migration function will only work if specific line endings (also known as carriage returns) are used in your code. It will not be immediately obvious to you what kind of line endings your code uses as default. If you receive an error that looks like this, or any error that seems to be pointing at the blank space at the end of a line of code, then you may be having this issue:\n\nError in parse(text = test_text) : &lt;text&gt;:21:1: unexpected input\n20: app$snapshot()\n21: \n    ^\n\nYou can modify the type of line endings used in your test code by going to Tools &gt; Global Options &gt; Code inside RStudio. In the “Saving” tab, there is a drop down box called “Line Ending Conversion”. You need to change this to “Posix (LF)” as in the image below:\n\nYou will need to modify and save your pre-existing test script after changing your settings to get this to work (adding and deleting a letter or a space somewhere should do). Once you’ve saved your test script, go to Session &gt; Restart R, and then re-run the shinytest2::migrate_from_shinytest(PATH_TO_APP) command. The migration should then be successful.\nThere is guidance on migrating to shinytest2 available. To complete the migration, run shinytest2::migrate_from_shinytest(PATH_TO_APP), making sure to include the full file path to where your project is saved, e.g. shinytest2::migrate_from_shinytest('C:/R projects/analysts-guide'). There are two ways to find the file path to your project:\n\nUse the getwd() function\nFind the file path in Windows Explorer. If you copy your file path from Windows Explorer, remember to change the \\ to / otherwise you’ll encounter an error.\n\n\n\n\nCompleting migration\n\nThe migrate_from_shinytest function will search for your “tests” folder, and then look inside the folder for any scripts containing UI tests. A new testthat folder will be automatically created, containing the code that runs your tests, as well as the snapshots generated when your tests are run. Don’t panic if your old test script is deleted as part of the migration, this is supposed to happen! It is recommended that someone who was familiar with the original tests completes QA of the new tests to make sure that everything looks as expected.\n\n\n\nPotential Issues\n\n\nChromote\n\nShinytest2 needs to connect to your local copy of either MS Edge or Google Chrome by running a chromote session. The following issues can occur when shinytest2 tries to connect to run a Chromote session:\n\nR can’t find Edge or Chrome / invalid path to chromote\nFailed to launch Chrome / Edge due to trying to run in old headless mode\n\n\n\n\n\n\nUnit Tests\n\nUnit tests should be used for apps that contain custom functions created from scratch, or that rely on existing functions being combined to interact in a certain way.\nUnit testing checks, for a set of defined variables, that the output is always the same when you run your custom function/s. This ensures that if slight tweaks are made to your function, or functions within packages you are using are updated, that your custom function still works.\nFor example, our QA app has many custom functions that we create to screen files before upload to EES. We have a series of dummy files that we have created to either pass or fail certain tests. Unit tests are written to check that these files always pass or fail as expected.\n\n\n\nTests and deployment\n\nBoth UI and unit tests need to be added to your app’s deployment pipeline. This ensures that the app will not be published if any of the tests fail.\nWe recommend using Github Actions to deploy the latest code whenever a push to your master branch occurs - this ensures that the published version of your app stays up to date.\nYou should also use GitHub Actions to run the automated tests for your app, which we recommend are done on at least every pull request.\nIf you’ve started from our template repository then all of this will be mostly set up for you to tweak and expand on, but if you haven’t then you’ll need to add the yaml files from the .github/workflows folder to your repository.\nGitHub actions are already well documented and their own guidance should be able to walk you through anything you need. That being said, if there’s anything else you’d like to see here let us know.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/dashboards_rshiny.html#dashboard-performance",
    "href": "writing-visualising/dashboards_rshiny.html#dashboard-performance",
    "title": "Creating R Shiny dashboards",
    "section": "Dashboard performance",
    "text": "Dashboard performance\nPerformance profiling represents a chance to breakdown how different elements within your dashboard perform relative to each other. This consists of running through a set of actions on a local run of your dashboard and timing each one. A useful tool for performing these tests is profvis, which carries out all the timings and visualisation of the results for you, whilst all you have to do is run the app and step through the actions that you want to profile.\nThe basic steps are as follows.\nWith your dashboard repository open in RStudio, run the following commands from the R console\ninstall.packages(\"profvis\")\nprofvis(shiny::runApp())\nThen go through a set of interactions in your dashboard (e.g. navigate to the dashboard page, step through changing the selected choices in each input field, switch between any different tabs and cycle through the relevant inputs again). Once you’ve performed a representative range of actions in the dashboard, close the window and go back to the RStudio window.\nAfter several seconds (depending on the range of actions you performed), a visualisation will appear in the main RStudio editor window showing the results of the profiling.\nThe profvis results are shown in the flame profiling chart. The top half of the chart shows any processing time measurements above 10ms for each line of code. It can be useful to focus in on lines that have measured times above 10ms by selecting “Hide lines of code with zero time” from the options menu in the top right.\n\nLines with timings of greater than about 5-100ms may warrant further investigation to see if they warrant some optimization. Common routes to optimising code are:\n\navoid any data processing in server.R, e.g. avoid using the summarise() function;\nuse reactive() elements to minimise repeated calls of given functions;\nuse memoise() to cache results of more intensive functions (e.g. ggplot) to minimise repeated calls.\n\nThe documentation for the profvis package can be found here: Profvis documentation\nWhilst profvis can help identify any bottlenecks, this will ideally be complemented in the future by full load testing of dashboards, whereby the behaviour of dashboards under real-life high demand scenarios can be tested. However, this type of testing is unavailable whilst our applications are hosted on ShinyApps. We will offer support on load testing once we move dashboards on to our own server systems.",
    "crumbs": [
      "Writing and visualising",
      "Creating R Shiny dashboards"
    ]
  },
  {
    "objectID": "writing-visualising/visualising.html",
    "href": "writing-visualising/visualising.html",
    "title": "Visualising data",
    "section": "",
    "text": "Things to consider when visualising data",
    "crumbs": [
      "Writing and visualising",
      "Visualising data"
    ]
  },
  {
    "objectID": "writing-visualising/visualising.html#introduction",
    "href": "writing-visualising/visualising.html#introduction",
    "title": "Visualising data",
    "section": "Introduction",
    "text": "Introduction\nAny analyst building charts should make themselves familiar with the Analysis Function data visualisation guidance.\n\n\nActive titles\n\nFor tables and charts, active titles are descriptive and tell the trend by highlighting the main story. They should be short, aim for 10 words or less and avoid going over more than one line. Active titles give users the main message without having to find the text that accompanies the chart, and makes it easier for journalists to use your chart directly without having to write their own summary.\nPut information such as the measure, source, population, geographical coverage and time period in a caption if they are not obvious from the chart content. Add further context and information in the main text. Remember that tables and charts should be usable even if isolated from the rest of the release.\n\n\n\nCharts\n\nThe majority of charts should be line or bar charts and be kept simple, and you should use the Analysis Function guidance on choosing charts to help guide your decision. You should only use complex charts where there is a clear user need, as simple charts are the easiest for users to understand\nDWP have created a Data Visualisation Thinking course that may be useful to look at when creating more complex charts.\n\n\n\nColours\n\nThe most important consideration when using colour is to avoid relying on it for interpretation. It should be seen as an enhancement, and your charts should be understandable without it.\nWhere not constraint by other style guides, you should use the suggested colours from the GSS colours in visualisations guidance. The following sections show the colours that are recommended for charts and suggest the way to use them with one another, more detail on why can be found in the Analysis Function guidance itself.\nWe strongly recommend you stick to the codes as outlined, if you can’t for any reason, please make sure you follow the guidance on developing your own colour palette to ensure that you are considering whether the colours are accessible to all.\n\n\nCategorical colour palette\n\nCategorical data can be divided into groups or categories by using names or labels. This palette has four colours. We recommend a limit of four categories as best practice for basic data visualisations. The ordering of the palette is important as not all colours are accessible when paired together.\n\n\n\nColour Name\nHex Code\nRGB\nCYMK\nColour Example\n\n\n\n\nDark blue\n#12436D\n18, 67, 109\n36, 16, 0, 57\n\n\n\nTurquoise\n#28A197\n40, 161, 151\n75, 0, 6, 37\n\n\n\nDark Pink\n#801650\n128, 22, 80\n0, 83, 38, 50\n\n\n\nOrange\n#F46A25\n244, 106, 37\n0, 57, 85, 4\n\n\n\n\n\n\n\n\n\nSequential colour palette\n\nSequential data is any sort of data where the order of series has some meaning. For example, age groups ascending in age. This palette has three colours and even this pushes the boundaries for contrast ratios. Any charts made with shades of a specific hue should be accessible without colour.\n\n\n\nColour Name\nHex Code\nRGB\nCYMK\nColour Example\n\n\n\n\nDark blue\n#12436D\n18, 67, 109\n36, 16, 0, 57\n\n\n\nMid Blue\n#2073BC\n32, 115, 188\n0, 57, 85, 4\n\n\n\nLight Blue\n#6BACE6\n107, 172, 230\n75, 0, 6, 37\n\n\n\n\n\n\n\n\n\nFocus palette\n\nOn focus charts, colour is used to highlight specific elements to help users understand the information. One line out of many will be highlighted as a colour and the rest of the series remain grey.\n\n\n\nColour Name\nHex Code\nRGB\nCYMK\nColour Example\n\n\n\n\nDark blue\n#12436D\n18, 67, 109\n36, 16, 0, 57\n\n\n\nGrey\n#BFBFBF\n191, 191, 191\n0, 0, 0, 25\n\n\n\n\n\n\n\n\n\nLarger palettes\n\nAll charts should be made as simple as possible so that the message is easy to understand and interpret.\nIf you do want to use additional colours you need to ensure that the chart is understandable without colour as no series with more than four colours will ever be fully accessible alone.\nFor categorical data, we recommend the following extensions to the categorical palette. Once again, the ordering is important. If you’re making a chart in EES these are not default colours, so you will need to use the colour picker to specify them.\n\n\n\nColour Name\nHex Code\nRGB\nCYMK\nColour Example\n\n\n\n\nDark blue\n#12436D\n18, 67, 109\n36, 16, 0, 57\n\n\n\nTurquoise\n#28A197\n40, 161, 151\n75, 0, 6, 37\n\n\n\nDark Pink\n#801650\n128, 22, 80\n0, 83, 38, 50\n\n\n\nOrange\n#F46A25\n244, 106, 37\n0, 57, 85, 4\n\n\n\nDark Grey\n#3D3D3D\n61, 61, 61\n0, 0, 0, 76\n\n\n\nLight Purple\n#A285D1\n162, 133, 209\n22, 36, 0, 18\n\n\n\n\n\n\nFor larger palettes when using sequential data, we recommend using ColorBrewer to find the hex codes for multiple shades of a given hue.",
    "crumbs": [
      "Writing and visualising",
      "Visualising data"
    ]
  },
  {
    "objectID": "statistics-production/api-data-standards.html",
    "href": "statistics-production/api-data-standards.html",
    "title": "Statistics API Data Standards",
    "section": "",
    "text": "Guidance on how to structure data files specifically for the EES API",
    "crumbs": [
      "Statistics production",
      "Statistics API Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/api-data-standards.html#introduction",
    "href": "statistics-production/api-data-standards.html#introduction",
    "title": "Statistics API Data Standards",
    "section": "Introduction",
    "text": "Introduction\nThe API offers analysts, both internal to the DfE and external consumers and communicators of education statistics, a way to programmatically access data on EES. However, in order to ensure a fit for purpose service, not all EES data will be accessible via the API, and any that is will need to pass a higher bar for quality. In effect API data must meet all the criteria laid out in our Open data standards guidance.\nWhilst the EES data screener tests for a significant base level of data quality and consistency, there are some additional criteria that are either too awkward to test for rigorously using the screener or are tested for but returned as warnings. Data intended for the EES API must pass all the base level screener tests, plus a number that only return warnings, plus manual inspection by the platform gatekeepers. These are primarily:\n\nStrict tidy data structures - i.e. appropriate use of filters and indicators.\nStandardised filter col_names and items consistent with the harmonised standards.\nStandardised indicator col_names meeting the naming standards.\nCharacter limits for col_names and filter items.\n\nExamples of these that do and don’t meet the API data standards are provided in the following sections.",
    "crumbs": [
      "Statistics production",
      "Statistics API Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/api-data-standards.html#character-limits-for-col_names-and-filter-items",
    "href": "statistics-production/api-data-standards.html#character-limits-for-col_names-and-filter-items",
    "title": "Statistics API Data Standards",
    "section": "Character limits for col_names and filter items",
    "text": "Character limits for col_names and filter items\nCharacter limits for fields in data uploaded to the API are:\n\n\nCharacter limits on column names, column labels and filter items.\n\n\nElement\nCharacter limit\n\n\n\n\nLocation codes\n30 characters\n\n\nFilter / indicator column names\n50 characters\n\n\nFilter / indicator column labels\n80 characters\n\n\nFilter items / location names\n120 characters",
    "crumbs": [
      "Statistics production",
      "Statistics API Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/api-data-standards.html#tidy-data-structure",
    "href": "statistics-production/api-data-standards.html#tidy-data-structure",
    "title": "Statistics API Data Standards",
    "section": "Tidy data structure",
    "text": "Tidy data structure\nThe key thing on tidy data structure is to avoid filter items being included within indicator col_names. Where you have collections of related terms appearing in indicator names (e.g. male, female, total), then these should be translated into a filter column, with the data being pivoted.\nAll data uploaded to EES should be in a tidy data structure form, but this is more strictly regulated for data intended for use with the API. More information on building tidy data structures can be found in the tidy data structure section.\nThe following give examples of how different examples of data structures could be adapted.\n\nExample 1 - Three metrics with a single filter\n\nExample of bad practice\n\n\nPupil counts and percentages in non-tidy format\n\n\n\n\n\n\n\n\n\n\n\nschool_count\npupil_count_male\npupil_count_female\npupil_count_total\npupil_percent_male\npupil_percent_female\npupil_percent_total\n\n\n\n\n2\n120\n130\n250\n48\n52\n100\n\n\n\n\n\n\nExample of good practice\n\n\nPupil counts and percentages in tidy format\n\n\nsex\nschool_count\npupil_count\npupil_percent\n\n\n\n\nMale\n2\n30\n60\n\n\nFemale\n2\n40\n80\n\n\nTotal\n2\n50\n100\n\n\n\n\n\n\n\nExample 2 - Metrics with hierarchical filters\n\nExample of bad practice\nThe following would not be accepted for publication via the API.\n\n\nAttendance statistics in non-tidy format\n\n\n\n\n\n\n\n\n\n\n\n\nattendance\noverall_absence\nauthorised_absence\nunauthorised_absence\nattendance_percent\noverall_absence_percent\nauthorised_absence_percent\nunauthorised_absence_percent\n\n\n\n\n180\n20\n12\n8\n90\n10\n6\n4\n\n\n\n\n\n\nExample of good practice\nThe following would be accepted for publication via the API. In this case, creating a hierarchical filter combination allows a clear representation of the data.\n\n\nAttendance statistics in tidy format with hierarchical filters\n\n\n\n\n\n\n\n\nattendance_status\nattendance_type\nsession_count\nsession_percent\n\n\n\n\nAttendance\nTotal\n180\n90\n\n\nAbsence\nTotal\n20\n10\n\n\nAbsence\nAuthorised absence\n12\n6\n\n\nAbsence\nUnauthorised absence\n8\n4\n\n\n\n\n\n\n\nExample 3 - Metrics with non-compatible filters\n\nExample of bad practice\nIn the example below, the different metrics contain different types of values that are split by very different filters. Specifically pupil counts and pupil percents are split into grade thresholds, whereas the score based metrics are not.\nThe following would not be accepted for publication via the API.\n\n\nAttainment grade rates and scores in non-tidy format\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npupil_count_grade9to5\npupil_count_grade9to4\npupil_count_grade9to1\npupil_percent_grade9to5\npupil_percent_grade9to4\npupil_percent_grade9to1\nprogress8_score_male\nprogress8_score_female\nprogress8_score\nattainment8_score_male\nattainment8_score_female\nattainment8_score\n\n\n\n\n30\n40\n50\n60\n80\n100\n0.2\n0.21\n0.21\n0.09\n0.08\n0.10\n\n\n\n\n\n\nExample of pivoting leading to excessive duplications and not applicable characters\nIf we were to try and pivot the above data as one file, it would lead to an unreasonably large number of cells with no valid entries (i.e. large numbers of z’s). For example, pivoting might create something like the following table, which suffers from both a large number of not applicable columns and duplication of data unnecessarily.\n\n\nExample of pivoted data showing excessive duplicated and not applicable fields.\n\n\n\n\n\n\n\n\n\n\nsex\ngrade_range\naccountability_measure\npupil_count\npupil_percent\nscore_average\n\n\n\n\nTotal\nGrades 9-5\nz\n30\n60\nz\n\n\nTotal\nGrades 9-4\nz\n40\n80\nz\n\n\nTotal\nGrades 9-1\nz\n50\n100\nz\n\n\nTotal\nz\nAttainment 8\n50\n100\n0.21\n\n\nFemale\nz\nAttainment 8\n50\n100\n0.21\n\n\nMale\nz\nAttainment 8\n50\n100\n0.20\n\n\nTotal\nz\nProgress 8\n50\n100\n0.08\n\n\nFemale\nz\nProgress 8\n50\n100\n0.08\n\n\nMale\nz\nProgress 8\n50\n100\n0.09\n\n\n\n\n\n\nExample of good practice\nThe following would be accepted for publication via the API. In this case, splitting the data into separate data files is required in order to create tidy data structures.\n\n\nAttainment grade rates in tidy format\n\n\ngrade_range\npupil_count\npupil_percent\n\n\n\n\nGrades 9-5\n30\n60\n\n\nGrades 9-4\n40\n80\n\n\nGrades 9-1\n50\n100\n\n\n\n\n\n\nAttainment scores in tidy format\n\n\nsex\naccountability_measure\nscore_average\n\n\n\n\nFemale\nProgress 8\n0.21\n\n\nMale\nProgress 8\n0.20\n\n\nTotal\nProgress 8\n0.21\n\n\nFemale\nAttainment 8\n0.08\n\n\nMale\nAttainment 8\n0.09\n\n\nTotal\nAttainment 8\n0.08",
    "crumbs": [
      "Statistics production",
      "Statistics API Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/api-data-standards.html#standardised-filter-col_names-and-items",
    "href": "statistics-production/api-data-standards.html#standardised-filter-col_names-and-items",
    "title": "Statistics API Data Standards",
    "section": "Standardised filter col_names and items",
    "text": "Standardised filter col_names and items\nThe explore education and statistics platforms team alongside the data harmonisation champions group and publication teams are developing a series of standardised filters that teams are required to use when creating data for the API. These are being built iteratively as more data is put forward for the API, so if the current standards don’t cater to your data set, you can contribute to building the harmonised standards for others to follow.\nThe standards can be used to create individual filter columns or combined filters (i.e. breakdown_topic / breakdown_topic).\nAreas for which harmonised standards are currently available are:\n\nestablishment / school / provider characteristics\nethnicity\nsex and gender\nspecial educational needs\n\nAreas which are currently under development are:\n\nattainment metrics\ndisadvantaged status\nfree school meal status\n\nWe encourage contributions to and feedback on all of the above and any other filter topic.\n\nExamples of common non-standard filter col_names\n\n\nExample non-standard col_names and their potential equivalents in the standardised framework.\n\n\n\n\n\n\nNon-standard\nPotential standard equivalents\n\n\n\n\nethnicity\nethnicity_major, ethnicity_minor\n\n\ncharacteristic_sex\nsex\n\n\nschool_type\nestablishment_type, establishment_type_group or education_phase\n\n\npupil_sen_status\nsen_status\n\n\ncharacteristic_primary_need\nsen_primary_need\n\n\ncharacteristic_topic\nbreakdown_topic, breakdown_topic_establishment\n\n\ncharacteristic\nbreakdown, breakdown_establishment",
    "crumbs": [
      "Statistics production",
      "Statistics API Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/api-data-standards.html#standardised-indicator-col_names",
    "href": "statistics-production/api-data-standards.html#standardised-indicator-col_names",
    "title": "Statistics API Data Standards",
    "section": "Standardised indicator col_names",
    "text": "Standardised indicator col_names\nIndicators should be named in line with the indicator naming conventions set out in the open data standards.\n\nExamples of common non-standard indicator col_names\n\n\nExample non-standard indicator col_names and their potential equivalents in the standardised framework.\n\n\n\n\n\n\nNon-standard\nPotential standard equivalents\n\n\n\n\nnumber_of_pupils\npupil_count\n\n\nNumberOfLearners, NumLearners\npupil_count, learner_count\n\n\ntotal_male, total_female\npupil_count (plus sex filter)\n\n\npt_SEN_support\npupil_percent (plus SEN status filter)\n\n\nnum_provider, num_providers\nestablishment_count\n\n\nno_schools, num_schools, total_schools\nestablishment_count\n\n\nnum_inst, total_institutions, number_institutions, inst_count\nestablishment_count",
    "crumbs": [
      "Statistics production",
      "Statistics API Data Standards"
    ]
  },
  {
    "objectID": "statistics-production/scrums.html",
    "href": "statistics-production/scrums.html",
    "title": "Publication scrums",
    "section": "",
    "text": "Things to consider when writing statistics publications",
    "crumbs": [
      "Statistics production",
      "Publication scrums"
    ]
  },
  {
    "objectID": "statistics-production/scrums.html#get-involved",
    "href": "statistics-production/scrums.html#get-involved",
    "title": "Publication scrums",
    "section": "Get involved",
    "text": "Get involved\nWe regularly run publication scrums where teams can put forward their publication(s) for review by a group of volunteer analysts, providing feedback as ‘unfamiliar new readers’ and to discuss ideas for future improvements. The sessions usually last for 1-2 hours.\nIf you’re interested in being involved in future scrums, please contact us.",
    "crumbs": [
      "Statistics production",
      "Publication scrums"
    ]
  },
  {
    "objectID": "statistics-production/scrums.html#checklists",
    "href": "statistics-production/scrums.html#checklists",
    "title": "Publication scrums",
    "section": "Checklists",
    "text": "Checklists\nThe guidance on content design is in the form of a handy checklist, co-produced with local statisticians, and supported by the Good Practice Team. This builds on ONS’s Best Practice guidance on Data Visualisationand Writing about Statistics.\nThe content checklist is for teams to use throughout their project cycle, so that good content design is at the heart of what they deliver at all stages, rather than considered late in the process - Content design checklist (.xlsx).\nThere is also a dashboards version of the content design checklist, that runs through a number of things to think about when developing dashboards to compliment official statistics - Dashboards checklist (.xlsx).\nIf you are responsible for signing off publications, then please download and see the Statistics Leadership Group paper highlighting top tips for content design.",
    "crumbs": [
      "Statistics production",
      "Publication scrums"
    ]
  },
  {
    "objectID": "statistics-production/scrums.html#scrum-information",
    "href": "statistics-production/scrums.html#scrum-information",
    "title": "Publication scrums",
    "section": "Scrum information",
    "text": "Scrum information\nYou can experience how a scrum runs through watching this scrum-along (features the scrum up until groups breakout to discuss different elements) and supporting slides\n\n\n\n\nA full before / after scrum along is available to watch, with slides available to download separately.\nFor a walk through of some of the end to end benefits the scrums have had, take a look at this video for Schools and pupils Statistics Team.\nFinally, an example of a team who has been through the scrum process for 3 publications talk through their realised benefits, showcasing the type of benefits potentially others could also realise is also available to watch on sharepoint.",
    "crumbs": [
      "Statistics production",
      "Publication scrums"
    ]
  },
  {
    "objectID": "statistics-production/pub.html",
    "href": "statistics-production/pub.html",
    "title": "Routes for publishing",
    "section": "",
    "text": "Guidance for how to publish different types of statistics",
    "crumbs": [
      "Statistics production",
      "Routes for publishing"
    ]
  },
  {
    "objectID": "statistics-production/pub.html#routes-for-publishing",
    "href": "statistics-production/pub.html#routes-for-publishing",
    "title": "Routes for publishing",
    "section": "Routes for publishing",
    "text": "Routes for publishing\nExplore education statistics (EES) is the Department’s statistics dissemination platform, designed to make DfE’s published statistics and data easier to find, access, use and understand.\nThe platform moved into Public Beta in March 2020 and is the home of all published official statistics from DfE. Some management information is also made available on the service too.\nMore information on why EES was introduced and the functionality within the platform can be found in these slides.\n\nAll national, official and ad-hoc statistics should be published on EES\nManagement Information should be published on GOV.UK\nInteractive dashboards built for particular user needs can be published separately, but should have a linked publication on either EES or GOV.UK\n\nThe following table outlines the key differences between publishing via EES and the old method via GOV.UK\n\n\n\n\n\n\n\nStatistics collections on GOV.UK\nExplore education statistics\n\n\n\n\nStatisticians request pdf and excel files are uploaded to release pages on gov.uk\nStatisticians load csv data files on to the service and use them to build release pages\n\n\nRelease attachments are reviewed and approved via emails\nReleases are reviewed and approved within the service\n\n\nRelease attachments are circulated for pre-release via email 24hours prior to publication date\nPre-release users are invited to preview releases within the service 24 hours prior to publication date\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are unsure which route to use for publication please contact the HoP Office.",
    "crumbs": [
      "Statistics production",
      "Routes for publishing"
    ]
  },
  {
    "objectID": "statistics-production/pub.html#ees",
    "href": "statistics-production/pub.html#ees",
    "title": "Routes for publishing",
    "section": "EES",
    "text": "EES\nAll national, official and ad-hoc statistics should be published on EES. This section covers how to start publishing on EES and all the things you need to consider before publication. For more detailed guidance on how to use the platform, visit our EES guidance page\n\n\nHow to publish\n\n\n\n\nEES create release one pager\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublication checklist\n\nBefore releasing statistics for the first time you may want to discuss the new process with key stakeholders and / or pre-release users to make them aware of the new service. You should also inform the explore education statistics platforms team and Statistics HoP Office.\nBefore you start creating a release in the platform you should have:\n\nAnnounced the upcoming release via GOV.UK\nSent metadata form to HoP\nContacted the BAU team so we can support you with your first release\nProduced your tidy CSV data files with appropriate disclosure control\nProduced metadata files for each CSV data file\nRan your data and metadata through our screener checks\n\nBefore you publish a release you have created in the platform you should have:\n\nChecked all the data has loaded successfully\nWritten footnotes\nWritten content (including tables and charts)\nCreated a data guidance document\nCreated a public pre-release access list\nEnsured methodology information is either linked off to or attached to the release\nPassed the release for higher review (senior sign-off)\nPreviewed your release\nScheduled the release date\nInvited your PRA list to preview 24 hours before it goes live\nRaised a web ticket for the associated gov.uk page\n\n\n\n\n\n\n\nNote\n\n\n\nWord templates for the data guidance, pra-list, and content can be found on sharepoint.\n\n\n\n\n\nLinking to GOV.UK\n\nYou will need to arrange a GOV.UK statistics publication page so that it links to EES. Here is how to do that:\nTwo days ahead of publication, you’ll need to raise a ticket with the Digital communications (gov.uk) team and ask them to create a new gov.uk statistics page with a link to EES, connect it to the announcement and add to any collections.\nIn your request, you’ll need to include:\n\ntitle, summary sentence and ‘detail’ for the new page – you can include a link to previous releases if you want it to be the same\nthe link for your EES release – if you don’t have it you can update the ticket when the link is available\nthe link for the announcement\nthe link of any gov.uk collections it needs to be added to\nan email with clearance from your deputy director\nan email confirming communications are happy for it to go if it’s for stats that aren’t pre-announced or it’s an update to stats made after publication – so that they can prepare reactive lines\n\nHere are examples of how the page will look like:\n\nPrimary School Performance Tables 2018\nHigher Education Student Statistics 2018 to 2019\n\n\n\n\n\n\n\nTip\n\n\n\nYou can find what the link to your EES release will be by looking at the ‘Sign off’ page within the release dashboard on EES.",
    "crumbs": [
      "Statistics production",
      "Routes for publishing"
    ]
  },
  {
    "objectID": "statistics-production/pub.html#gov.uk",
    "href": "statistics-production/pub.html#gov.uk",
    "title": "Routes for publishing",
    "section": "GOV.UK",
    "text": "GOV.UK\nManagement information releases can be published directly on gov.uk.\n\n\nHow to publish\n\nAhead of publication, you’ll need to raise a ticket with the Digital communications (gov.uk) team and ask them to create a new release (or add to a series if this already exists).\nIn your request, you’ll need to include:\n\ntitle, summary sentence and ‘detail’ for the new page\nthe link for the announcement\nthe link of any gov.uk collections it needs to be added to\nan attachment with the accessible data files\nan email with clearance from your deputy director\n\nWhen you raise the ticket please ensure that all the documents you submit are in an accessible format as stated in the accessibility guidance. There is also guidance on accessibility for Excel workbooks in Teams.\n\n\n\nPublication checklist\n\nBefore you start creating your release you should have:\n\nAnnounced the upcoming release via gov.uk\nSent metadata form to HoP\n\nBefore you publish a release you should have:\n\nChecked that the data meets accessibility requirements, ideally as a .csv file, or an accessible Excel file where this is not possible\nWritten footnotes where appropriate\nPassed the release for higher review (senior sign-off)\nSent the final files and sign-off email to the digital communications (gov.uk) team",
    "crumbs": [
      "Statistics production",
      "Routes for publishing"
    ]
  },
  {
    "objectID": "statistics-production/pub.html#dashboards",
    "href": "statistics-production/pub.html#dashboards",
    "title": "Routes for publishing",
    "section": "Dashboards",
    "text": "Dashboards\nDashboards built to meet a particular user need can be another helpful tool to help users access and understand your data, and can be hosted outside of EES or gov.uk.\nGuidance to follow when creating and publishing dashboards is available via the dashboards page.",
    "crumbs": [
      "Statistics production",
      "Routes for publishing"
    ]
  },
  {
    "objectID": "statistics-production/user-analytics.html",
    "href": "statistics-production/user-analytics.html",
    "title": "EES analytics",
    "section": "",
    "text": "Guidance on understanding how users are interacting with your statistics published via EES\nWe track analytics of users on the explore education statistics service using Google Analytics as well as other tools. These analytics can be found on our EES Analytics dashboard. This tells you a range of things like:",
    "crumbs": [
      "Statistics production",
      "EES analytics"
    ]
  },
  {
    "objectID": "statistics-production/user-analytics.html#user-led-content-improvements",
    "href": "statistics-production/user-analytics.html#user-led-content-improvements",
    "title": "EES analytics",
    "section": "User-led content improvements",
    "text": "User-led content improvements\nThe below table gives some pointers on how you can use these stats to improve your release for users:\n\n\n\n\n\n\n\nMeasure\nGuidance\n\n\n\n\nSearch terms\nFor popular search terms, add additional commentary or signposting using subheadings and accordions for to support your user’s interests. If you refer to a popular search term by another name, consider changing it so users know what you’re talking about.\n\n\nAccordion openings\nConsider removing accordions with low numbers of clicks from your release. You could move them further up the page or group them into existing accordions if this commentary is essential to your release.\n\n\nDownloads\nFor files with low numbers of downloads, consider combining information into other files, or consider removing from your publication entirely if users are no longer interested in this data.\n\n\nTables built\nFor tables with low numbers of interactions in the table tool, create featured tables to demonstrate the useful information they hold.\n\n\nPermalinks\nSave popular permalinks as featured tables in future, so users can easily get to the data they want.\n\n\nTraffic sources\nConsider where your users are being routed from and what is being written about your stats - clearly highlight key stories in the Headlines section and make use of the blue key stat tiles to make them easy to digest.",
    "crumbs": [
      "Statistics production",
      "EES analytics"
    ]
  },
  {
    "objectID": "statistics-production/user-analytics.html#guidance-for-analytics",
    "href": "statistics-production/user-analytics.html#guidance-for-analytics",
    "title": "EES analytics",
    "section": "Guidance for analytics",
    "text": "Guidance for analytics\nPerformance analysis: GDS starter guide to using Google Analytics\nPerformance analysis: GDS Advanced Analytics Reporting Techniques",
    "crumbs": [
      "Statistics production",
      "EES analytics"
    ]
  },
  {
    "objectID": "statistics-production/stats_tools.html",
    "href": "statistics-production/stats_tools.html",
    "title": "Tools for statistics",
    "section": "",
    "text": "Guidance for how to use the tools available for statistics publishers\n\n\nWe also have a number of tools for statistics production teams to use to help them in their processes, they can be accessed at the links below when using a DfE device.\n\nEES data screening app\nThis application allows you to screen your data files against the underlying data standards for Official and National statistical publications. If your file passes the checks it also gives you some basic options for exploring the data in the file.\n\nEES data screener\nData screener code on GitHub\n\n\n\nEES analytics\nThere is an R Shiny dashboard that provides access to the Google Analytics data for the explore education statistics service.\n\nExplore education statistics analytics dashboard\nCode is not yet publicly available, it is stored in Azure DevOps, contact us if you are interested in seeing it or contributing.\n\n\n\nPublication RAP self-assessment tool\nA self assessment tool allowing official statistics publications to understand how their production processes match up against RAP expectations.\n\nRAP self-assessment tool\nRAP self-assessment code mirrored on GitHub\n\n\n\nTemplate code\n\nTemplate DfE R Shiny dashboard\nTemplate QA code repository\ndfeR R package\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ADA/databricks_fundamentals.html#what-is-databricks",
    "href": "ADA/databricks_fundamentals.html#what-is-databricks",
    "title": "Databricks fundamentals",
    "section": "What is Databricks?",
    "text": "What is Databricks?\nDatabricks is a web based platform for large scale data manipulation and analysis using code to create reproducible data pipelines. Primarily it takes the form of a website which you can create data pipelines and perform analysis in. It currently supports the languages R, SQL, Python and Scala, and integrates well with Git based version control systems such as GitHub or Azure DevOps.\nBehind the scenes it is a distributed cloud computing platform which utilizes the Apache Spark engine to split up heavy data processing into smaller chunks. It then distributes them to different ‘computers’ within the cloud to perform the processing of each chunk in parallel. Once each ‘computer’ is finished processing the results are recombined and passed back to the user or stored.\nDue to the parallel processing capabilities this improves the performance of the data processing and allows for the manipulation of very large data sets in a relatively short amount of time.\nIn addition, it also provides new tools within the platform to construct and automate complex data transformations and processing.",
    "crumbs": [
      "Learning resources",
      "Databricks fundamentals"
    ]
  },
  {
    "objectID": "ADA/databricks_fundamentals.html#key-differences",
    "href": "ADA/databricks_fundamentals.html#key-differences",
    "title": "Databricks fundamentals",
    "section": "Key differences",
    "text": "Key differences\nUnderpinning the technology are some key differences in how computers we’re familiar with, and Databricks (and distributed computing in general) are structured.\n\n\nTraditional computing\n\nCurrently, we are used to using a PC or laptop to do our data processing. A traditional computer has all of the components it needs to function:\n\nA processor and memory to do calculations\nA hard drive to store data permanently on\nA keyboard and mouse to capture user input\nA screen to provide outputs to the user\n\n\nAny traditional computer is limited by its hardware meaning there is an upper limit on the size and complexity of data it can process.\nIn order to increase the amount of data a computer can process, you would have to switch out the physical hardware of the machine for something more powerful.\n\n\n\nOn Databricks\n\nIn Databricks you can scale the components of your machine up (CPU cores, RAM) without having to build a physical machine to house them, essentially temporarily ‘borrowing’ processor power, memory and storage from a super computer.\nThis means you can perform very heavy analyses that your laptop wouldn’t be able to cope with. The Databricks platform provides a way for you to interface with the cloud computer in place of the keyboard/mouse and screen, taking your inputs and providing the resulting outputs back to you.\nThe storage and computation are separated into different components rather than being within the same ‘machine’. Processing (processor and memory) is handled by a ‘compute’ resource, and storage (hard drive) is centralised in the ‘unity catalog’.\n\n\n\nBenefits of cloud compute\n\n\nScalable - if you need more computing power, you can increase your computing power and only pay for what you use rather than having to build an expensive new machine\nCentralised - All data, scripts, and processes are available in a single place and access for any other user can be controlled by their author, or the wider Department as required\nData Governance - The Department is able to ‘see’ all of its data and organisational knowledge. This enables it to ensure it is access controlled and align with GDPR and data protection legislation and guidelines\nAuditing and version control - The Platform itself generates a lot of metadata which enables it to keep versioned history of its data, outputs, etc\nAutomation - Complex data processing pipelines can be set up using Databricks workflows and set to automatically run, either on a timer or a specific trigger allowing for a fully automated production process\n\nEach of these aspects bring benefits to the wider Department and for analysts within it.\nDue to the scalability of compute resources you can request a more powerful processing cluster allowing you to deal with larger datasets more simply, focussing on the analytical logic of it rather than having to build processes around technical limitations such as storage space and processing power.\nThe centralised nature of the data storage makes navigation of the Department’s data estate much simpler with everything existing in a single environment. Combined with stronger data governance this makes discovery of supplementary or related data the Department holds much easier. In addition, it allows for datasets that are commonly used across the Department - such as ONS geography datasets - to be standardised and made available to all teams, ensuring consistency of data and it’s formatting across the Departments publications.\nThe auditing, and automation facilities provide a lot of benefits when building data pipelines. These can be set up to run as required with little manual effort from analysts, and can build automated quality assurance into the pipeline so that analysts can be confident in the outputs. In addition, the auditing keeps a record of all inputs and outputs each time a process is run. Combining this with robust documentation stored in Notebooks allows you debug issues retrospectively without having to repeatedly step through the process to see where unexpected issues have occurred.",
    "crumbs": [
      "Learning resources",
      "Databricks fundamentals"
    ]
  },
  {
    "objectID": "ADA/databricks_fundamentals.html#key-concepts",
    "href": "ADA/databricks_fundamentals.html#key-concepts",
    "title": "Databricks fundamentals",
    "section": "Key concepts",
    "text": "Key concepts\n\nStorage\nThere are a few different ways of storing files and data on Databricks. Your data, and modelling areas will reside in the ‘unity catalog’, whereas your scripts and code will live on your ‘workspace’.\n\n\n\nUnity catalog\n\nThe majority of data and files on Databricks should be stored in the ‘unity catalog’. This is similar in concept to a traditional database server, however the unity catalog also contains file storage in the form of volumes.\nThe unity catalog can be accessed through the ‘Catalog’ option in the Databricks sidebar.\n\n\n\nStructure of the unity catalog\n\nThere is one ‘unity catalog’ for the whole of the Department for Education, this is what enables the Department to keep track of all of it’s data in a single place. The ‘unity catalog’ can contain any number of catalogs (equivalent to databases).\nA catalog can contain any number of schemas.\nA schema can contain any number of tables, views and volumes.\n\n\n\n\nCatalogs not databases\n\nThe ‘unity catalog’ is a single catalog that contains all the other catalogs of data in the Department. Catalogs are very similar in concept to a SQL database in that they they contain schemas, tables of data and views of data.\n\n\n\nSchemas, tables and views\n\nLike a SQL database a catalog has schemas, tables, and views which store data in a structured (usually tabular) format.\nA schema is a sub-division of a catalog which allows for logical separation of data stored in the catalog. Whoever creates a schema is its owner, and is able to set fine grained permissions on who can see / edit the data within it. Permissions can also be set for groups of analysts, and can be modified by the ADA team if the original owner is no longer available.\nTables are equivalent to SQL tables, and store data in a tabular format. Tables in Databricks have the ability to turn on version control which audits each change to the data and allows a user to go back in time to see earlier versions of the table.\nViews look and act the same as tables, however instead of storing the data as it is presented a view is created from a query which is ran when the view is referenced. This allows you to provide alternative ways to format data from tables without storing duplicated data.\nTables and views sit within a schema and these are where you would store your core datasets and pick up data to analyse from.\n\n\n\nVolumes\n\nUnlike a SQL database the unity catalog also contains volumes, which are file storage similar to a shared drive. They can be used for storing any type of file.\nVolumes are stored under a schema within a catalog. Files in here can be accessed and manipulated through code.\nExamples of files suitable to be stored in a volume include CSVs, JSON and other formats of data files, or supporting files / images for applications you develop through the platform. You can also upload files to a volume through the user interface.\n\n\n\n\nWorkspaces - Databricks file system (DBFS)\n\nEach user has their own workspace which serves as a personal document and code storage area. It contains a user folder which is only accessible to that user by default, along with any Git repositories that you have cloned or created within Databricks.\nYour workspace can be accessed through the ‘Catalog’ option in the Databricks sidebar.\n\n\n\n\n\nEverything in your workspace is only accessible to you unless you share it with other users. When you do share a script or file you can specify whether the person you’re sharing it with is able to view/edit/run the file you’re sharing.\n\n\n\n\n\n\nDon’t overshare\n\n\n\nSharing code this way can be useful but has it’s risks. If you allow other users to edit and run your workbooks it’s possible that they can make changes or run it simultaneously resulting in unexpected results.\nFor collaboration on code you should use a GitHub/DevOps repository which each user can clone and work on independently.\n\n\n\n\n\nRepositories for version control\n\nAll code should be managed through a versioned repository on GitHub or Azure DevOps. You can commit and push your changes to the central repository from the Databricks platform. Pull requests to merge your changes into the ‘main’ branch still take place on your Git provider.\nTo connect Databricks to a repository refer to the Databricks and version control article.",
    "crumbs": [
      "Learning resources",
      "Databricks fundamentals"
    ]
  },
  {
    "objectID": "ADA/databricks_fundamentals.html#compute",
    "href": "ADA/databricks_fundamentals.html#compute",
    "title": "Databricks fundamentals",
    "section": "Compute",
    "text": "Compute\nIn order to access data and run code you need to set up a compute resource. A compute resource provides processing power and memory to pick up and manipulate the data and files stored in the ‘unity catalog’. The compute page can be accessed through the ‘Compute’ option in the Databricks sidebar.\n\nThere are several types of compute available and you will need to make the most appropriate choice for the kind of processing you’re wanting to do. The types are:\n\nSQL Warehouse - multiple users, SQL only; best to use if you’re only querying data from Databricks using SQL\nPersonal cluster - single user, supports R, SQL, python and scala; best to use if you require languages other than SQL\nShared cluster - multiple users, supports SQL, python and scala;\n\nYou are able to create a personal cluster yourself, whereas shared clusters and SQL warehouses have to be requested through the ADA team.\nIn most cases a personal cluster will be the most versatile and easily accessible option. However your team may want to request a shared SQL warehouse if you have a lot of processing heavy SQL queries and do not need to use any other language in the Databricks platform itself.\nAll compute options can be used both within the Databricks platform and be connected to through other applications. Instructions on how to connect R / RStudio to a SQL Warehouse, or a personal cluster can be found on the following pages:\n\nSetup Databricks SQL Warehouse with RStudio\nSetup Databricks Personal Compute cluster with RStudio\n\nOnce you have a compute resource you can begin using Databricks. You can do this either through connecting to Databricks through RStudio, or you can begin coding in the Databricks platforms using scripts, or Notebooks.",
    "crumbs": [
      "Learning resources",
      "Databricks fundamentals"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster.html",
    "href": "ADA/databricks_rstudio_personal_cluster.html",
    "title": "Personal cluster with RStudio and odbc / DBI",
    "section": "",
    "text": "Important\n\n\n\nPlease be aware that the Databricks platform is regularly updated and may look different from the guidance included on this site. If you notice any discrepancies between the content on this site and the Databricks platform, please let us know by contacting statistics.development@education.gov.uk.\nThe following instructions set up an ODBC connection between your laptop and your Databricks cluster, which can then be used in RStudio to query data using an ODBC based package.\nPersonal clusters can be used within the Databricks environment, or through RStudio. You can set one up yourself if you don’t have access to a SQL warehouse or shared cluster.\nA personal cluster is the most versatile type of compute and the most easily accessible for analysts within the Department. If you are considering converting an existing R / RStudio process to the Databricks environment a personal cluster is the best choice as you will be able to run like for like code on both environments.\nWithin the Databricks environment personal clusters are able to use SQL, R, Python and Scala, and are more flexible than SQL Warehouses. Using this ODBC method from RStudio you will be able to query data using similar methods as you currently use when connecting to Microsoft SQL server. Usually this will take the form of submitting SQL queries to Databricks from R code running on your laptop.\nYou can use data from Databricks with R code in two different ways:",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `odbc` / `DBI`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster.html#pre-requisites",
    "href": "ADA/databricks_rstudio_personal_cluster.html#pre-requisites",
    "title": "Personal cluster with RStudio and odbc / DBI",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou must have:\n\nAccess to Databricks and the data you’ll be working with\nAccess to a personal cluster on Databricks\nR and RStudio downloaded",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `odbc` / `DBI`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster.html#compute-resources",
    "href": "ADA/databricks_rstudio_personal_cluster.html#compute-resources",
    "title": "Personal cluster with RStudio and odbc / DBI",
    "section": "Compute resources",
    "text": "Compute resources\nWhen your data is moved to Databricks, it will be stored in the Unity Catalog and you will need to use a compute resource to access it from other software such as RStudio.\nA compute resource allows you to run your code using cloud computing power instead of using your laptop’s processing power. This means that using compute resources can allow your code to run faster than it would if you ran it locally, as it is like using the processing resources of multiple computers at once. On this page, we will be referring to the use of personal clusters as the compute resource to run your code.\n\n\nPersonal clusters\n\nA personal cluster is a compute resource that supports the use of multiple code languages (R, SQL, Scala and Python) in the Databricks environment and can be set up to connect to RStudio as well. You can create your own personal cluster within the Databricks interface.\nWhen you set up your personal cluster, you will be asked to select a runtime for that cluster. Different runtimes allow you to use different features and package versions. Certain packages are installed by default on a personal cluster and do not need to be installed manually. The specific packages installed are based on the Databricks Runtime (DBR) version your cluster is set up with. A comprehensive list of packages included in each DBR is available in the Databricks documentation.\nCompute resources, including personal clusters, have no storage of their own. This means that if you install libraries or packages onto a cluster they will only remain installed until the cluster is stopped. Once re-started those libraries will need to be installed again.\nAn alternative to this is to specify packages / libraries to be installed on the cluster at start up. To do this click the name of your cluster from the ‘Compute’ page, then go to the ‘Libraries’ tab and click the ‘Install new’ button.\n\n\n\n\n\n\nClusters will shut down after being idle for an hour\n\n\n\nUse of compute resources are charged by the hour, and so personal clusters have been set to shut down after being unused for an hour in order to prevent unnecessary cost to the Department.",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `odbc` / `DBI`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster.html#process",
    "href": "ADA/databricks_rstudio_personal_cluster.html#process",
    "title": "Personal cluster with RStudio and odbc / DBI",
    "section": "Process",
    "text": "Process\nThere are four steps to complete before your connection can be established. These are:\n\nCreating a personal compute resource (if you do not already have one)\nInstalling an ODBC driver on your laptop to enable a connection between your laptop and Databricks\nModifying your .Renviron file to establish a connection between RStudio and Databricks\nAdding connection code to your existing scripts in RStudio\n\n\n\nCreating a personal compute resource\n\n\nTo create your own personal compute resource click the ‘Create with DfE Personal Compute’ button on the compute page\n\n\n\n\nYou’ll then be presented with a screen to configure the cluster. There are 2 options here under the performance section which you will want to pay attention to; Databricks runtime version, and Node type\n\nDatabricks runtime version - This is the version of the Databricks software that will be present on your compute resource. Generally it is recommended you go with the latest LTS (long term support) version. At the time of writing this is ‘15.4 LTS’\n\nNode type - This option determines how powerful your cluster is and there are 2 options available by default:\n\n\nStandard 14GB 4-Core Nodes\n\nLarge 28GB 8-Core Nodes\n\nIf you require a larger personal cluster this can be requested by the ADA team.\n\n\n\nClick the ‘Create compute’ button at the bottom of the page. This will create your personal cluster and begin starting it up. This usually takes around 5 minutes\n\n\nOnce the cluster is up and running the icon under the ‘State’ header on the ‘Compute’ page will appear as a green tick",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `odbc` / `DBI`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster.html#setting-up-the-odbc-driver",
    "href": "ADA/databricks_rstudio_personal_cluster.html#setting-up-the-odbc-driver",
    "title": "Personal cluster with RStudio and odbc / DBI",
    "section": "Setting up the ODBC driver",
    "text": "Setting up the ODBC driver\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously set up an ODBC connection, or followed the set up Databricks SQL Warehouse with RStudio guidance, then you can skip this step.\n\n\n\nOpen the Software Centre via the start menu\nIn the ‘Applications’ tab, click Simba Spark ODBC Driver 64-bit\n\n\n\n\n\nClick install\n\n\n\nEstablishing an RStudio connection using environment variables\n\nThe ODBC package in RStudio allows you to connect to Databricks by creating and modifying three environment variables in your .Renviron file.\n\n\n\n\n\n\nNote\n\n\n\nIf you have previously established a connection between a SQL Warehouse and RStudio, then some of these variables will already be in your .Renviron file.\n\n\nTo set the environment variables, call usethis::edit_r_environ(). You will then need to enter the following information:\nDATABRICKS_HOST = \"databricks-host\"\nDATABRICKS_CLUSTER_PATH = \"databricks-cluster-path\"\nDATABRICKS_TOKEN = \"personal-access-token\"\nOnce you have entered the details, save and close your .Renviron file and restart R (Session &gt; Restart R).\n\n\n\n\n\n\nNote\n\n\n\nEveryone in your team that wishes to connect to the data in Databricks and run your code must set up their .Renviron file individually, otherwise their connection will fail.\n\n\nThe sections below describe where to find the information needed for each of the environment variables.\n\n\nDatabricks host\n\nThe Databricks host is the instance of Databricks that you want to connect to. It’s the URL that you see in your browser bar when you’re on the Databricks site and should end in “azuredatabricks.net” (ignore anything after this section of the URL).\n\n\n\nDatabricks cluster path\n\nIn Databricks, go to Compute in the left hand menu, and click on the name of your personal cluster:\n\n\n\nOn the Configuration tab, scroll to the bottom of the page and click Advanced Options &gt; JDBC/ODBC &gt; HTTP Path, and copy the text after the last forward slash. This is your cluster path.\n\n\n\nDatabricks token\n\nThe Databricks token is a personal access token.\nA personal access token is is a security measure that acts as an identifier to let Databricks know who is accessing information from the personal cluster. Access tokens are usually set for a limited amount of time, so they will need renewing periodically.\n\nIn Databricks, click on your email address in the top right corner, then click ‘User settings’\nGo to the ‘Developer’ tab in the side bar. Next to ‘Access tokens’, click the ‘Manage’ button\n\n\n\n\n\nClick the ‘Generate new token’ button\nName the token, then click ‘Generate’\n\n\n\n\n\n\n\nNote\n\n\n\nNote that access tokens will only last as long as the value for the ‘Lifetime (days)’ field. After this period the token will expire, and you will need to create a new one to re-authenticate. Access tokens also expire if they are unused after 90 days. For this reason, we recommend setting the Lifetime value to be 90 days or less.\n\n\n\nMake a note of the ‘Databricks access token’ it has given you\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is very important that you immediately copy the access token that you are given, as you will not be able to see it through Databricks again. If you lose this access token before pasting it into RStudio then you must generate a new access token to replace it.\n\n\n\n\n\n\nPulling data into RStudio from Databricks\n\nNow that you have enabled ODBC connections on your laptop, and enabled a connection between Databricks and RStudio, you can add code to your existing scripts to pull data into RStudio for analysis. If you have connected to databases before, this code will look quite familiar to you.\nTo access the data, we will make use of the odbc and DBI packages.\nInclude the following code in your R Script:\nlibrary(odbc)\nlibrary(DBI)\n\ncon &lt;- DBI::dbConnect(odbc::databricks(),\n                      httpPath = Sys.getenv(\"DATABRICKS_CLUSTER_PATH\"))\n\ndbGetQuery(con, \"SHOW CATALOGS;\")",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `odbc` / `DBI`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_sql_warehouse.html",
    "href": "ADA/databricks_rstudio_sql_warehouse.html",
    "title": "SQL Warehouse with RStudio",
    "section": "",
    "text": "Important\n\n\n\nPlease be aware that the Databricks platform is regularly updated and may look different from the guidance included on this site. If you notice any discrepancies between the content on this site and the Databricks platform, please let us know by contacting statistics.development@education.gov.uk.\nThe following instructions will help you to set up a connection between your laptop and your Databricks SQL warehouse which can then be used in RStudio to query data.\nYou can use data from Databricks in two different ways:",
    "crumbs": [
      "Databricks Setup Guides",
      "SQL Warehouse with RStudio"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_sql_warehouse.html#compute-resources",
    "href": "ADA/databricks_rstudio_sql_warehouse.html#compute-resources",
    "title": "SQL Warehouse with RStudio",
    "section": "Compute resources",
    "text": "Compute resources\nWhen your data is moved to Databricks, it will be stored in the Unity Catalog and you will need to use a compute resource to access it from other software such as RStudio.\nA compute resource allows you to run your code using cloud computing power instead of using your laptop’s processing power. This means that using compute resources can allow your code to run faster than it would if you ran it locally, as it is like using the processing resources of multiple computers at once. On this page, we will be referring to the use of SQL Warehouses as the compute resource to run your code.\n\n\nSQL Warehouse\n\nA SQL Warehouse is a SQL-only compute option which is quick to start and optimised for SQL querying. Although the name “warehouse” suggests storage, a SQL Warehouse in Databricks is actually a virtual computing resource that allows you to interact with Databricks by connecting to your data and running code.\nThis option is recommended if you only require SQL functionality in Databricks and is ideal if you already have existing RAP pipelines set up using SQL scripts in a Git repo.\nSQL Warehouses do not support R, Python or Scala code. Currently they also do not support widgets within Databricks notebooks. If you want to use compute resources to run widgets or R or Python code, then you will need to use a personal cluster. There is guidance on the use of personal clusters on the Using personal clusters with Databricks page.\nSQL Warehouses enable you to access tables in the Unity Catalog, but not volumes within the Unity Catalog. Volumes are storage areas for files (e.g. .txt files or .csv files) rather than tables. You can learn more about volumes on the Databricks documentation site or on our Databricks fundamentals page. To access a volume, you will also need to use a personal cluster.",
    "crumbs": [
      "Databricks Setup Guides",
      "SQL Warehouse with RStudio"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_sql_warehouse.html#pre-requisites",
    "href": "ADA/databricks_rstudio_sql_warehouse.html#pre-requisites",
    "title": "SQL Warehouse with RStudio",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nBefore you start, you must have:\n\nAccess to the Databricks platform\nAccess to data in a SQL Warehouse on Databricks\nR and RStudio downloaded and installed\nThe odbc, DBI and usethis packages installed in RStudio\n\nIf you do not have access to Databricks or a SQL Warehouse within Databricks, you can request this using a service request form.\nIf you do not have R or RStudio, you can find them both in the Software Centre. Note that you need both R and RStudio installed.",
    "crumbs": [
      "Databricks Setup Guides",
      "SQL Warehouse with RStudio"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_sql_warehouse.html#process",
    "href": "ADA/databricks_rstudio_sql_warehouse.html#process",
    "title": "SQL Warehouse with RStudio",
    "section": "Process",
    "text": "Process\nThere are three steps to complete before your connection can be established. These are:\n\nInstalling an ODBC driver on your laptop to enable a connection between your laptop and Databricks\nModifying your .Renviron file to establish a connection between RStudio and Databricks\nAdding connection code to your existing scripts in RStudio\n\nEach of these steps is described in more detail in the sections below.\n\n\nSetting up the ODBC driver\nAn ODBC driver is required for the odbc package in R to work - you must install it before attempting to use the package to connect to your data.\n\n\nInstall the Simba Spark ODBC driver from the Software Centre\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously set up an ODBC connection, or followed the set up Databricks personal compute cluster with RStudio guidance, then you can skip this step.\n\n\n\nOpen the Software Centre via the start menu\nIn the ‘Applications’ tab, click Simba Spark ODBC Driver 64-bit\n\n\n\n\n\nClick install\n\n\n\n\n\nEstablishing an RStudio connection using environment variables\n\nThe odbc package in RStudio allows you to connect to Databricks by creating and modifying three environment variables in your .Renviron file.\nTo set the environment variables, call usethis::edit_r_environ(). You will then need to enter the following information:\nDATABRICKS_HOST = \"databricks-host\"\nDATABRICKS_SQL_PATH = \"sql-warehouse-path\"\nDATABRICKS_TOKEN = \"personal-access-token\"\nThe sections below describe where to find the information needed for each of the four environment variables.\n\n\n\n\n\n\nNote\n\n\n\nEveryone in your team that wishes to connect to the SQL Warehouse in Databricks and run your code must set up their .Renviron file individually, otherwise their connection will fail.\n\n\nOnce you have entered the details, save and close your .Renviron file and restart R (Session &gt; Restart R).\n\n\nDatabricks host\n\nThe Databricks host is the instance of Databricks that you want to connect to. It’s the URL that you see in your browser bar when you’re on the Databricks site and should end in “azuredatabricks.net” (ignore anything after this section of the URL).\n\n\n\nDatabricks SQL Warehouse Path\n\nAs described in the SQL Warehouses section, in Databricks, SQL Warehouses are a way to gain access to your data in the Unity Catalog. They run queries and return the results either to the user or to a table.\nTo get the Warehouse ID, follow these steps:\n\nclick ‘SQL Warehouses’ under the ‘SQL’ section of the left hand menu on Databricks\nclick on the warehouse name that you’d like to get the ID for\nthe warehouse id is the ‘HTTP Path’ in the ‘Connection details’ tab\nthe ID should start with something similar to “/sql/1.0/warehouses/”\n\n\n\n\nDatabricks token\n\nThe Databricks token is a personal access token.\nA personal access token is is a security measure that acts as an identifier to let Databricks know who is accessing information from the SQL warehouse. Access tokens are usually set for a limited amount of time, so they will need renewing periodically.\n\nIn Databricks, click on your user icon in the top right corner, then click ‘Settings’\nGo to the ‘Developer’ tab in the side bar. Next to ‘Access tokens’, click the ‘Manage’ button\n\n\n\n\n\nClick the ‘Generate new token’ button\nName the token, then click ‘Generate’\n\n\n\n\n\n\n\nNote\n\n\n\nNote that access tokens will only last as long as the value for the ‘Lifetime (days)’ field. After this period the token will expire, and you will need to create a new one to re-authenticate. Access tokens also expire if they are unused after 90 days. For this reason, we recommend setting the Lifetime value to be 90 days or less.\n\n\n\nMake a note of the ‘Databricks access token’ it has given you\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is very important that you immediately copy the access token that you are given, as you will not be able to see it through Databricks again. If you lose this access token before pasting it into RStudio then you must generate a new access token to replace it.\n\n\n\n\n\n\nPulling data into RStudio from Databricks\n\nNow that you have enabled ODBC connections on your laptop, and enabled a connection between Databricks and RStudio, you can add code to your existing scripts to pull data into RStudio for analysis. If you have connected to other SQL databases before, this code will look quite familiar to you.\nTo access the data, we will make use of the odbc package. You can find documentation about this package on the Posit website. You will also need to have the DBI package installed.\nInclude the following code in your R Script:\n\nlibrary(odbc)\nlibrary(DBI)\n\ncon &lt;- DBI::dbConnect(\n  odbc::databricks(),\n  httpPath = Sys.getenv(\"DATABRICKS_SQL_PATH\")\n)\n\nodbcListObjects(con)",
    "crumbs": [
      "Databricks Setup Guides",
      "SQL Warehouse with RStudio"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster_sparklyr.html",
    "href": "ADA/databricks_rstudio_personal_cluster_sparklyr.html",
    "title": "Personal cluster with RStudio and sparklyr",
    "section": "",
    "text": "Important\n\n\n\nPlease be aware that the Databricks platform is regularly updated and may look different from the guidance included on this site. If you notice any discrepancies between the content on this site and the Databricks platform, please let us know by contacting statistics.development@education.gov.uk.\nThe following instructions set up a sparklyr connection between your laptop and your Databricks cluster, which can then be used in RStudio to query data using tidyverse syntax.\nTo use sparklyr you will need a personal cluster set up on Databricks. They can be used within the Databricks environment, or through RStudio. You can set one up yourself if you don’t have access to one already.\nWithin the Databricks environment they are able to use SQL, R, Python and Scala, and are more flexible than SQL Warehouses. From RStudio you will be able to use this method to execute queries using R with the performance benefits provided by Databricks.\nYou can use data from Databricks with R code in two different ways:",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `sparklyr`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#pre-requisites",
    "href": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#pre-requisites",
    "title": "Personal cluster with RStudio and sparklyr",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou must have:\n\nAccess to Databricks and the data you’ll be working with\nAccess to a personal cluster on Databricks\nR and RStudio downloaded and installed on your laptop\nRTools installed on your laptop (available from the Software Centre)\nGit installed on your laptop\nMembership of the active directory group ‘AZURE INTERNET EXCLUDE INSPECTION PA’\n\nCurrently this involves logging an ‘Access to Restricted Group’ IT ticket through the ServiceNow portal",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `sparklyr`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#compute-resources",
    "href": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#compute-resources",
    "title": "Personal cluster with RStudio and sparklyr",
    "section": "Compute resources",
    "text": "Compute resources\nWhen your data is moved to Databricks, it will be stored in the Unity Catalog and you will need to use a compute resource to access it from other software such as RStudio.\nA compute resource allows you to run your code using cloud computing power instead of using your laptop’s processing power. This means that using compute resources can allow your code to run faster than it would if you ran it locally, as it is like using the processing resources of multiple computers at once. On this page, we will be referring to the use of personal clusters as the compute resource to run your code.\n\n\nPersonal clusters\n\nA personal cluster is a compute resource that supports the use of multiple code languages (R, SQL, Scala and Python) in the Databricks environment and can be set up to connect to RStudio as well. You can create your own personal cluster within the Databricks interface.\nWhen you set up your personal cluster, you will be asked to select a runtime for that cluster. Different runtimes allow you to use different features and package versions. Certain packages are installed by default on a personal cluster and do not need to be installed manually. The specific packages installed are based on the Databricks Runtime (DBR) version your cluster is set up with. A comprehensive list of packages included in each DBR is available in the Databricks documentation. Generally speaking a higher DBR version number will provide more functionality.\nCompute resources, including personal clusters, have no storage of their own. This means that if you install libraries or packages onto a cluster they will only remain installed until the cluster is stopped. Once re-started those libraries will need to be installed again.\nAn alternative to this is to specify packages / libraries to be installed on the cluster at start up. To do this click the name of your cluster from the ‘Compute’ page, then go to the ‘Libraries’ tab and click the ‘Install new’ button.\n\n\n\n\n\n\nClusters will shut down after being idle for an hour\n\n\n\nUse of compute resources are charged by the hour, and so personal clusters have been set to shut down after being unused for an hour in order to prevent unnecessary cost to the Department.",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `sparklyr`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#process",
    "href": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#process",
    "title": "Personal cluster with RStudio and sparklyr",
    "section": "Process",
    "text": "Process\nThere are four steps to complete before your connection can be established. These are:\n\nCreating a personal compute resource (if you do not already have one)\nInstalling the reticulate, pysparklyr and sparklyr packages\nModifying your .Renviron file to establish a connection between RStudio and Databricks\nAdding connection code to your existing scripts in RStudio\n\n\n\nCreating a personal compute resource\n\n\nTo create your own personal compute resource click the ‘Create with DfE Personal Compute’ button on the compute page\n\n\n\n\nYou’ll then be presented with a screen to configure the cluster. There are 2 options here under the performance section which you will want to pay attention to; Databricks runtime version, and Node type\n\nDatabricks runtime version - This is the version of the Databricks software that will be present on your compute resource. Generally it is recommended you go with the latest LTS (long term support) version. At the time of writing this is ‘15.4 LTS’\n\nNode type - This option determines how powerful your cluster is and there are 2 options available by default:\n\n\nStandard 14GB 4-Core Nodes\n\nLarge 28GB 8-Core Nodes\n\nIf you require a larger personal cluster this can be requested by the ADA team.\n\n\n\nClick the ‘Create compute’ button at the bottom of the page. This will create your personal cluster and begin starting it up. This usually takes around 5 minutes\n\n\nOnce the cluster is up and running the icon under the ‘State’ header on the ‘Compute’ page will appear as a green tick",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `sparklyr`"
    ]
  },
  {
    "objectID": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#setting-up-the-odbc-driver",
    "href": "ADA/databricks_rstudio_personal_cluster_sparklyr.html#setting-up-the-odbc-driver",
    "title": "Personal cluster with RStudio and sparklyr",
    "section": "Setting up the ODBC driver",
    "text": "Setting up the ODBC driver\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously set up an ODBC connection, or followed the set up Databricks SQL Warehouse with RStudio guidance, then you can skip this step.\n\n\n\nOpen the Software Centre via the start menu\nIn the ‘Applications’ tab, click Simba Spark ODBC Driver 64-bit\n\n\n\n\n\nClick install\n\n\n\nEstablishing an RStudio connection using environment variables\n\nThe sparklyr package in RStudio allows you to connect to Databricks by creating and modifying three environment variables in your .Renviron file.\n\n\n\n\n\n\nNote\n\n\n\nIf you have previously established a connection between a SQL Warehouse or personal cluster and RStudio, then some of these variables will already be in your .Renviron file.\n\n\nTo set the environment variables, call usethis::edit_r_environ(). You will then need to enter the following information:\nDATABRICKS_HOST = \"databricks-host\"\nDATABRICKS_CLUSTER_ID = \"databricks-cluster-id\"\nDATABRICKS_TOKEN = \"personal-access-token\"\nOnce you have entered the details, save and close your .Renviron file and restart R (Session &gt; Restart R).\n\n\n\n\n\n\nNote\n\n\n\nEveryone in your team that wishes to connect to the data in Databricks and run your code must set up their .Renviron file individually, otherwise their connection will fail.\n\n\nThe sections below describe where to find the information needed for each of the environment variables.\n\n\nDatabricks host\n\nThe Databricks host is the instance of Databricks that you want to connect to. It’s the URL that you see in your browser bar when you’re on the Databricks site and should end in “azuredatabricks.net” (ignore anything after this section of the URL).\n\n\n\nDatabricks cluster id\n\nIn Databricks, go to Compute in the left hand menu, and click on the name of your personal cluster:\n\n\n\nOn the Configuration tab, take the code between clusters/ and ? in the page URL. This is the ID for your personal cluster.\n\n\n\nDatabricks token\n\nThe Databricks token is a personal access token.\nA personal access token is is a security measure that acts as an identifier to let Databricks know who is accessing information from the personal cluster. Access tokens are usually set for a limited amount of time, so they will need renewing periodically.\n\nIn Databricks, click on your email address in the top right corner, then click ‘User settings’\nGo to the ‘Developer’ tab in the side bar. Next to ‘Access tokens’, click the ‘Manage’ button\n\n\n\n\n\nClick the ‘Generate new token’ button\nName the token, then click ‘Generate’\n\n\n\n\n\n\n\nNote\n\n\n\nNote that access tokens will only last as long as the value for the ‘Lifetime (days)’ field. After this period the token will expire, and you will need to create a new one to re-authenticate. Access tokens also expire if they are unused after 90 days. For this reason, we recommend setting the Lifetime value to be 90 days or less.\n\n\n\nMake a note of the ‘Databricks access token’ it has given you\n\n\n\n\n\n\n\nWarning\n\n\n\nIt is very important that you immediately copy the access token that you are given, as you will not be able to see it through Databricks again. If you lose this access token before pasting it into RStudio then you must generate a new access token to replace it.\n\n\n\n\n\n\nSetting up your sparklyr connection\n\nTo connect to Databricks through sparklyr you will first need to install other packages which it depends on. The following steps are only required once, and once you have successfully connected you will only need to use the code from the ‘Using sparklyr to access and navigate data’ section below.\n\nReticulate and python\nBehind the scenes sparklyr converts your queries to python when interfacing with Databricks. The reticulate package is used for managing python environments through R. It can be installed using the following line of code.\ninstall.packages(\"reticulate\")\nYou will also need to use reticulate to install a python environment in the background, the version of python you install should be higher than 3.10. The version is passed to the function as a string as below.\nreticulate::install_python(\"3.10\")\nThis step can take quite a while to complete. If you get an error here please consult the ‘Troubleshooting’ section below.\nTo confirm that python has installed successfully you can use the function reticulate::py_version() to check the version number of python you are running.\n\n\nPysparklyr\nThe second dependency is pysparklyr, however currently there is a bug in the main version of pysparklyr that causes it to fail when building a python environment for Databricks on Windows machines.\nDue to this you will need to install the development version from GitHub. We can do this using the remotes package using the code below.\nremotes::install_github(\"mlverse/pysparklyr\")\n\n\n\n\n\n\nNote\n\n\n\nIf you get an Error in loadNamespace(x) : there is no package called 'remotes' this means the remotes package isn’t installed. Use install.packages(\"remotes\") to install it and re-run the command above.\n\n\nThe installation may prompt you to update a number of packages. If this happens enter 1 (to update all packages) into the prompt.\n\n\n\n\n\n\nWarning\n\n\n\nIn this step there is often an issue where the curl package will not update and it will restore the previous version. Unfortunately this will cause issues as pysparklyr depends on functions from a later version of curl.\nIf this occurs use the function remove_packages(\"curl\") to manually uninstall it, then run install.packages(\"curl\") to install the latest version.\n\n\n\n\nSparklyr\nFinally we can install the sparklyr package.\ninstall.packages(\"sparklyr\")\n\n\n\n\nUsing sparklyr to access and navigate data\n\nNow that you have all the sparklyr dependencies set up on your laptop, and your environmental variables setup, you can add code to your existing scripts to pull data into RStudio for analysis. If you have connected to databases before, this code will look quite familiar to you.\n\nConnect to Databricks through sparklyr\nInclude the following code in your R Script, the query below will show the names of the catalogs available to you.\n\n\n\n\n\n\nNote\n\n\n\nIf your personal cluster hasn’t been started yet it will automatically begin booting up when you run the code below. If so, expect it to take a few minutes before you get any results as the cluster will have to start up first.\nIf your cluster is already running it should begin executing your queries immediately.\n\n\nlibrary(sparklyr)\n\n\nsc &lt;- spark_connect(\n  cluster_id = Sys.getenv(\"DATABRICKS_CLUSTER_ID\"),\n  method = \"databricks_connect\"\n)\n\nsdf_sql(sc, \"SHOW CATALOGS;\")\nFrom here you can use sdf_sql() to execute queries on your data. The first argument is always the spark connection sc, followed by the query you want to execute.\n\n\nNavigating catalogs and schema\nYou can refer to a table or view with it’s full three name reference catalog_name.schema_name.table_name, however always using the three names has drawbacks.\nFirstly the names can get quite long due to naming conventions, but more importantly it can create more code maintenance. For example, if you wanted to recreate a query in a different schema you would have to go and edit every table reference.\nInstead you can tell Databricks to use a specific catalog and all further queries through that spark connection will be executed within that catalog. This means you no longer have to specify the catalog when you refer to a schema, table or view.\nsdf_sql(sc, \"USE CATALOG catalog_40_copper_analyst_training;\")\n\n\n\n\n\n\nNote\n\n\n\nYou can also do this with the schema with USE schema_name; as the query. This would mean you can reference tables and views within that schema with just their name.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you set a catalog and schema for Databricks to use, but also want to refer to data in another catalog or schema you can still use catalog_name.schema_name.table_name or schema_name.table_name to access it.\n\n\n\n\nOverview of tables and columns\nLike SQL Server every catalog has an information_schema which contains all of the metadata about the tables, columns, etc.\nTo look at in all the tables and views in any data catalog you can use the following code, which will store the result in a variable called tables.\ntables &lt;- sdf_sql(sc, \"SELECT * FROM information_schema.tables;\")\nIf you View(tables) now you’ll see that it is not currently a data frame but a list with several entities in it. This is because sparklyr is storing the connection to the data, along with any instructions on how to transform it rather than storing the data itself. This allows it to pass all of the instructions to Databricks so that it can do the heavy lifting, and only bring back the data required into R memory.\nSince we don’t need to do any transformations here, we can simply tell sparklyr to collect() the data, which will turn it into an R data.frame.\ntables_df &lt;- tables %&gt;% collect()\nView(tables_df)\nSimilarly you can look at the columns, data types, etc. within a catalog using the information_schema.\ncolumns_df &lt;- sdf_sql(sc, \"SELECT * FROM information_schema.columns;\") %&gt;% collect()\nView(columns_df)\n\n\n\n\nTroubleshooting\n\n\nPython won’t install\n\nThe most likely reason that python would fail to install is that you haven’t yet been granted access to the AD group ‘AZURE INTERNET EXCLUDE INSPECTION PA’ yet.\nIf this is the case you will likely see a long error message which will contain the text SSL: CERTIFICATE_VERIFY_FAILED (you may need to scroll across the error message to see this as it’s quite chunky).\nTo fix this issue submit an IT service desk ticket requesting access to the group above and retry once your access has been granted.\n\n\n\nUsing spark_connect() throws error: “‘curl_parse_url’ is not an exported object from namespace:curl”\n\nThis error appears when your version of the curl package is too old to have the functions that sparklyr depends on.\nTo uninstall the old version and update it use the following code and then re-run spark_connect().\nremove.packages(\"curl\")\ninstall.packages(\"curl\")\n\n\n\nUsing spark_connect() throws error: “Databricks connect is not available in the current Python environment”\n\nThis error is usually caused by having another python environment already loaded in your R session.\nTo resolve this restart your R session (Session &gt; Restart R) then re-run the spark_connect() function.\n\n\n\nUsing spark_connect() fails to create virtual environment\n\nIf when you run spark_connect() for the first time it fails to create a virtual environment and throws an error similar to below:\nError: Error installing package(s): \"\\\"databricks-connect==16.1.0\\\"\", \"\\\"pandas!=2.1.0\\\"\", \"PyArrow\", \"grpcio\", \"google-api-python-client\", \"grpcio_status\", \"rpy2\"\nThe most likely cause is that you’re using the official pysparklyr package rather than the development version. To fix this use the following code and rerun the spark_connect() function.\nremove.packages(\"pysparklyr\")\nremotes::install_github(\"mlverse/pysparklyr\")\n\n\n\nI’m having another problem with virtual environments\n\nThere are a number of issues that can arise when sparkylr creates virtual environments, and sometimes trying to work out what is wrong is quite difficult.\nThe easiest way to resolve the issue is usually to remove the virtual environments and let sparklyr create them again.\nThere are 2 methods of doing this, the first uses the reticulate package to view and remove them. First we can list what virtual environments exist using the code below.\nreticulate::reticulate::virtualenv_list()\n#output: \"r-sparklyr-databricks-16.1\"\nWe can then remove that environment\nreticulate::virtualenv_remove(\"r-sparklyr-databricks-16.1\") \n#where \"r-sparklyr-databricks-16.1\" is what is returned from the code above\nOnce you’ve done this you can re-run spark_connect() and it should recreate the environment.\nIf for some reason removing the environment through the command line doesn’t work we can delete them using file explorer. However, first we need to find where they are kept. We can do this with the following line of code which will print the full system path of any virtual environments that exist on your machine.\nlist.files(reticulate::virtualenv_root(), full.names = T)\nOnce you have the path you can navigate to the .virtualenvs folder and manually delete the folder within it that corresponds to the name of the faulty environment. You can then re-run spark_connect() and it will rebuild it from scratch.",
    "crumbs": [
      "Databricks Setup Guides",
      "Personal cluster with RStudio and `sparklyr`"
    ]
  },
  {
    "objectID": "ADA/databricks_workflows.html#workflows-user-interface",
    "href": "ADA/databricks_workflows.html#workflows-user-interface",
    "title": "Databricks workflows",
    "section": "Workflows user interface",
    "text": "Workflows user interface\nWorkflows allow you to build complex data pipelines by chaining together multiple scripts, queries, notebooks and logic. They can be used to build Reproducible Analytical Pipelines (RAP) that can be re-run with different parameters and have all inputs and outputs audited automatically. Other recommended uses of workflows are any data modelling tasks such as cleaning your source data and collating it into a more analytically friendly format in your modelling area.\nEach step in a workflow is referred to as a task and each task has dependencies. They are accessible through the ‘Workflows’ link on the left hand menu of the Databricks UI.\n\nParameters can be set either at a workflow level or a task level and referred to in your scripts / notebooks, allowing you to reuse tasks / workflows for similar operations.\nEach task can have dependencies on other tasks and can be set to only run under certain conditions, for example all of the previous tasks have completed successfully. These can be configured when tasks are added to the workflow through the user interface.\nWorkflows and tasks can also be configured to send notifications to users upon success or failure. These can be configured from the Workflow and Task user interfaces.\nWorkflows also come with robust support for GitHub and Azure DevOps repositories and can be set to run from a specific repo, branch, commit or tag.\n\n\nSetting up a workflow\n\n\nYou can create a new workflow from the Workflow page by clicking the ‘Create job’ button in the top right of the screen.\n\n\n\nYou will then be presented with a page titled ‘New Job &lt;timestamp&gt;’ which you can edit to give the workflow a meaningful name.\n\n\n\n\n\n\n\nThe ‘Job Details’ pane on the right allows you to configure workflow level schedules and triggers, parameters which will be accessible to all tasks in the workflow, email notifications for successful / failed runs, and permissions on who can access and run the workflow. You can also add tags and descriptions to your workflow to help you keep track of them.\n\nIn the task pane you can configure settings at an individual task level, including a unique name, the type of task (notebook, query, script, etc.) the source (your workspace, or a Git repository), the path to the code for the task and the compute resource the workflow should run on.\n\nAny libraries or packages that the workflow depends on can be specified and these will be installed on the cluster by Databricks when it begins (not applicable to SQL warehouses).\n\nYou are also able to set task level parameters here which mean that you are able to reuse the same notebook for different tasks by setting different inputs. This allows you to create notebooks that serve as ‘functions’ from R or Python, or ‘stored procedures’ from SQL.\n\nHere you can also setup notifications for the specific task. In addition you can specify the number of times a task should be retried if it fails, and a duration threshold for the task before forcing it to fail.\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\n\n\nThe source by default is set to your workspace, but it is recommended that you use a version controlled Git repository instead. This prevents you changing the code of a notebook during a workflow run as the tasks are sourced from a specific repository version, branch, commit or tag rather than a workbook you may be working on.\n\n\n\n\n\n\n\n\nCompute\n\n\n\nDuring active development it is probably easiest to use your personal compute or a SQL warehouse you have access to.\nOnce development is complete and the workflow becomes business as usual it may be worth requesting a specific job cluster to be created by the ADA team to be solely responsible for running the workflow.\n\n\n\nClick the ‘Create task’ button to save the details. After this you will be able to add additional tasks.\nOnce you begin creating a second task all tasks will now have 2 additional configurations; ‘depends on’, and ‘run if dependencies’.\n\nDepends on - a list of tasks that must be run before the start of this task\nRun if dependencies - Instructions for the conditions to run the task based on the dependencies set above. The default option is ‘All succeeded’ but there are also the following options:\n\n\nAt least one succeeded\n\nNone failed\n\nAll done\n\nAt least one failed\n\nAll failed\n\n\n\nAfter setting up a flow of tasks you will be presented with a graphical presentation of the workflow as seen below:\n\n\n\n\n\nAuditing\n\nEach time a workflow is run, Databricks audits:\n- any input parameters\n- all outputs\n- the success and failure of each task\n- when it was run and who by\n\nThis makes workflows a very powerful debugging tool as you can refer back to results from previous runs. This means that if your pipeline fails you can review the notebook(s) that failed and troubleshoot the issue.\nWorkflows that fail also allow you to repair the workflow once you have found and fixed the issue. This prevents having to re-run the whole pipeline from scratch and allows it to pick up from the point where it failed.",
    "crumbs": [
      "Learning resources",
      "Databricks workflows"
    ]
  },
  {
    "objectID": "ADA/databricks_workflows.html#coded-workflows",
    "href": "ADA/databricks_workflows.html#coded-workflows",
    "title": "Databricks workflows",
    "section": "Coded workflows",
    "text": "Coded workflows\nAnother useful aspect of workflows is that they can be defined and ran using code through the DataBricks Jobs API.\nThere is an R library which has been created to interface with the DataBricks API, meaning that you can script jobs in R using the DataBricks SDK for R package using lists instead of JSON.\nFor instructions on how to use the Databricks SDK for R package to script workflows see the articles below:\n\nScripting workflows in Databricks\nScripting workflows in RStudio",
    "crumbs": [
      "Learning resources",
      "Databricks workflows"
    ]
  }
]