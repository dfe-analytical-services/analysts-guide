---
title: "RAP case studies"
format: html
---

------------------------------------------------------------------------

<p class="text-muted">Case studies which illustrate how RAP principles have been applied in practice.</p>

---

## RAP case studies

Each case study provides an overview of the project, the context and background, and how the RAP principles were implemented.

These case studies serve as practical examples to help you understand how to apply RAP principles in your own work. They demonstrate the benefits of following these principles, such as improved data quality, enhanced reproducibility, and more efficient workflows. We would like to thank the RAP Champions Network for providing these case studies. 

### Case Study 1: Function to group any variable in a data set

#### Analysis Type

Ad-hoc analysis, likely to be repeated e.g., different breakdowns

#### Context and Background 

We collect data on the FE workforce. Each row of data is a person. We collect data on their characteristics, pay, subjects taught and roles. For our publication and ad-hoc analysis, we frequently need to know the number of staff split by various characteristics.

#### Process and principles applied 

I created a function called “create grouped totals and rates”. This function groups by the inputs input. It creates totals at the provider level, LA level, regional level and national level. It allows staff to be summed by their headcount or the FTE. The function was QA-d at length to ensure that the output was correct. The function is now used to produce any total for the publication, as well as any totals that are needed for internal ad-hoc analysis.

Some measures e.g. disability status, have a prefer not to say option. However, for some of these measures, we show percentages only for staff who have chosen to disclose. E.g. the percentage of staff who have answer “Yes”, “No” should sum to 100%. This function takes this into account. If a measure should include/not include the “prefer not to say” option in the percentages, the function is dynamic like this.

#### Outcome/results 

This function improved quality. We have QA-d the function and the raw data entering the function. This means that the output produced from the function for ad-hoc analysis only needs a smaller QA, since it has come from a repeatable process.

This function has saved time for our team. It has meant that almost any question asked about total number of staff that fit a certain criteria and geography can be quickly created.

E.g. if I want to know the total number of staff in each role and each provider type with each gender, I would use the function like this:

![](/images/rap-case-study-1.png){fig-alt="Screenshot of R code showing the use of the create_grouped_totals_and_rates function to create the workforce by provider type, role and gender."}

It would create an output across all the geographies specified above. It would show the % of each staff by role and provider type, who have each gender.

### Case Study 2: High Needs National Funding Formula 

#### Analysis Type

Spreadsheet based analysis, analysis repeated on different datasets

#### Context and Background 

Each year, the High Needs NFF is used to allocate funding for children with special educational needs (SEN) and children in alternative provision (AP). The funding is allocated at local authority level, using a range of different datasets that are proxies for SEN and AP.


#### Process and principles applied 

**Preparing data**

•	Data is either sourced from published files, or requested from other teams in DfE.

•	Data is stored in a suitable file system. We have a different folder for each dataset, where we store the data, and also carry out some sense checks before inputting the data into the model.

•	A data log is kept up to date to keep track of what has been sourced and what is still to be updated.

•	Data updates are done one at a time, so that the impact of each data update can be properly assessed.

**Writing code** 

•	The publication includes a step-by-step of the calculation, so using Excel allows us to very clearly set out each step of the process.

•	Within the model, every dataset is referenced using links to where the data is stored or where it is sourced from.

•	The model is dual built by a third party (an analyst outside of our team who has no prior experience of High Needs allocations). They follow our technical note, source the data themselves, and reproduce our outputs, which we make sure reconcile to at least 5 decimal places at all stages of the calculation. This dual build not only QAs our outputs, but QAs our technical note as well.

•	Automated checks are built in to make sure that totals are as we would expect, as shown below, and other checks ensure there are no zeros in the data.

![](/images/rap-case-study-hnnff-b.png){fig-alt="A screenshot of the QA dashboard, which checks that allocations match funding quantum."}

•	We have thorough documentation, including a technical note which is published, a data specification, and a user guide.

•	There is a complete model map as shown below, which details how each sheet of the Excel model links together.

![](/images/rap-case-study-hnnff-c.png){fig-alt="A screenshot of the model map which helps to demonstrate how each Excel sheet feeds into the next."}

**Version Control** 

•	A weekly plan is kept up to date with our progress, to help us in tracking back any errors.

#### Outcome/results 

•	The clear file structure means that updates can be carried out quickly and smoothly, easily passed between different team members depending on who is available.

•	The version control is thorough yet concise, so easy to follow if we need to roll back to an earlier version.

•	The fact that the model is successfully dual built means that the model is reproducible. This is due to our extensive documentation, as well as a great deal of planning to ensure the model is dual built in time.

•	Automated QA checks is a huge time saver and gives us extra security that there are no mistakes.

•	The Schools National Funding Formula undergoes a similar process – but the model is built in R instead. One of the reasons for this is that the Schools NFF calculations are done at school level rather than LA level, so there is a lot more data. If the High Needs NFF were to change to allocate at school level instead, then this would be an appropriate time to replatform the model to R.

