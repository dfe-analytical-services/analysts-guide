---
title: "RAP for general analysis"
format: html
---

------------------------------------------------------------------------

<p class="text-muted">Guidance for how to implement the principles of Reproducible Analytical Pipelines (RAP) into general analysis processes</p>

---

If you're unfamiliar with RAP, or want a refresher on what RAP is and why it matters, start with our [What is RAP? page](../rap/what-is-rap.html). It covers the background and benefits of RAP, as well as an introduction RAP principles. 

------------------------------------------------------------------------

### Getting started with RAP for analysis

------------------------------------------------------------------------

To get started with RAP for analysis, start by exploring the RAP principles in the diagram below, which outline what good, great and best practice looks like.

Once you are familiar with the principles, take a look at the different types of analysis commonly carried out across the department. This will help you understand which RAP principles are recommended for your type of analysis.

Implementing RAP may involve combining the use of SQL, R and using clear, consistent version control to increase efficiency and accuracy in our work. For more information on what these tools are, why we are using them, and resources to help upskill in those areas, see our [learning resources](../learning-development/learning-support.html) page.

------------------------------------------------------------------------

### Applying RAP principles proportionally

------------------------------------------------------------------------

RAP should be **proportional** to the analysis in the same way that [QA is proportional](https://educationgovuk.sharepoint.com/sites/sarpi/g/SitePages/Clearance-of-analysis.aspx).

Applying RAP pinciples proportionately means tailoring your approach to the scale, complexity and potential impact of the analysis, just as you would with QA. Not every piece of work will require a fully automated pipeline. but all analysis should aim to be reproducible where possible. 

When deciding what RAP principles to apply, consider factors like who will use the analysis, how it will be used, the level of public or ministerial interest and the potential risks involved. Use the guidance below as a starting point, but apply judgement based on the specific context of your work, as you would when determining proportionate QA. 


For example, you might not create a full RAP process for an ad hoc piece of work, you could still version control your code so that it could be reused if similar requests came in, and you should get your code peer reviewed by someone before sending out any results.

Expectations for statistics production are clearer because these outputs are published regularly and follow consistent processes (see the [RAP in Statistics page](RAP/rap-statistics.html) for more information).

------------------------------------------------------------------------

## RAP for analysis principles

------------------------------------------------------------------------

The diagram below highlights what RAP means for us, and the varying levels at which it can be applied across all types of analysis. You can click on each of the hexagons in the diagram to learn more about each of the RAP principles and how to use them in practice.

<!-- draw.io diagram -->
<div class="mxgraph" style="max-width:100%;" data-mxgraph="{&quot;lightbox&quot;:false,&quot;nav&quot;:true,&quot;dark-mode&quot;:&quot;auto&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; agent=\&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36 Edg/141.0.0.0\&quot; version=\&quot;28.2.5\&quot;&gt;\n  &lt;diagram id=\&quot;_ltWYIdbVKAG_Xyvna0Y\&quot; name=\&quot;Page-1\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1818\&quot; dy=\&quot;756\&quot; grid=\&quot;0\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;0\&quot; fold=\&quot;1\&quot; page=\&quot;0\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; background=\&quot;none\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;UserObject label=\&quot;Sensible folder and file structure\&quot; link=\&quot;../RAP/rap-start-guide.html#sensible-folder-and-file-structure\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-5\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fontColor=#FFFFFF;strokeColor=none;fillColor=#4472C4;spacingLeft=7;spacingRight=7;movable=1;resizable=1;rotatable=1;deletable=1;editable=1;connectable=1;fontSize=14;locked=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;69\&quot; y=\&quot;326\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Use appropriate tools e.g., R, SQL, Python\&quot; link=\&quot;../RAP/rap-start-guide.html#use-appropriate-tools\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-6\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fontColor=#FFFFFF;fillColor=#4472C4;spacingLeft=7;spacingRight=7;rounded=0;shadow=0;sketch=0;backgroundOutline=0;gradientDirection=north;strokeColor=none;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;-55\&quot; y=\&quot;326\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Processing is done with code, where possible\&quot; link=\&quot;../RAP/rap-start-guide.html#processing-is-done-with-code\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-7\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fontColor=#FFFFFF;strokeColor=none;fillColor=#4472C4;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;4\&quot; y=\&quot;421\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Automated high level checks\&quot; link=\&quot;../RAP/rap-start-guide.html#basic-automated-qa\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-8\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fontColor=#FFFFFF;strokeColor=none;fillColor=#4472C4;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;69\&quot; y=\&quot;514\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Documentation\&quot; link=\&quot;../RAP/rap-start-guide.html#documentation\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-11\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fontColor=#FFFFFF;strokeColor=none;fillColor=#4472C4;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;-58\&quot; y=\&quot;515\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Version controlled final code scripts\&quot; link=\&quot;../RAP/rap-start-guide.html#version-controlled-final-code-scripts\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-12\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#70AD47;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;320\&quot; y=\&quot;327\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Recyclable code for future use\&quot; link=\&quot;../RAP/rap-start-guide.html#recyclable-code-for-future-use\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-13\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#70AD47;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;195\&quot; y=\&quot;326\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Peer code review\&quot; link=\&quot;../RAP/rap-start-guide.html#dataset-production-scripts\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-14\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#70AD47;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;257\&quot; y=\&quot;421\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Project specific automated sense checks\&quot; link=\&quot;../RAP/rap-start-guide.html#publication-specific-automated-qa\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-15\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#70AD47;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;197\&quot; y=\&quot;517\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;&amp;#39;Clean&amp;#39; final code\&quot; link=\&quot;../RAP/rap-start-guide.html#clean-final-code\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-20\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#ED7D31;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;arcSize=20;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;556\&quot; y=\&quot;327\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;&amp;lt;div style=&amp;quot;&amp;quot;&amp;gt;&amp;lt;font style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;Whole pipeline can be run from a single script or workflow&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;\&quot; link=\&quot;../RAP/rap-start-guide.html#whole-publication-production-scripts\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-21\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#ED7D31;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;align=center;arcSize=20;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;497\&quot; y=\&quot;424\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Automated reproducible reports\&quot; link=\&quot;../RAP/rap-start-guide.html#use-open-source-repositories\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-22\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#ED7D31;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;arcSize=20;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;623\&quot; y=\&quot;424\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Peer review from outside the team\&quot; link=\&quot;../RAP/rap-start-guide.html#peer-review-of-code-from-outside-the-team\&quot; id=\&quot;t2OgNqa9NwPCjF_SyjkI-24\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#ED7D31;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;arcSize=20;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;562\&quot; y=\&quot;520\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;mxCell id=\&quot;t2OgNqa9NwPCjF_SyjkI-25\&quot; value=\&quot;&amp;lt;font style=&amp;quot;font-size: 32px&amp;quot; color=&amp;quot;#4472c4&amp;quot;&amp;gt;Good practice&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;fontColor=#FFFFFF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;-38\&quot; y=\&quot;652\&quot; width=\&quot;210\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;t2OgNqa9NwPCjF_SyjkI-26\&quot; value=\&quot;&amp;lt;font style=&amp;quot;font-size: 32px&amp;quot; color=&amp;quot;#70ad47&amp;quot;&amp;gt;Great practice&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;fontColor=#FFFFFF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;206\&quot; y=\&quot;652\&quot; width=\&quot;220\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;t2OgNqa9NwPCjF_SyjkI-27\&quot; value=\&quot;&amp;lt;font style=&amp;quot;font-size: 32px&amp;quot; color=&amp;quot;#ed7d31&amp;quot;&amp;gt;Best practice&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;fontColor=#FFFFFF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;572\&quot; y=\&quot;652\&quot; width=\&quot;200\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;S840XJ58BQc_PQPuIZfj-1\&quot; value=\&quot;\&quot; style=\&quot;shape=curlyBracket;whiteSpace=wrap;html=1;rounded=1;labelPosition=left;verticalLabelPosition=middle;align=right;verticalAlign=middle;fontSize=13;rotation=90;strokeWidth=2;strokeColor=#4472c4;gradientColor=none;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;168.5\&quot; y=\&quot;26\&quot; width=\&quot;47\&quot; height=\&quot;538\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;S840XJ58BQc_PQPuIZfj-2\&quot; value=\&quot;&amp;lt;font style=&amp;quot;font-size: 32px&amp;quot; color=&amp;quot;#4472c4&amp;quot;&amp;gt;Baseline expectation&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;fontColor=#FFFFFF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;35.5\&quot; y=\&quot;216\&quot; width=\&quot;313\&quot; height=\&quot;50\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;S840XJ58BQc_PQPuIZfj-4\&quot; value=\&quot;\&quot; style=\&quot;shape=curlyBracket;whiteSpace=wrap;html=1;rounded=1;labelPosition=left;verticalLabelPosition=middle;align=right;verticalAlign=middle;fontSize=13;rotation=90;strokeWidth=2;strokeColor=#ed7d31;gradientColor=none;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;627\&quot; y=\&quot;146\&quot; width=\&quot;47\&quot; height=\&quot;292\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;S840XJ58BQc_PQPuIZfj-5\&quot; value=\&quot;&amp;lt;font color=&amp;quot;#ed7d31&amp;quot; style=&amp;quot;font-size: 32px&amp;quot;&amp;gt;Ambition&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;fontColor=#FFFFFF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;583\&quot; y=\&quot;216\&quot; width=\&quot;143\&quot; height=\&quot;50\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;UserObject label=\&quot;Source data is acquired and stored sensibly\&quot; link=\&quot;../RAP/rap-start-guide.html#processing-is-done-with-code\&quot; id=\&quot;m64QDAUWRqu3AbltyGjd-1\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fontColor=#FFFFFF;strokeColor=none;fillColor=#4472C4;spacingLeft=7;spacingRight=7;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;129\&quot; y=\&quot;420\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n        &lt;UserObject label=\&quot;Collaboratively develop code using git\&quot; link=\&quot;../RAP/rap-start-guide.html#use-open-source-repositories\&quot; id=\&quot;Q9CUxytIHNbMGaBZu9zH-1\&quot;&gt;\n          &lt;mxCell style=\&quot;shape=hexagon;perimeter=hexagonPerimeter2;whiteSpace=wrap;html=1;fixedSize=1;rotation=90;size=30;direction=east;horizontal=0;fillColor=#ED7D31;strokeColor=none;fontColor=#FFFFFF;spacingLeft=7;spacingRight=7;fontSize=14;arcSize=20;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n            &lt;mxGeometry x=\&quot;682\&quot; y=\&quot;326\&quot; width=\&quot;120\&quot; height=\&quot;120\&quot; as=\&quot;geometry\&quot; /&gt;\n          &lt;/mxCell&gt;\n        &lt;/UserObject&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>


------------------------------------------------------------------------

## Choosing the right RAP principles for your analysis

------------------------------------------------------------------------

Whilst all RAP principles can be applied to any type of analysis, we recognise that different types of analysis conducted across the department may benefit from a tailored approach. To support this, weâ€™ve outlined a selection of common analysis types below, along with the RAP principles we suggest as essential and recommended for each. This guidance is intended to help teams prioritise their efforts and embed RAP in a way that is both practical and impactful.

::: {.callout-tip collapse="true"}
## Ad-hoc analysis (unlikely to be repeated)

Analysis that is performed outside of regular reporting processes, often to answer a specific need or question, but is unlikely to be requested again in any form.

We recommend the following principles for this type of analysis. Analysts should also consider the full set of principles for analysis and apply any others that are appropriate, useful, and proportionate to their project.

### Recommended principles

![](../images/good.svg)

-   Source data is acquired and stored sensibly
-   Sensible folder and file structure
-   Processing is done with code, where possible
-   Use of appropriate tools e.g., R, SQL, Python
-   Documentation

![](../images/great.svg)

-   Peer code review

![](../images/best.svg)

-   'Clean' final code
:::

::: {.callout-tip collapse="true"}
## Ad-hoc analysis (likely to be repeated e.g., different breakdowns)

Analysis that is performed outside of regular reporting processes, often to answer a specific need or question and is likely to be requested again in the same or a similar form.

We recommend the following principles for this type of analysis. Analysts should also consider the full set of principles for analysis and apply any others that are appropriate, useful, and proportionate to their project.

### Recommended principles

![](../images/good.svg)

-   Source data is acquired and stored sensibly
-   Sensible folder and file structure
-   Processing is done with code, where possible
-   Use of appropriate tools e.g., R, SQL, Python
-   Automated high level checks
-   Documentation

![](../images/great.svg)

-   Recyclable code for future use
-   Peer code review
-   Project specific automated sense checks
-   Version controlled final scripts

![](../images/best.svg)

-   'Clean' final code
:::

::: {.callout-tip collapse="true"}
## Internal regular reporting

Recurring analysis produced within the department to support ongoing operational needs. It often follows a consistent format and schedule, and is used to monitor key internal activities.

We recommend the following principles for this type of analysis. Analysts should also consider the full set of principles for analysis and apply any others that are appropriate, useful, and proportionate to their project.

### Recommended principles

![](../images/good.svg)

-   Source data is acquired and stored sensibly
-   Sensible folder and file structure
-   Processing is done with code, where possible
-   Use of appropriate tools e.g., R, SQL, Python
-   Automated high level checks
-   Documentation

![](../images/great.svg)

-   Recyclable code for future use
-   Peer code review
-   Project specific automated sense checks
-   Version controlled final scripts

![](../images/best.svg)
-   Whole pipeline can be run from a single script or workflow
-   'Clean' final code
-   Automated reproducible reports
-   Code collaboratively developed using Git
:::

::: {.callout-tip collapse="true"}
## Analysis at pace

Analysis which must be quickly conducted and often not pre-planned, aimed at delivering timely insights to support fast decision making.

We recommend the following principles for this type of analysis. Analysts should also consider the full set of principles for analysis and apply any others that are appropriate, useful, and proportionate to their project.

### Recommended principles

![](../images/good.svg)

-   Source data is acquired and stored sensibly
-   Sensible folder and file structure
-   Processing is done with code, where possible
-   Use of appropriate tools e.g., R, SQL, Python
-   Documentation

![](../images/great.svg)

-   Peer code review
:::

::: {.callout-tip collapse="true"}
## Analysis repeated on different datasets

This refers to the use of a consistent analytical approach applied to multiple datasets. Although the method remains the same, the data inputs vary e.g., by time period, breakdown or variable.

We recommend the following principles for this type of analysis. Analysts should also consider the full set of principles for analysis and apply any others that are appropriate, useful, and proportionate to their project.

### Recommended principles

![](../images/good.svg)

-   Source data is acquired and stored sensibly
-   Sensible folder and file structure
-   Processing is done with code, where possible
-   Use of appropriate tools e.g., R, SQL, Python
-   Automated high level checks
-   Documentation

![](../images/great.svg)

-   Recyclable code for future use
-   Peer code review
-   Project specific automated sense checks
-   Version controlled final scripts

![](../images/best.svg)

-   Whole pipeline can be run from a single script or workflow
-   'Clean' final code
-   Code collaboratively developed using Git
:::

::: {.callout-tip collapse="true"}
## Data collection and preparation

The collection of, and routine checking of data as it is coming into the department is an area that RAP can be applied to. However, the levels of control in this area vary wildly from team to team. If you would like advice and help to automate any particular processes, feel free to [contact the Statistics Development Team](mailto:statistics.development@education.gov.uk).

We recommend the following principles for this type of analysis. Analysts should also consider the full set of principles for analysis and apply any others that are appropriate, useful, and proportionate to their project.

### Recommended principles

![](../images/good.svg)

-   Source data is acquired and stored sensibly
-   Sensible folder and file structure
-   Processing is done with code, where possible
-   Use of appropriate tools e.g., R, SQL, Python
-   Automated high level checks
-   Documentation

![](../images/great.svg)

-   Recyclable code for future use
-   Peer code review
-   Version controlled final scripts

![](../images/best.svg)

-   'Clean' final code
-   Code collaboratively developed using Git
:::

<!-- Use DfT checklist to recreate one here!!! -->

------------------------------------------------------------------------

## How does RAP fit into ADA / Databricks

------------------------------------------------------------------------

Teams can use data held on the Databricks platform to implement RAP principles like version control, automated QA and automated pipelines. If you have an existing pipeline, please see the [ADA guidance on statistics publications](../ADA/ada.html#what-ada-means-for-statistics-publications) to see how migrating to ADA and Databricks will affect your work.

------------------------------------------------------------------------

## Core principles {#core-principles}

------------------------------------------------------------------------

RAP has three core principles:

[Preparing data](#preparing-data): Data sources for a publication are stored in the same database

[Writing code](#writing-code): Underlying data files are produced using code, with no manual steps

[Version control](#version-control): Files and scripts should be appropriately version controlled

Within each of these principles are separate elements of RAP. Each of these is discussed in detail below so that you know what is expected of you as an analyst.

------------------------------------------------------------------------

## Preparing data {#preparing-data}


Preparing data is our first core RAP principle, which contains the following elements:

-   [Source data is acquired and stored sensibly](#source-data-is-acquired-and-stored-sensibly)
-   [Sensible file and folder structure](#sensible-folder-and-file-structure)

Your team should store the internal raw data in a Microsoft SQL Server database or in the Databricks platform. These are similar to a Sharepoint area or a shared folder, but offer dedicated data storage areas and allow multiple users to use the same file at once. This means that you can also run code against a single source of data, further reducing the risk of error.

Where external data is used, the process of acquiring and storing the data should be well documented. The data should be stored appropriately in areas with the correct levels of security.

------------------------------------------------------------------------

{{< include common-rap-principles/source-data-acquired-sensibly.qmd >}}

------------------------------------------------------------------------

{{< include common-rap-principles/sensible-folder-and-file-structure.qmd >}}

------------------------------------------------------------------------

## Writing code {#writing-code}

------------------------------------------------------------------------

Writing code is our second core RAP principle, and is made up of the following elements:

-   [Processing is done with code](#processing-is-done-with-code)
-   [Use appropriate tools](#use-appropriate-tools)
-   [Whole pipeline run script](#whole-pipeline-run-script)
-   [Recyclable code for future use](#recyclable-code-for-future-use)
-   [Clean final code](#clean-final-code)
-   [Peer review of code within team](#peer-review-of-code-within-team)
-   [Peer review of code from outside the team](#peer-review-of-code-from-outside-the-team)
-   [Basic automated QA](#basic-automated-qa)
-   [More specific/advanced automated QA/summaries](#more-specificadvanced-automated-qasummaries)
-   [Automated outputs using markdown/Quarto](#automated-outputs-sing-markdownquarto)

::: callout-tip
The key thing to remember is that we should be automating everything we can, and the key to automation is writing code. Using code is as simple as telling your computer what to do. Code is just a list of instructions in a language that your computer can understand. We have links to many resources to help you learn to code on our [learning support page](/learning-development/learning-support.html).
:::

------------------------------------------------------------------------

{{< include common-rap-principles/processing-done-with-code.qmd >}}

------------------------------------------------------------------------

{{< include common-rap-principles/use-appropriate-tools.qmd >}}

------------------------------------------------------------------------

{{< include common-rap-principles/whole-pipeline-run-scripts.qmd >}}

------------------------------------------------------------------------

{{< include common-rap-principles/recyclable-code-for-future-use.qmd >}}

------------------------------------------------------------------------

### Clean final code {#clean-final-code}

------------------------------------------------------------------------

![](../images/best.svg)

**What does this mean?**

-   This code should meet the best practice standards below (for SQL and R). If you are using a different language, such as Python, then contact us for advice on the best standards to use when writing code.

-   There should be no redundant or duplicated code, even if this has been commented out. It should be removed from the files to prevent confusion further down the line.

-   The only comments left in the code should be those describing the decisions you have made to help other analysts (and future you) to understand your code. More guidance on [commenting in code](#commenting-in-code) can be found later on this page.

**Why do it?**

Clean code is efficient, easy to write, easy to review, and easy to amend for future use. Below are some recommended standards to follow when writing code in SQL and R.

**How to get started**

Watch [this coffee and coding session](https://web.microsoftstream.com/embed/video/624e3442-aa66-44e7-bb4f-717a6508b056) introducing good code practice, which covers:

-   key principles of good code practice
-   writing and refining code to make it easier to understand and modify
-   a real-life example of code improvement from within DfE

Then you should also watch [the follow up intermediate session](https://web.microsoftstream.com/embed/video/92d4ba05-d304-4009-b543-e5f4563f1d30), which covers:

-   version control
-   improving code structure with functions
-   documentation and Markdown
-   interactive notebooks

Clean code should include comments. Comment why you've made decisions, don't comment what you are doing unless it is particularly complex as the code itself describes what you are doing. If in doubt, more comments are better than too few though. Ideally any specific comments or documentation should be alongside the code itself, rather than in separate documents.

------------------------------------------------------------------------

#### SQL

------------------------------------------------------------------------

For best practice on writing T-SQL code used in SQL Server, here is a particularly useful [Word document](../resources/TSQL_Coding_Standards.docx){target="_blank" rel="noopener noreferrer"} produced by our Data Hub. This outlines a variety of best practices, ranging from naming conventions, to formatting your SQL code so that it is easy to follow visually.

------------------------------------------------------------------------

#### R

------------------------------------------------------------------------

When using R, it is generally best practice to use [R projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects){target="_blank" rel="noopener noreferrer"} as directories for your work.

The recommended standard for styling your code in R is the [tidyverse styling](https://style.tidyverse.org/){target="_blank" rel="noopener noreferrer"}, which is fast becoming the global standard. What is even better is that you can automate this using the [styler](https://styler.r-lib.org/){target="_blank" rel="noopener noreferrer"} package, which will literally style your code for you at the click of a button, and is well worth a look.

![](../images/styler.gif)

There is also plenty of guidance around the internet for [best practice](https://waterdata.usgs.gov/blog/intro-best-practices/){target="_blank" rel="noopener noreferrer"} when writing [efficient R code](https://waterdata.usgs.gov/blog/intro-best-practices/){target="_blank" rel="noopener noreferrer"}.

To help you standardise your code further, you can make use of the functions contained within our [dfeR package](https://dfe-analytical-services.github.io/dfeR/index.html). The package includes functions to standardise formatting and rounding, to pull the latest ONS geography lookups, and to create a pre-populated folder structure, amongst many other things.

------------------------------------------------------------------------

#### HTML

------------------------------------------------------------------------

If you ever find yourself writing HTML, or creating it through RMarkdown, you can check your HTML using [W3's validator](https://validator.w3.org/){target="_blank" rel="noopener noreferrer"}.

------------------------------------------------------------------------

### Peer review of code within team {#peer-review-of-code-within-team}

------------------------------------------------------------------------

![](../images/great.svg)

::: callout-note
'Testing by a second analyst' is part of the [DfE Mandatory QA checklist](https://educationgovuk.sharepoint.com/:w:/r/sites/sarpi/g/_layouts/15/Doc.aspx?sourcedoc=%7BEE1AF5FD-C4DA-4191-8E5E-285196D8237E%7D&file=mandatory_checklist.docx&action=default&mobileredirect=true).
:::

**What does this mean?**

Peer review is an important element of quality assuring our work. We often do it without realising by bouncing ideas off of one another and by getting others to 'idiot check' our work. When writing code, ensuring that we get our work formally peer reviewed is particularly important for ensuring it's quality and value. The [Duck Book](https://best-practice-and-impact.github.io/qa-of-code-guidance/peer_review.html) and [Tidyteam](https://code-review.tidyverse.org/) contain detailed guidance on peer review, but we have summarised some of the information here for you as well.

Prior to receiving code for peer review, the author should ensure that all code files are clean, commented appropriately and for larger projects should be held in a repo with an appropriate [README](#writing-a-readme-file) file.

You should check:

-   Is someone else in the team able to generate the same outputs?

-   Has someone else in the team reviewed the code and given feedback?

-   Have you taken on their feedback and improved the code?

**Why do it?**

There are many benefits to this, for example:

-   Ensuring consistency across the team

-   Minimizing mistakes and their impact

-   Ensuring the requirements are met

-   Improving code performance

-   Sharing of techniques and knowledge

**How to get started**

When peer reviewing code you should consider the following questions -

-   Do you understand what the code does? If not, is there supporting documentation or code comments that allow you to understand it?
-   Does the code do what the author intended?
-   Have any dependencies (either on separate pieces of code, data files, or packages) been documented?
-   Are there any tests / checks that could be added into the code that would help to give greater confidence that it is doing what it is intended to?
-   Are there comments explaining why any decisions have been made?
-   Is the code written and structured sensibly?
-   Are there any ways to make the code more efficient (either in number of lines or raw speed)? Is there duplication that could be simplified using functions?
-   Does the code follow best practice for styling and structure?
-   Are there any other teams/bits of code you're aware of that do similar things and would be useful to point the authors towards?
-   At the end of the review, was there any information you needed to ask about that should be made more apparent in the code or documentation?

Depending on your access you may or may not be able to run the code yourself, but there should be enough information within the code and documentation to be able to respond to the questions above. If you are able to run the code, you could also check -

-   Does the code run without errors? If warnings are displayed, are they explained?
-   If the project has unit/integration tests, do they pass?
-   Can you replicate previous output using the same code and input data?

If you would like a more thorough list of questions to follow, then the Duck Book has checklists available for three levels of peer review, based on risk:

-   [Lower](https://best-practice-and-impact.github.io/qa-of-code-guidance/checklist_lower.html)
-   [Moderate](https://best-practice-and-impact.github.io/qa-of-code-guidance/checklist_moderate.html)
-   [Higher](https://best-practice-and-impact.github.io/qa-of-code-guidance/checklist_higher.html)

If you're unfamiliar with giving feedback on someone's code then it can be daunting at first. Feedback should always be constructive and practical. It is recommended that you use the CEDAR model to structure your comments:

-   Context - describe the issue and the potential impact

-   Examples - give specific examples of when and where the issue has been present (specifying the line numbers of the code where the issue can be found can be useful here)

-   Diagnosis - use the example to discuss why this approach was taken, what could have been done differently and why alternatives could be an improvement

-   Actions - ask the person receiving feedback to suggest actions that they could follow to avoid this issue in future

-   Review - if you have time, revisit the discussion to look for progress following on from the feedback

-   Other tips for getting started with peer review can be found in the [Duck Book](https://best-practice-and-impact.github.io/qa-of-code-guidance/peer_review.html){target="_blank" rel="noopener noreferrer"}

-   The Duck Book also contains some helpful [code QA checklists](https://best-practice-and-impact.github.io/qa-of-code-guidance/checklists.html){target="_blank" rel="noopener noreferrer"} to help get you thinking about what to check

------------------------------------------------------------------------

#### Improving code performance

------------------------------------------------------------------------

Peer reviewing code and not sure where to start? Improving code performance can be a great quick-win for many production teams. There will be cases where code you are reviewing does things in a slightly different way to how you would: profiling the R code with the microbenchmark package is a way to objectively figure out which method is more efficient.

For example below, we are testing out case_when, if_else and ifelse.

```{r microbenchmark, eval=FALSE}
microbenchmark::microbenchmark( 
   case_when(1:1000 < 3 ~ "low", TRUE ~ "high"), 
   if_else(1:1000 < 3, "low", "high"),
   ifelse(1:1000 < 3, "low", "high") 
)
```

Running the code outputs a table in the R console, giving profile stats for each expression. Here, it is clear that on average, if_else() is the fastest function for the job.

```{r code_outputs, eval=FALSE}
Unit: microseconds
                                         expr     min       lq     mean   median       uq      max neval
 case_when(1:1000 < 3 ~ "low", TRUE ~ "high") 167.901 206.2510 372.7321 300.2515 420.1005 4187.001   100
           if_else(1:1000 < 3, "low", "high")  55.301  74.0010 125.8741 103.7015 138.3010  538.201   100
            ifelse(1:1000 < 3, "low", "high") 266.200 339.4505 466.7650 399.7010 637.6010  851.502   100

```

------------------------------------------------------------------------

### Peer review of code from outside the team {#peer-review-of-code-from-outside-the-team}

------------------------------------------------------------------------

![](../images/best.svg)

**What does this mean?**

-   Has someone from outside of the team and publication area reviewed the code and given feedback?

-   Have you taken on their feedback and improved the code?

We have already stated that [RAP should be proportional](#rap-proportional), and this is also true here. If you have a really tight deadline or it's a short, simple piece of analysis, this might not be needed or might not be possible if work is sensitive. However, if the work is business critical, high-profile, or long-term, it is worth considering whether it's appropriate and beneficial.

**Why do it?**

All of the benefits you get from peer reviewing within your own team, multiple times over. Having someone external offers new perspectives, holds you to account by breaking down assumptions, and offers far greater opportunity for building capability through knowledge sharing.

**How to get started**

While peer reviewing code within the team is often practical, having external analysts peer review your code can bring a fresh perspective. If you're interested in this, please contact us, and we can help you to arrange someone external to your team to review your processes. For this to work smoothly, we recommend that your code is easily accessible for other analysts, such as hosted in an Azure DevOps repo and mirrored to github.

------------------------------------------------------------------------

### Basic automated QA {#basic-automated-qa}

------------------------------------------------------------------------

![](../images/good.svg)

**What does this mean?**

In most analytical pipelines, there will be data files ingested, and data files produced. We should be quality assuring at every step that we can. QA can be kept more basic on smaller analytical projects, but it can never be ignored or avoided altogether! The QA team have put together a [flowchart to help you identify the QA needed in different scenarios](https://educationgovuk.sharepoint.com/sites/sarpi/g/SitePages/Quality-Assurance.aspx#%E2%80%8Bwhat-qa-tasks-do-i-need-to-carry-out).

Inputs, outputs, and even the code/process, can all have **automated QA** embedded. Some teams are already making great progress with automated QA and realising the benefits of it. The Statistics Development Team are working with these to provide generalised code that teams can use as a starting point for automated QA. The intention is that teams can then run this as a minimum, before then looking to develop more area specific checks to the script and/or continue with current checking processes in tandem. If your team already use, or are working towards using, automated QA then get in touch as we'd be keen to see what you have.

It is assumed that when using R, automated scripts will output .html reports that the team can read through to understand their data and identify any issues, and save as a part of their process documentation.

For more information on general quality assurance best practice in DfE, see the [How to QA guide](https://dfe-analytical-services.github.io/how-to-qa/index.html).

Cam and Sarah ran a session introducing how to get started with automated QA in relation to RAP, slides are available on [GitHub](https://sarahmwong.github.io/intro-to-automating-QA/#1){target="_blank" rel="noopener noreferrer"} or you can [watch the recording of the session](https://web.microsoftstream.com/embed/video/57eab29d-afeb-4651-a9f5-687098b84d13).

The list of basic automated QA checks, with code examples can be found below and in our [GitHub repository](https://github.com/dfe-analytical-services/automated-data-qa){target="_blank" rel="noopener noreferrer"}:

-   Checking for [minimum](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_minimum_values.R){target="_blank" rel="noopener noreferrer"}, [maximum](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_maximum_values.R){target="_blank" rel="noopener noreferrer"}, and [average](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_average_values.R){target="_blank" rel="noopener noreferrer"} values across your data

-   Checking for [extreme values and outliers](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_extreme_values.R){target="_blank" rel="noopener noreferrer"}

-   Ensuring there are no [duplicate rows](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_duplicate_rows.R){target="_blank" rel="noopener noreferrer"} or [duplicate columns](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_duplicate_columns.R){target="_blank" rel="noopener noreferrer"}

-   Checking that where appropriate, [geographical subtotals add up to totals](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/check_LA_subtotals_vs_region.R){target="_blank" rel="noopener noreferrer"} (e.g. all the numeric values for LAs in Yorkshire and The Humber add up to the regional total)

-   Basic [trend analysis using scatter plots](https://github.com/dfe-analytical-services/automated-data-qa/blob/main/R/create_scatter_plot.R){target="_blank" rel="noopener noreferrer"}, to help you spot outliers and help tell the story of your data.

The Statistics Development Team have developed [the QA app](https://rsconnect/rsc/dfe-published-data-qa/){target="_blank" rel="noopener noreferrer"} to include some of these basic QA outputs.

**Why do it?**

Quality is one of the three pillars that our [code of practice](https://code.statisticsauthority.gov.uk/the-code/quality/){target="_blank" rel="noopener noreferrer"} is built upon. These basic level checks allow us to have confidence that we are accurately processing the data.

Automating these checks ensures their accuracy and reliability, as well as being dramatically quicker than doing these manually.

**How to get started**

Try using our [template code snippets](https://github.com/dfe-analytical-services/automated-data-qa) to get an idea of how you could automate QA of your own publication files. A recording of our introduction to automated QA is also available at the top of the page.

------------------------------------------------------------------------

### More specific/advanced automated QA/summaries {#more-specificadvanced-automated-qasummaries}

------------------------------------------------------------------------

![](../images/great.svg)

**What does this mean?**

The QA team have put together a [flowchart to help you identify the QA needed in different scenarios](https://educationgovuk.sharepoint.com/sites/sarpi/g/SitePages/Quality-Assurance.aspx#%E2%80%8Bwhat-qa-tasks-do-i-need-to-carry-out). When it's proportional to the work, QA should go further than the most basic/generic checks above. In these cases, it is expected that teams develop their own automated QA checks to QA specificities of their analysis not covered by the basic checks.

As a part of automating QA, we should also be looking to automate the production of summary statistics. This provides us with instant insight into the stories underneath the numbers. Summary outputs are automated and used to explore the stories of the data.

**Why do it?**

Quality is one of the three pillars that our [code of practice](https://code.statisticsauthority.gov.uk/the-code/quality/){target="_blank" rel="noopener noreferrer"} is built upon. By building upon the basic checks to develop bespoke QA for our publications, we can increase our confidence in the quality of the processes and outputs that they produce.

Value is another one of the three pillars of our [code of practice](https://code.statisticsauthority.gov.uk/the-code/value/){target="_blank" rel="noopener noreferrer"}. Even more specifically it states that **'Statistics and data should be presented clearly, explained meaningfully and provide authoritative insights that serve the public good.'**. As a result, we should be developing automated summaries to help us to better understand the story of the data and be authoritative and rigorous in our telling of it.

**How to get started**

We expect that the basic level of automated QA will cover most generic needs. However, we also expect that each analytical project will have it's own quirks that require a more bespoke approach. Try to consider what things you'd usually check as flags that something hasn't gone right with your data. What are the unique aspects of your project's data, and how can you automate checks against them to give you confidence in it's accuracy and reliability?

For those who are interested in starting writing their own QA scripts, it's worth looking at packages in R such as [testthat](https://testthat.r-lib.org/){target="_blank" rel="noopener noreferrer"}, including the [coffee and coding resources](https://educationgovuk.sharepoint.com/:f:/r/sites/sarpi/g/WorkplaceDocuments/Induction%20learning%20and%20career%20development/Coffee%20and%20Coding/190306_peter_autotesting?csf=1&web=1&e=F945Xq){target="_blank" rel="noopener noreferrer"} on it by Peter Curtis, as well as this [guide on testing](http://r-pkgs.had.co.nz/tests.html){target="_blank" rel="noopener noreferrer"} by Hadley Wickham.

The [janitor](https://garthtarr.github.io/meatR/janitor.html){target="_blank" rel="noopener noreferrer"} package in R also has some particularly useful functions, such as `clean_names()` to automatically clean up your variable names, `remove_empty()` to remove any completely empty rows and columns, and `get_dupes()` which retrieves any duplicate rows in your data - this last one is particularly powerful as you can feed it specific columns and see if there's any duplicate instances of values across those columns.

------------------------------------------------------------------------

### Automated outputs using markdown/Quarto

------------------------------------------------------------------------

![](../images/best.svg)

**What does this mean?**

Often, outputs of an analytical project are not just data files. They can include written reports, presentations and visualisations. If you have used code to perform your analysis, you can also use code to automate the production of these outputs, filling in quoted figures, charts and tables automatically (also meaning these will automatically update when the process is re-run!)

**Why do it?**

Writing, producing and updating these types of outputs can be one of the longest parts of an analytical process. If you have automated templates, you could save large amounts of time that is usually spent manually copying, pasting, and updating documents. You can also be confident that each time you run the pipeline, these outputs will be updated with the most up-to-date figures, without the risk of missing anything in a manual copy and paste process.

**How to get started**

For **visualisations** we suggest you use the `ggplot` and `ggraph` packages. In our dashboards guidance we have a section on [R chart styling and colours](../writing-visualising/dashboards_rshiny.html#consistency-in-r-chart-styling-and-colours). The [`afcharts` package](https://analysisfunction.civilservice.gov.uk/blog/automated-data-visualisation-best-practice-in-r-afcharts-release/) is also recommended, as it automates setting colours, fonts, text sizing, axis labelling and much more. Their [cookbook](https://best-practice-and-impact.github.io/afcharts/articles/cookbook.html) provides handy examples of how to use the code.

For **written reports** and **presentations** we suggest you use quarto (more recent/up-to-date than Rmarkdown!). You can use quarto to generate html pages, pdf documents, slides and even word documents. Here are some examples to get you started:

-   The guidance site you're reading right now was written in quarto! You can view the code here: [Analysts guide repo](https://github.com/dfe-analytical-services/analysts-guide)
-   Online quarto guidance is extensive and a great place to start: [Quarto guide](https://quarto.org/docs/guide/)
-   Two examples of quarto slides/presentations can be seen on GitHub:
    -   [bslib coffee & coding slides](https://github.com/dfe-analytical-services/coffee-and-coding/tree/main/2024-07-02-bslib)
    -   ['targets' coffee & coding slides](https://github.com/JT-39/targets-coffee-code)
-   Our [basic automated QA guidance](https://dfe-analytical-services.github.io/analysts-guide/RAP/rap-statistics.html#basic-automated-qa) provides the link to [an example markdown QA report](https://github.com/dfe-analytical-services/automated-data-qa/tree/main).

------------------------------------------------------------------------

## Version control {#version-control}

Version control is our third core RAP principle, and is made up of the following elements:

-   [Documentation](#documentation)
-   [Version controlled final code scripts](#version-controlled-final-code-scripts)
-   [Code is stored in an appropriate repository](#code-is-stored-in-an-appropriate-repository)
-   [Collaboratively develop code using Git](#collaboratively-develop-code-using-git)

Version control is just a way to track changes to files. Using proper version control can avoid lots of potential problems, including running old scripts by accident, losing files, or ending up with a folder full of documents with suffixes like "final_final_FINAL". It also makes it much easier for new people to pick up your processes.

*When you assume you make an 'ass' out of 'u' and 'me'*. Everyone knows this saying, yet few of us heed its warning.

The aim should be to leave your work in a state that others (including future you!), can pick it up and immediately find what they need, understanding the processes that have happened previously. Changes to files should be documented, and published versions should be clearly named and stored in their own folder.

As we work with code to process our data more and more, we can begin to utilise version control software to make this process much easier, allowing simultaneous collaboration on files.

------------------------------------------------------------------------

### Documentation {#documentation}

------------------------------------------------------------------------

![](../images/good.svg)

**What does this mean?**

-   You should be annotating as you go, ensuring that every process and decision made is written down. Processes are ideally written with code, and decisions in comments.

-   There should be a [README](#writing-a-readme-file) notes file, that clearly details the steps in the process, any dependencies (such as places where access needs to be requested to) and how to carry out the process.

-   Any specialist terms should also be defined if required (e.g. The NFTYPE lookup can be found in xxxxx. "NFTYPE" means school type).

**Why do it?**

When documenting your processes you should leave nothing to chance, we all have wasted time in the past trying to work out what it was that we had done before, and that time increases even more when we are picking up someone else's work. Thorough documentation saves us time, and provides a clear audit trail of what we do. This is key for the 'Reproducible' part of RAP, our processes must be easily reproducible and clear documentation is fundamental to that.

**How to get started**

Take a look at your processes and be critical - could another analyst pick them up without you there to help them? If the answer is no (don't feel ashamed, it will be for many teams) then go through and note down areas that require improvement, so that you can revise them with your team.

Take a look at the sections below for further guidance on improving your documentation.

------------------------------------------------------------------------

#### Commenting in code {#commenting-in-code}

------------------------------------------------------------------------

When writing code, whether that is SQL, R, or something else, make sure you're commenting as you go. Start off every file by outlining the date, author, purpose, and if applicable, the structure of the file, like this:

```{r comments_example, eval=FALSE}
----------------------------------------------------------------------------------------
-- Script Name:		Section 251 Table A 2019 - s251_tA_2019.sql
-- Description:		Extraction of data from IStore and production of underlying data file
-- Author:		    Cam Race
-- Creation Date:   15/11/2019
----------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------
--//  Process
-- 1. Extract the data for each available year
-- 2. Match in extra geographical information
-- 3. Create aggregations - both categorical and geographical totals
-- 4. Tidy up and output results
-- 5. Metadata creation
----------------------------------------------------------------------------------------
```

Commented lines should begin with -- (SQL) or \# (R), followed by one space and your comment. Remember that **comments should explain the why, not the what.**

In SQL you can also use `/**` and `**/` to bookend comments over multiple lines.

In rmarkdown documents you can bookend comments by using `<!--` and `-->`.

Use commented lines of - to break up your files into scannable chunks based upon the structure and subheadings, like the R example below:

```{r comments_example_2, eval=FALSE}
# Importing the data -------------------------------------------------------------------

```

Doing this can visually break up your code into sections that are easy to navigate around. It will also add that section to your outline, which can be used in RStudio using Ctrl-Shift-O. More details on the possibilities for this can be found in the [RStudio guidance on folding and sectioning code](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections){target="_blank" rel="noopener noreferrer"}.

You might be thinking that it would be nice if there was software that could help you with documentation, if so, read on, as Git is an incredibly powerful tool that can help us easily and thoroughly document versions of our files. If you're at the stage where you are developing your own functions and packages in R, then take a look at [roxygen2](https://roxygen2.r-lib.org/){target="_blank" rel="noopener noreferrer"} as well.

------------------------------------------------------------------------

#### Writing a README file {#writing-a-readme-file}

------------------------------------------------------------------------

**What does this mean?**

A README is a markdown file (.md) that introduces and explains a project. It contains information that is required to understand what the project is about and how to use it. Markdown (.md) files are used for READMEs because they support formatting and render nicely on platforms like GitHub and Azure DevOps, meaning that users can see them on the main page of the repository. You can find guidance on basic markdown syntax on the [Markdown Guide](https://www.markdownguide.org/basic-syntax/){target="_blank" rel="noopener noreferrer"}.

**Why do it?**

It's an easy way to answer questions that your audience will likely have regarding how to install and use your project and also how to collaborate with you.

**How to get started**

For new projects, you can use the create_project function in dfeR. Set create_publication_proj to TRUE to create a pre-populated project with a custom folder structure, including a [README template](https://github.com/dfe-analytical-services/dfeR/blob/main/README_template.md). You can find more information on this in the [dfeR reference](https://dfe-analytical-services.github.io/dfeR/reference/create_project.html).

If you are creating your own README for existing projects, you should include all of the sections listed below:

**Introduction**

-   Purpose: briefly explain the purpose of the code.
-   Overview: Provide a high-level summary of the contents and structure of the repository.

**Requirements**

-   Access: Detail any permissions or access needed to use the repository at the top of this section, e.g. access to specific SQL databases. This is crucial for enabling new users to use the repository.
-   Skills/knowledge: Outline the required skills or knowledge, such as familiarity with specific packages in R, or SQL.
-   Version control/Renv: State how version control is managed and whether Renv is being used.

**Getting started**

-   Setup instructions: Provide step-by-step instructions on how to set up the environment, including installing dependencies.
-   Data input/output: Describe the expected input data and where it can be found, as well as what output should be expected from the code.

**How to run and update**

-   Running the code: Explain how users can best run the code, for example by running a run all script.
-   Updating guidelines: Outline the process for updating and contributing to the repository, including specific scripts and lines where updates are frequently needed. Describe how to get changes reviewed.
-   Issue reporting: Explain how to report issues or suggest improvements. This could be through issues if using GitHub, boards in Azure DevOps or by emailing the team.

**Contact details**

-   Main contacts: List the names and contact information of people who maintain the repository.
-   Support channels: Provide any information on how to get support, such as email addresses or teams channels.

The [Self-assessment tool](https://github.com/dfe-analytical-services/publication-self-assessment-copy){target="_blank" rel="noopener noreferrer"} and the [QA app](https://github.com/dfe-analytical-services/dfe-published-data-qa){target="_blank" rel="noopener noreferrer"} give two examples of readme files structured like this.

------------------------------------------------------------------------

### Version controlled final code scripts {#version-controlled-final-code-scripts}

------------------------------------------------------------------------

![](../images/great.svg)

**What does this mean?**

This means having the final copies of code and documentation saved in an appropriate git-controlled repository (typically in Azure DevOps or GitHub). Access to Azure DevOps is restricted only to people in DfE with specific account permissions. This is different to GitHub, which makes code publicly available.

Analysis should always be stored in the correct type of repository - what this means is dependent on the analysis/project.

-   **Sensitive or unpublished** work should never be shared outside of the department, and sometimes cannot be shared outside of a small team/area. In these instances, the code, files and folders should be stored in an **Azure DevOps** repository with the correct security procedures. The security settings of your repository are probably down to your project admins. The [Statistics Development Team](mailto:statistics.development@education.gov.uk) have experience and knowledge setting up Azure DevOps projects with the correct permissions, so get in touch if with them if you have questions or need any help.

-   **Published analysis** that uses public data should be stored in the public [DfE analytical services GitHub area](https://github.com/dfe-analytical-services) repository in line with transparency, audibility and trustworthiness. We currently have brilliant examples of this in our [DfE analytical services GitHub area](https://github.com/dfe-analytical-services), in which all of the code used to create public dashboards is publicly available.

If you do not already have git downloaded, you can [download the latest version from their website](https://git-scm.com/downloads).

For now, take a look at the [resources for learning git](../learning-development/git.html) in the learning resources section.

**Why do it?**

Having the final copy of the scripts version controlled gives assurance around how the data was created. It also allows teams to easily record any last minute changes to the code after the initial final version by using the version control to log this.

**How to get started**

The first step is to get your final versions of code and documentation together in a single folder.

Many teams/work areas have their own Azure DevOps project areas. Investigate whether this is the case, and if you aren't aware of any area being available to you, you can request to create one via an IT ticket.

To either create a new project area or gain access to an existing one, please raise a request on service desk by navigating through the pages detailed in the animation below.

![](../images/servicedesk_request.gif)

Access is usually granted within a few working days. Check who the administrators of your new area are, as they will be the ones with the ability to create a repository for you and set the correct permissions.

------------------------------------------------------------------------

#### renv

------------------------------------------------------------------------

We recommend the use of the [renv package](https://rstudio.github.io/renv/articles/renv.html) to help maintain consistent versions of packages within your work. You can learn more about how to use renv [on our R page](../learning-development/r.html#renv).

------------------------------------------------------------------------

#### Avoid revealing sensitive information

------------------------------------------------------------------------

Here are some general best practice tips:

-   Using .gitignore to ignore files and folders to prevent committing anything sensitive
-   Never committing outputs unless theyâ€™ve been checked over, even aggregates. We suggest only outputting to an output folder which is in the .gitignore file, to ensure this doesnâ€™t happen by mistake
-   Keeping datasets and secrets (e.g. API keys) outside the repository as much as possible, make use of secure variables
-   Checking Git histories: if someone is planning on open-sourcing code that has previously been in a private repository or only version-controlled locally, you want to be careful not to have anything sensitive in the commit history. You can do this by following the above rules. When in doubt, you can remove the git history and start the public repo without it
-   You can [remove a file from the entire commit history](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/removing-sensitive-data-from-a-repository) if you did commit anything sensitive, although you still need to follow the usual procedures if this was a data breach

You can find out more in the Duck Book's [guidance on using Git](https://best-practice-and-impact.github.io/qa-of-code-guidance/version_control.html#avoid-commiting-sensitive-information-to-git-repositories).

------------------------------------------------------------------------

### Collaboratively develop code using Git {#collaboratively-develop-code-using-git}

------------------------------------------------------------------------

![](../images/best.svg)

**What does this mean?**

-   Has code development taken place in Git, collaboratively across the team?

-   Are you making use of pull requests for team members to review and comment on code updates?

-   Is there a clear paper trail of changes to code (commits)?

**Why do it?**

Using Git allows multiple people to simultaneously develop the same code using branches, all with a crystal clear audit trail showing what changes were made when using commits. It makes it easy for team members to review changes via pull requests.

**How to get started**

To get started you should:

------------------------------------------------------------------------

#### Get your code into a Git controlled folder {#get-your-code-into-a-git-controlled-folder}

------------------------------------------------------------------------

Get code into a Git controlled folder in whatever version it is currently in. Use the following steps to do so:

1.  Open the folder where your project is saved, right click anywhere in that window, and click "Git Bash Here".

2.  This will open a black box (the terminal). Type in the following and hit enter

```{r git_init, eval=FALSE}
git init
```

3.  After hitting enter, type in the following and hit enter again after each line. You will need the URL of your Azure DevOps repository to complete this step. Contact the [Statistics Development Team](mailto:statistics.development@education.gov.uk) if you are not sure what this is or do not have one.

```{r git_steps, eval=FALSE}
git add .

git commit -m "first commit"

git remote add origin YOUR_URL_HERE

git push -f origin --all
```

-   You may be prompted for either your Windows or Git credentials at this stage.

    -   If prompted for your **Windows** credentials, enter the username and password combination you use to log into your DfE device.

    -   If prompted for your **Git** credentials, visit your online repository, click on the blue "clone" box, and click "generate Git credentials". This will generate a username and password for you to enter.

4.  Visit your repository online, and check that all the files have uploaded. Other members of your team will now be able to work from your code.

------------------------------------------------------------------------

#### Build capability within the team

------------------------------------------------------------------------

-   Ensure all team members have access to your repository. Contact the [Statistics Development Team](mailto:statistics.development@education.gov.uk) if there are any issues.

-   Get team members to clone your repository in to their personal area, so everyone is able to work on code at the same time.

To clone code, they will need to do the following:

1.  Run through steps 1 - 2a of [getting a file into a Git controlled folder](#get-your-code-into-a-git-controlled-folder)

2.  After running those lines, type in the following with your repository URL in the "YOUR_URL_HERE" space. This will clone the online repository to your local area.

```{r git_team_steps, eval=FALSE}
git clone YOUR_URL_HERE
```

-   Make use of Git and version control in your team projects regularly. Like learning anything new, putting it into practice regularly is the best way to become confident in using it.

-   Please refer to the other links on the [Git learning resources](../learning-development/git.html#other-resources) page to learn more about how to use Git in practice.

------------------------------------------------------------------------


## Integrating RAP principles into spreadsheets

------------------------------------------------------------------------

In some cases, spreadsheet tools like Microsoft Excel are still used for some types of analysis, particularly ad hoc, exploratory, or early-stage work. While the long-term aim is to move towards fully  (RAP) processes using code, this section outlines how RAP principles can be applied to spreadsheet work to improve quality, transparency, and maintainability, even before moving fully into code.

### RAP Principles adaptable to spreadsheets
Spreadsheets can incorporate several RAP-aligned practices:

**Source data is acquired and stored sensibly**: Ensure input data is stored securely and referenced clearly, ideally in a separate tab or file.

**Sensible folder and file structure**: Keep raw data, calculations, and outputs clearly separated and consistently organised.

**Automated high level checks**: Include high-level checks such as totals, flags, or conditional formatting to catch errors early.

**Documentation** : Use cell comments, notes and separate README files to explain the purpose, logic, and structure of the spreadsheet.

**Peer review**: Ask a colleague to review the spreadsheetâ€™s logic, formulas, and outputsâ€”especially for repeated or high-impact analysis.

### Supporting the transition to RAP

For repeatable, cross-departmental, or high-risk analysis, spreadsheets should ideally be replaced with code-based RAP workflows. Hybrid approaches such as using code to generate inputs or to perform calculations further into the process can be a useful stepping stone.


------------------------------------------------------------------------